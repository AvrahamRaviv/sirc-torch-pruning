2026-02-13 00:29:14,781 | INFO | Logging to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_imagenet.log
2026-02-13 00:29:14,781 | INFO | ============================================================
2026-02-13 00:29:14,781 | INFO | VBP ImageNet Reproduction Script
2026-02-13 00:29:14,781 | INFO | ============================================================
2026-02-13 00:29:14,781 | INFO |   model_type: cnn
2026-02-13 00:29:14,781 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/MNv2_TP/mobilenet_v2_weights.pth
2026-02-13 00:29:14,781 | INFO |   cnn_arch: mobilenet_v2
2026-02-13 00:29:14,781 | INFO |   pretrained: True
2026-02-13 00:29:14,781 | INFO |   interior_only: True
2026-02-13 00:29:14,781 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 00:29:14,781 | INFO |   train_batch_size: 64
2026-02-13 00:29:14,781 | INFO |   val_batch_size: 128
2026-02-13 00:29:14,781 | INFO |   num_workers: 4
2026-02-13 00:29:14,781 | INFO |   max_batches: 200
2026-02-13 00:29:14,781 | INFO |   keep_ratio: 0.8
2026-02-13 00:29:14,781 | INFO |   global_pruning: True
2026-02-13 00:29:14,781 | INFO |   max_pruning_ratio: 1.0
2026-02-13 00:29:14,782 | INFO |   norm_per_layer: False
2026-02-13 00:29:14,782 | INFO |   no_compensation: False
2026-02-13 00:29:14,782 | INFO |   no_recalib: False
2026-02-13 00:29:14,782 | INFO |   criterion: variance
2026-02-13 00:29:14,782 | INFO |   epochs_ft: 10
2026-02-13 00:29:14,782 | INFO |   lr_ft: 0.01
2026-02-13 00:29:14,782 | INFO |   opt_ft: sgd
2026-02-13 00:29:14,782 | INFO |   momentum_ft: 0.9
2026-02-13 00:29:14,782 | INFO |   wd_ft: 4e-05
2026-02-13 00:29:14,782 | INFO |   use_kd: True
2026-02-13 00:29:14,782 | INFO |   kd_alpha: 0.7
2026-02-13 00:29:14,782 | INFO |   kd_T: 2.0
2026-02-13 00:29:14,782 | INFO |   pat: False
2026-02-13 00:29:14,782 | INFO |   pat_steps: 5
2026-02-13 00:29:14,782 | INFO |   pat_epochs_per_step: 3
2026-02-13 00:29:14,782 | INFO |   var_loss_weight: 0.0
2026-02-13 00:29:14,782 | INFO |   sparse_mode: none
2026-02-13 00:29:14,782 | INFO |   epochs_sparse: 5
2026-02-13 00:29:14,782 | INFO |   lr_sparse: 0.0001
2026-02-13 00:29:14,782 | INFO |   l1_lambda: 0.0001
2026-02-13 00:29:14,782 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 00:29:14,782 | INFO |   disable_ddp: False
2026-02-13 00:29:14,782 | INFO |   local_rank: 0
2026-02-13 00:29:14,782 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8
2026-02-13 00:29:14,782 | INFO |   rank: 0
2026-02-13 00:29:14,782 | INFO |   world_size: 2
2026-02-13 00:29:14,783 | INFO | Loading ImageNet dataset...
2026-02-13 00:29:14,783 | INFO | Using cached sample lists for fast loading
2026-02-13 00:29:15,124 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 00:29:16,867 | INFO | Loaded mobilenet_v2 (pretrained=True)
2026-02-13 00:29:18,259 | INFO | Baseline: 0.32G MACs, 3.50M params
2026-02-13 00:29:18,260 | INFO | Evaluating original model...
2026-02-13 00:30:26,677 | INFO | Original accuracy: 0.7187, loss: 1.1474
2026-02-13 00:30:26,705 | INFO | Created teacher model for knowledge distillation
2026-02-13 00:30:26,705 | INFO | PAT: 1 steps, per_step_keep=0.8000, epochs_per_step=0, target_keep=0.8, criterion=variance
2026-02-13 00:30:26,705 | INFO | 
============================================================
2026-02-13 00:30:26,705 | INFO | PAT Step 1/1
2026-02-13 00:30:26,705 | INFO | ============================================================
2026-02-13 00:30:26,726 | INFO | Auto-detected 35 CNN target layers for stats
2026-02-13 00:30:26,726 | INFO | Collecting activation variance statistics...
2026-02-13 00:30:42,697 | INFO | Statistics collected for 35 layers
2026-02-13 00:30:42,697 | INFO |   Conv2d: mean_var=0.027803
2026-02-13 00:30:42,697 | INFO |   Conv2d: mean_var=0.381779
2026-02-13 00:30:42,697 | INFO |   Conv2d: mean_var=0.045613
2026-02-13 00:30:42,697 | INFO |   Conv2d: mean_var=0.291214
2026-02-13 00:30:42,697 | INFO |   Conv2d: mean_var=0.015346
2026-02-13 00:30:42,855 | INFO | Variance â€” entropy=0.8414, cv=2.1452, gini=0.8129
2026-02-13 00:30:43,265 | INFO | Pruning: per_step_prune=0.2000
2026-02-13 00:30:43,500 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 00:30:43,500 | INFO | Recalibrating BN running stats...
2026-02-13 00:30:44,293 | INFO | BN recalib [1/100]
2026-02-13 00:30:44,493 | INFO | BN recalib [6/100]
2026-02-13 00:30:45,232 | INFO | BN recalib [11/100]
2026-02-13 00:30:45,527 | INFO | BN recalib [16/100]
2026-02-13 00:30:46,027 | INFO | BN recalib [21/100]
2026-02-13 00:30:46,336 | INFO | BN recalib [26/100]
2026-02-13 00:30:46,757 | INFO | BN recalib [31/100]
2026-02-13 00:30:47,051 | INFO | BN recalib [36/100]
2026-02-13 00:30:47,592 | INFO | BN recalib [41/100]
2026-02-13 00:30:47,888 | INFO | BN recalib [46/100]
2026-02-13 00:30:48,262 | INFO | BN recalib [51/100]
2026-02-13 00:30:48,552 | INFO | BN recalib [56/100]
2026-02-13 00:30:49,048 | INFO | BN recalib [61/100]
2026-02-13 00:30:49,345 | INFO | BN recalib [66/100]
2026-02-13 00:30:49,842 | INFO | BN recalib [71/100]
2026-02-13 00:30:50,138 | INFO | BN recalib [76/100]
2026-02-13 00:30:50,488 | INFO | BN recalib [81/100]
2026-02-13 00:30:50,796 | INFO | BN recalib [86/100]
2026-02-13 00:30:51,365 | INFO | BN recalib [91/100]
2026-02-13 00:30:51,604 | INFO | BN recalib [96/100]
2026-02-13 00:30:52,007 | INFO | BN recalib [100/100]
2026-02-13 00:30:52,415 | INFO | BN recalibration done (100 batches)
2026-02-13 00:31:57,391 | INFO | Step 1 retention: acc=0.0503, loss=6.1030
2026-02-13 00:31:57,391 | INFO |   cumulative_keep=0.8000, MACs=0.27G, params=2.92M
2026-02-13 00:31:57,391 | INFO | 
Post-prune fine-tuning for 10 epochs (sgd, lr=0.01, wd=4e-05)...
2026-02-13 00:31:58,387 | INFO | FT 1 [1/10009] loss=7.9456
2026-02-13 00:32:46,817 | INFO | FT 1 [501/10009] loss=3.8254
2026-02-13 00:33:34,802 | INFO | FT 1 [1001/10009] loss=3.4460
2026-02-13 00:34:22,909 | INFO | FT 1 [1501/10009] loss=3.2421
2026-02-13 00:35:10,892 | INFO | FT 1 [2001/10009] loss=3.1135
2026-02-13 00:35:58,979 | INFO | FT 1 [2501/10009] loss=3.0216
2026-02-13 00:36:47,128 | INFO | FT 1 [3001/10009] loss=2.9459
2026-02-13 00:37:35,083 | INFO | FT 1 [3501/10009] loss=2.8876
2026-02-13 00:38:23,208 | INFO | FT 1 [4001/10009] loss=2.8348
2026-02-13 00:39:11,386 | INFO | FT 1 [4501/10009] loss=2.7914
2026-02-13 00:39:59,458 | INFO | FT 1 [5001/10009] loss=2.7532
2026-02-13 00:40:47,340 | INFO | FT 1 [5501/10009] loss=2.7180
2026-02-13 00:41:35,660 | INFO | FT 1 [6001/10009] loss=2.6861
2026-02-13 00:42:23,905 | INFO | FT 1 [6501/10009] loss=2.6569
2026-02-13 00:43:12,252 | INFO | FT 1 [7001/10009] loss=2.6313
2026-02-13 00:44:00,570 | INFO | FT 1 [7501/10009] loss=2.6064
2026-02-13 00:44:48,723 | INFO | FT 1 [8001/10009] loss=2.5865
2026-02-13 00:45:36,682 | INFO | FT 1 [8501/10009] loss=2.5659
2026-02-13 00:46:24,914 | INFO | FT 1 [9001/10009] loss=2.5461
2026-02-13 00:47:12,970 | INFO | FT 1 [9501/10009] loss=2.5278
2026-02-13 00:48:01,129 | INFO | FT 1 [10001/10009] loss=2.5110
2026-02-13 00:48:01,926 | INFO | FT 1 [10009/10009] loss=2.5107
2026-02-13 00:49:09,846 | INFO | FT ep 1/10: train_loss=2.5107, val_acc=0.5696
2026-02-13 00:49:09,903 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 00:49:10,792 | INFO | FT 2 [1/10009] loss=1.9647
2026-02-13 00:49:59,023 | INFO | FT 2 [501/10009] loss=2.1419
2026-02-13 00:50:46,959 | INFO | FT 2 [1001/10009] loss=2.1341
2026-02-13 00:51:35,031 | INFO | FT 2 [1501/10009] loss=2.1374
2026-02-13 00:52:23,168 | INFO | FT 2 [2001/10009] loss=2.1306
2026-02-13 00:53:11,212 | INFO | FT 2 [2501/10009] loss=2.1245
2026-02-13 00:53:59,462 | INFO | FT 2 [3001/10009] loss=2.1180
2026-02-13 00:54:47,601 | INFO | FT 2 [3501/10009] loss=2.1132
2026-02-13 00:55:35,618 | INFO | FT 2 [4001/10009] loss=2.1115
2026-02-13 00:56:23,776 | INFO | FT 2 [4501/10009] loss=2.1073
2026-02-13 00:57:11,763 | INFO | FT 2 [5001/10009] loss=2.1036
2026-02-13 00:57:59,979 | INFO | FT 2 [5501/10009] loss=2.1001
2026-02-13 00:58:48,429 | INFO | FT 2 [6001/10009] loss=2.0951
2026-02-13 00:59:36,773 | INFO | FT 2 [6501/10009] loss=2.0909
2026-02-13 01:00:25,075 | INFO | FT 2 [7001/10009] loss=2.0863
2026-02-13 01:01:13,450 | INFO | FT 2 [7501/10009] loss=2.0818
2026-02-13 01:02:01,696 | INFO | FT 2 [8001/10009] loss=2.0788
2026-02-13 01:02:49,807 | INFO | FT 2 [8501/10009] loss=2.0754
2026-02-13 01:03:37,973 | INFO | FT 2 [9001/10009] loss=2.0715
2026-02-13 01:04:26,138 | INFO | FT 2 [9501/10009] loss=2.0677
2026-02-13 01:05:14,371 | INFO | FT 2 [10001/10009] loss=2.0643
2026-02-13 01:05:15,162 | INFO | FT 2 [10009/10009] loss=2.0643
2026-02-13 01:06:20,918 | INFO | FT ep 2/10: train_loss=2.0643, val_acc=0.5941
2026-02-13 01:06:20,973 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 01:06:21,947 | INFO | FT 3 [1/10009] loss=2.2091
2026-02-13 01:07:10,274 | INFO | FT 3 [501/10009] loss=1.9376
2026-02-13 01:07:58,408 | INFO | FT 3 [1001/10009] loss=1.9427
2026-02-13 01:08:46,497 | INFO | FT 3 [1501/10009] loss=1.9433
2026-02-13 01:09:34,637 | INFO | FT 3 [2001/10009] loss=1.9388
2026-02-13 01:10:22,778 | INFO | FT 3 [2501/10009] loss=1.9338
2026-02-13 01:11:10,916 | INFO | FT 3 [3001/10009] loss=1.9312
2026-02-13 01:11:58,909 | INFO | FT 3 [3501/10009] loss=1.9308
2026-02-13 01:12:47,018 | INFO | FT 3 [4001/10009] loss=1.9289
2026-02-13 01:13:34,973 | INFO | FT 3 [4501/10009] loss=1.9271
2026-02-13 01:14:23,177 | INFO | FT 3 [5001/10009] loss=1.9289
2026-02-13 01:15:11,606 | INFO | FT 3 [5501/10009] loss=1.9280
2026-02-13 01:16:00,004 | INFO | FT 3 [6001/10009] loss=1.9251
2026-02-13 01:16:48,396 | INFO | FT 3 [6501/10009] loss=1.9231
2026-02-13 01:17:36,812 | INFO | FT 3 [7001/10009] loss=1.9213
2026-02-13 01:18:24,965 | INFO | FT 3 [7501/10009] loss=1.9200
2026-02-13 01:19:12,912 | INFO | FT 3 [8001/10009] loss=1.9184
2026-02-13 01:20:01,037 | INFO | FT 3 [8501/10009] loss=1.9159
2026-02-13 01:20:49,117 | INFO | FT 3 [9001/10009] loss=1.9125
2026-02-13 01:21:37,146 | INFO | FT 3 [9501/10009] loss=1.9103
2026-02-13 01:22:25,197 | INFO | FT 3 [10001/10009] loss=1.9081
2026-02-13 01:22:25,976 | INFO | FT 3 [10009/10009] loss=1.9080
2026-02-13 01:23:34,096 | INFO | FT ep 3/10: train_loss=1.9080, val_acc=0.6199
2026-02-13 01:23:34,159 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 01:23:35,020 | INFO | FT 4 [1/10009] loss=1.8460
2026-02-13 01:24:23,321 | INFO | FT 4 [501/10009] loss=1.8131
2026-02-13 01:25:11,423 | INFO | FT 4 [1001/10009] loss=1.8149
2026-02-13 01:25:59,584 | INFO | FT 4 [1501/10009] loss=1.8161
2026-02-13 01:26:47,857 | INFO | FT 4 [2001/10009] loss=1.8145
2026-02-13 01:27:35,956 | INFO | FT 4 [2501/10009] loss=1.8140
2026-02-13 01:28:24,128 | INFO | FT 4 [3001/10009] loss=1.8156
2026-02-13 01:29:12,276 | INFO | FT 4 [3501/10009] loss=1.8161
2026-02-13 01:30:00,363 | INFO | FT 4 [4001/10009] loss=1.8131
2026-02-13 01:30:48,149 | INFO | FT 4 [4501/10009] loss=1.8120
2026-02-13 01:31:36,377 | INFO | FT 4 [5001/10009] loss=1.8133
2026-02-13 01:32:24,835 | INFO | FT 4 [5501/10009] loss=1.8130
2026-02-13 01:33:13,201 | INFO | FT 4 [6001/10009] loss=1.8132
2026-02-13 01:34:01,602 | INFO | FT 4 [6501/10009] loss=1.8123
2026-02-13 01:34:50,007 | INFO | FT 4 [7001/10009] loss=1.8103
2026-02-13 01:35:38,106 | INFO | FT 4 [7501/10009] loss=1.8084
2026-02-13 01:36:26,206 | INFO | FT 4 [8001/10009] loss=1.8060
2026-02-13 01:37:14,346 | INFO | FT 4 [8501/10009] loss=1.8033
2026-02-13 01:38:02,513 | INFO | FT 4 [9001/10009] loss=1.8010
2026-02-13 01:38:50,529 | INFO | FT 4 [9501/10009] loss=1.7994
2026-02-13 01:39:38,710 | INFO | FT 4 [10001/10009] loss=1.7980
2026-02-13 01:39:39,497 | INFO | FT 4 [10009/10009] loss=1.7981
2026-02-13 01:40:47,328 | INFO | FT ep 4/10: train_loss=1.7981, val_acc=0.6326
2026-02-13 01:40:47,408 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 01:40:48,418 | INFO | FT 5 [1/10009] loss=1.9314
2026-02-13 01:41:36,632 | INFO | FT 5 [501/10009] loss=1.7195
2026-02-13 01:42:24,902 | INFO | FT 5 [1001/10009] loss=1.7222
2026-02-13 01:43:13,158 | INFO | FT 5 [1501/10009] loss=1.7257
2026-02-13 01:44:01,316 | INFO | FT 5 [2001/10009] loss=1.7257
2026-02-13 01:44:49,341 | INFO | FT 5 [2501/10009] loss=1.7245
2026-02-13 01:45:37,486 | INFO | FT 5 [3001/10009] loss=1.7232
2026-02-13 01:46:25,660 | INFO | FT 5 [3501/10009] loss=1.7218
2026-02-13 01:47:13,709 | INFO | FT 5 [4001/10009] loss=1.7203
2026-02-13 01:48:01,816 | INFO | FT 5 [4501/10009] loss=1.7199
2026-02-13 01:48:50,251 | INFO | FT 5 [5001/10009] loss=1.7182
2026-02-13 01:49:38,635 | INFO | FT 5 [5501/10009] loss=1.7178
2026-02-13 01:50:27,067 | INFO | FT 5 [6001/10009] loss=1.7163
2026-02-13 01:51:15,408 | INFO | FT 5 [6501/10009] loss=1.7142
2026-02-13 01:52:03,696 | INFO | FT 5 [7001/10009] loss=1.7125
2026-02-13 01:52:51,878 | INFO | FT 5 [7501/10009] loss=1.7106
2026-02-13 01:53:40,085 | INFO | FT 5 [8001/10009] loss=1.7088
2026-02-13 01:54:28,198 | INFO | FT 5 [8501/10009] loss=1.7074
2026-02-13 01:55:16,348 | INFO | FT 5 [9001/10009] loss=1.7054
2026-02-13 01:56:04,460 | INFO | FT 5 [9501/10009] loss=1.7049
2026-02-13 01:56:52,641 | INFO | FT 5 [10001/10009] loss=1.7025
2026-02-13 01:56:53,449 | INFO | FT 5 [10009/10009] loss=1.7023
2026-02-13 01:57:58,829 | INFO | FT ep 5/10: train_loss=1.7023, val_acc=0.6456
2026-02-13 01:57:58,878 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 01:57:59,838 | INFO | FT 6 [1/10009] loss=1.9052
2026-02-13 01:58:47,616 | INFO | FT 6 [501/10009] loss=1.6603
2026-02-13 01:59:35,482 | INFO | FT 6 [1001/10009] loss=1.6545
2026-02-13 02:00:23,661 | INFO | FT 6 [1501/10009] loss=1.6487
2026-02-13 02:01:11,616 | INFO | FT 6 [2001/10009] loss=1.6471
2026-02-13 02:01:59,783 | INFO | FT 6 [2501/10009] loss=1.6480
2026-02-13 02:02:47,872 | INFO | FT 6 [3001/10009] loss=1.6449
2026-02-13 02:03:36,055 | INFO | FT 6 [3501/10009] loss=1.6440
2026-02-13 02:04:24,072 | INFO | FT 6 [4001/10009] loss=1.6428
2026-02-13 02:05:12,432 | INFO | FT 6 [4501/10009] loss=1.6405
2026-02-13 02:06:00,816 | INFO | FT 6 [5001/10009] loss=1.6372
2026-02-13 02:06:49,267 | INFO | FT 6 [5501/10009] loss=1.6354
2026-02-13 02:07:37,630 | INFO | FT 6 [6001/10009] loss=1.6343
2026-02-13 02:08:25,988 | INFO | FT 6 [6501/10009] loss=1.6331
2026-02-13 02:09:14,215 | INFO | FT 6 [7001/10009] loss=1.6320
2026-02-13 02:10:02,471 | INFO | FT 6 [7501/10009] loss=1.6308
2026-02-13 02:10:50,701 | INFO | FT 6 [8001/10009] loss=1.6300
2026-02-13 02:11:38,830 | INFO | FT 6 [8501/10009] loss=1.6278
2026-02-13 02:12:27,175 | INFO | FT 6 [9001/10009] loss=1.6253
2026-02-13 02:13:15,213 | INFO | FT 6 [9501/10009] loss=1.6229
2026-02-13 02:14:03,340 | INFO | FT 6 [10001/10009] loss=1.6212
2026-02-13 02:14:04,129 | INFO | FT 6 [10009/10009] loss=1.6211
2026-02-13 02:15:10,158 | INFO | FT ep 6/10: train_loss=1.6211, val_acc=0.6584
2026-02-13 02:15:10,220 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 02:15:11,118 | INFO | FT 7 [1/10009] loss=1.7067
2026-02-13 02:15:59,111 | INFO | FT 7 [501/10009] loss=1.5725
2026-02-13 02:16:47,182 | INFO | FT 7 [1001/10009] loss=1.5709
2026-02-13 02:17:35,302 | INFO | FT 7 [1501/10009] loss=1.5625
2026-02-13 02:18:23,602 | INFO | FT 7 [2001/10009] loss=1.5614
2026-02-13 02:19:11,692 | INFO | FT 7 [2501/10009] loss=1.5593
2026-02-13 02:19:59,955 | INFO | FT 7 [3001/10009] loss=1.5565
2026-02-13 02:20:47,916 | INFO | FT 7 [3501/10009] loss=1.5546
2026-02-13 02:21:36,140 | INFO | FT 7 [4001/10009] loss=1.5563
2026-02-13 02:22:24,492 | INFO | FT 7 [4501/10009] loss=1.5539
2026-02-13 02:23:12,943 | INFO | FT 7 [5001/10009] loss=1.5539
2026-02-13 02:24:01,298 | INFO | FT 7 [5501/10009] loss=1.5514
2026-02-13 02:24:49,657 | INFO | FT 7 [6001/10009] loss=1.5492
2026-02-13 02:25:37,803 | INFO | FT 7 [6501/10009] loss=1.5490
2026-02-13 02:26:25,913 | INFO | FT 7 [7001/10009] loss=1.5490
2026-02-13 02:27:14,175 | INFO | FT 7 [7501/10009] loss=1.5475
2026-02-13 02:28:02,368 | INFO | FT 7 [8001/10009] loss=1.5461
2026-02-13 02:28:50,623 | INFO | FT 7 [8501/10009] loss=1.5443
2026-02-13 02:29:38,762 | INFO | FT 7 [9001/10009] loss=1.5429
2026-02-13 02:30:26,830 | INFO | FT 7 [9501/10009] loss=1.5418
2026-02-13 02:31:14,866 | INFO | FT 7 [10001/10009] loss=1.5406
2026-02-13 02:31:15,671 | INFO | FT 7 [10009/10009] loss=1.5406
2026-02-13 02:32:23,826 | INFO | FT ep 7/10: train_loss=1.5406, val_acc=0.6642
2026-02-13 02:32:23,875 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 02:32:24,936 | INFO | FT 8 [1/10009] loss=1.3485
2026-02-13 02:33:12,968 | INFO | FT 8 [501/10009] loss=1.4945
2026-02-13 02:34:00,960 | INFO | FT 8 [1001/10009] loss=1.4910
2026-02-13 02:34:49,025 | INFO | FT 8 [1501/10009] loss=1.4915
2026-02-13 02:35:37,088 | INFO | FT 8 [2001/10009] loss=1.4921
2026-02-13 02:36:25,131 | INFO | FT 8 [2501/10009] loss=1.4915
2026-02-13 02:37:13,103 | INFO | FT 8 [3001/10009] loss=1.4910
2026-02-13 02:38:01,177 | INFO | FT 8 [3501/10009] loss=1.4889
2026-02-13 02:38:49,496 | INFO | FT 8 [4001/10009] loss=1.4865
2026-02-13 02:39:37,801 | INFO | FT 8 [4501/10009] loss=1.4851
2026-02-13 02:40:26,092 | INFO | FT 8 [5001/10009] loss=1.4838
2026-02-13 02:41:14,446 | INFO | FT 8 [5501/10009] loss=1.4838
2026-02-13 02:42:02,704 | INFO | FT 8 [6001/10009] loss=1.4828
2026-02-13 02:42:50,704 | INFO | FT 8 [6501/10009] loss=1.4809
2026-02-13 02:43:38,765 | INFO | FT 8 [7001/10009] loss=1.4797
2026-02-13 02:44:26,826 | INFO | FT 8 [7501/10009] loss=1.4786
2026-02-13 02:45:14,953 | INFO | FT 8 [8001/10009] loss=1.4777
2026-02-13 02:46:03,109 | INFO | FT 8 [8501/10009] loss=1.4779
2026-02-13 02:46:51,280 | INFO | FT 8 [9001/10009] loss=1.4764
2026-02-13 02:47:39,396 | INFO | FT 8 [9501/10009] loss=1.4763
2026-02-13 02:48:27,612 | INFO | FT 8 [10001/10009] loss=1.4752
2026-02-13 02:48:28,389 | INFO | FT 8 [10009/10009] loss=1.4753
2026-02-13 02:49:35,063 | INFO | FT ep 8/10: train_loss=1.4753, val_acc=0.6733
2026-02-13 02:49:35,123 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 02:49:36,041 | INFO | FT 9 [1/10009] loss=1.5421
2026-02-13 02:50:24,125 | INFO | FT 9 [501/10009] loss=1.4565
2026-02-13 02:51:12,218 | INFO | FT 9 [1001/10009] loss=1.4469
2026-02-13 02:52:00,210 | INFO | FT 9 [1501/10009] loss=1.4427
2026-02-13 02:52:48,219 | INFO | FT 9 [2001/10009] loss=1.4422
2026-02-13 02:53:36,371 | INFO | FT 9 [2501/10009] loss=1.4407
2026-02-13 02:54:24,135 | INFO | FT 9 [3001/10009] loss=1.4393
2026-02-13 02:55:12,398 | INFO | FT 9 [3501/10009] loss=1.4400
2026-02-13 02:56:00,752 | INFO | FT 9 [4001/10009] loss=1.4392
2026-02-13 02:56:49,046 | INFO | FT 9 [4501/10009] loss=1.4399
2026-02-13 02:57:37,445 | INFO | FT 9 [5001/10009] loss=1.4388
2026-02-13 02:58:25,769 | INFO | FT 9 [5501/10009] loss=1.4376
2026-02-13 02:59:13,964 | INFO | FT 9 [6001/10009] loss=1.4368
2026-02-13 03:00:02,011 | INFO | FT 9 [6501/10009] loss=1.4355
2026-02-13 03:00:50,160 | INFO | FT 9 [7001/10009] loss=1.4351
2026-02-13 03:01:38,349 | INFO | FT 9 [7501/10009] loss=1.4343
2026-02-13 03:02:26,588 | INFO | FT 9 [8001/10009] loss=1.4341
2026-02-13 03:03:14,676 | INFO | FT 9 [8501/10009] loss=1.4327
2026-02-13 03:04:02,827 | INFO | FT 9 [9001/10009] loss=1.4320
2026-02-13 03:04:51,031 | INFO | FT 9 [9501/10009] loss=1.4317
2026-02-13 03:05:39,223 | INFO | FT 9 [10001/10009] loss=1.4317
2026-02-13 03:05:40,002 | INFO | FT 9 [10009/10009] loss=1.4317
2026-02-13 03:06:45,919 | INFO | FT ep 9/10: train_loss=1.4317, val_acc=0.6768
2026-02-13 03:06:45,970 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 03:06:46,921 | INFO | FT 10 [1/10009] loss=1.6310
2026-02-13 03:07:34,869 | INFO | FT 10 [501/10009] loss=1.4165
2026-02-13 03:08:23,018 | INFO | FT 10 [1001/10009] loss=1.4176
2026-02-13 03:09:11,241 | INFO | FT 10 [1501/10009] loss=1.4148
2026-02-13 03:09:59,358 | INFO | FT 10 [2001/10009] loss=1.4117
2026-02-13 03:10:47,091 | INFO | FT 10 [2501/10009] loss=1.4121
2026-02-13 03:11:35,460 | INFO | FT 10 [3001/10009] loss=1.4124
2026-02-13 03:12:23,874 | INFO | FT 10 [3501/10009] loss=1.4111
2026-02-13 03:13:12,285 | INFO | FT 10 [4001/10009] loss=1.4124
2026-02-13 03:14:00,631 | INFO | FT 10 [4501/10009] loss=1.4124
2026-02-13 03:14:49,035 | INFO | FT 10 [5001/10009] loss=1.4131
2026-02-13 03:15:37,147 | INFO | FT 10 [5501/10009] loss=1.4131
2026-02-13 03:16:25,141 | INFO | FT 10 [6001/10009] loss=1.4138
2026-02-13 03:17:13,256 | INFO | FT 10 [6501/10009] loss=1.4128
2026-02-13 03:18:01,355 | INFO | FT 10 [7001/10009] loss=1.4131
2026-02-13 03:18:49,683 | INFO | FT 10 [7501/10009] loss=1.4112
2026-02-13 03:19:37,753 | INFO | FT 10 [8001/10009] loss=1.4107
2026-02-13 03:20:25,992 | INFO | FT 10 [8501/10009] loss=1.4103
2026-02-13 03:21:13,955 | INFO | FT 10 [9001/10009] loss=1.4096
2026-02-13 03:22:01,963 | INFO | FT 10 [9501/10009] loss=1.4093
2026-02-13 03:22:49,985 | INFO | FT 10 [10001/10009] loss=1.4093
2026-02-13 03:22:50,777 | INFO | FT 10 [10009/10009] loss=1.4094
2026-02-13 03:23:58,136 | INFO | FT ep 10/10: train_loss=1.4094, val_acc=0.6791
2026-02-13 03:23:58,189 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 03:25:04,998 | INFO | ============================================================
2026-02-13 03:25:04,999 | INFO | Summary
2026-02-13 03:25:04,999 | INFO | ============================================================
2026-02-13 03:25:04,999 | INFO | Base MACs:    0.32G -> Pruned: 0.27G (84.8%)
2026-02-13 03:25:04,999 | INFO | Base Params:  3.50M -> Pruned: 2.92M (83.4%)
2026-02-13 03:25:04,999 | INFO | Original Acc: 0.7187
2026-02-13 03:25:04,999 | INFO | Final Acc:    0.6791
2026-02-13 03:25:04,999 | INFO | Best Acc:     0.6791
2026-02-13 03:25:05,045 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.8/vbp_final.pth
