2026-02-13 00:04:37,097 | INFO | Logging to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_imagenet.log
2026-02-13 00:04:37,098 | INFO | ============================================================
2026-02-13 00:04:37,098 | INFO | VBP ImageNet Reproduction Script
2026-02-13 00:04:37,098 | INFO | ============================================================
2026-02-13 00:04:37,098 | INFO |   model_type: cnn
2026-02-13 00:04:37,098 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/MNv2_TP/mobilenet_v2_weights.pth
2026-02-13 00:04:37,098 | INFO |   cnn_arch: mobilenet_v2
2026-02-13 00:04:37,098 | INFO |   pretrained: True
2026-02-13 00:04:37,098 | INFO |   interior_only: True
2026-02-13 00:04:37,098 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 00:04:37,098 | INFO |   train_batch_size: 64
2026-02-13 00:04:37,098 | INFO |   val_batch_size: 128
2026-02-13 00:04:37,098 | INFO |   num_workers: 4
2026-02-13 00:04:37,098 | INFO |   max_batches: 200
2026-02-13 00:04:37,098 | INFO |   keep_ratio: 0.9
2026-02-13 00:04:37,098 | INFO |   global_pruning: True
2026-02-13 00:04:37,098 | INFO |   max_pruning_ratio: 1.0
2026-02-13 00:04:37,098 | INFO |   norm_per_layer: False
2026-02-13 00:04:37,098 | INFO |   no_compensation: False
2026-02-13 00:04:37,098 | INFO |   no_recalib: False
2026-02-13 00:04:37,098 | INFO |   criterion: variance
2026-02-13 00:04:37,098 | INFO |   epochs_ft: 10
2026-02-13 00:04:37,099 | INFO |   lr_ft: 0.01
2026-02-13 00:04:37,099 | INFO |   opt_ft: sgd
2026-02-13 00:04:37,099 | INFO |   momentum_ft: 0.9
2026-02-13 00:04:37,099 | INFO |   wd_ft: 4e-05
2026-02-13 00:04:37,099 | INFO |   use_kd: True
2026-02-13 00:04:37,099 | INFO |   kd_alpha: 0.7
2026-02-13 00:04:37,099 | INFO |   kd_T: 2.0
2026-02-13 00:04:37,099 | INFO |   pat: False
2026-02-13 00:04:37,099 | INFO |   pat_steps: 5
2026-02-13 00:04:37,099 | INFO |   pat_epochs_per_step: 3
2026-02-13 00:04:37,099 | INFO |   var_loss_weight: 0.0
2026-02-13 00:04:37,099 | INFO |   sparse_mode: none
2026-02-13 00:04:37,099 | INFO |   epochs_sparse: 5
2026-02-13 00:04:37,099 | INFO |   lr_sparse: 0.0001
2026-02-13 00:04:37,099 | INFO |   l1_lambda: 0.0001
2026-02-13 00:04:37,099 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 00:04:37,099 | INFO |   disable_ddp: False
2026-02-13 00:04:37,099 | INFO |   local_rank: 0
2026-02-13 00:04:37,099 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9
2026-02-13 00:04:37,099 | INFO |   rank: 0
2026-02-13 00:04:37,099 | INFO |   world_size: 2
2026-02-13 00:04:37,100 | INFO | Loading ImageNet dataset...
2026-02-13 00:04:37,102 | INFO | Using cached sample lists for fast loading
2026-02-13 00:04:37,549 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 00:04:41,142 | INFO | Loaded mobilenet_v2 (pretrained=True)
2026-02-13 00:04:42,653 | INFO | Baseline: 0.32G MACs, 3.50M params
2026-02-13 00:04:42,653 | INFO | Evaluating original model...
2026-02-13 00:06:13,352 | INFO | Original accuracy: 0.7187, loss: 1.1474
2026-02-13 00:06:13,384 | INFO | Created teacher model for knowledge distillation
2026-02-13 00:06:13,384 | INFO | PAT: 1 steps, per_step_keep=0.9000, epochs_per_step=0, target_keep=0.9, criterion=variance
2026-02-13 00:06:13,384 | INFO | 
============================================================
2026-02-13 00:06:13,384 | INFO | PAT Step 1/1
2026-02-13 00:06:13,384 | INFO | ============================================================
2026-02-13 00:06:13,409 | INFO | Auto-detected 35 CNN target layers for stats
2026-02-13 00:06:13,409 | INFO | Collecting activation variance statistics...
2026-02-13 00:06:39,480 | INFO | Statistics collected for 35 layers
2026-02-13 00:06:39,480 | INFO |   Conv2d: mean_var=0.027800
2026-02-13 00:06:39,480 | INFO |   Conv2d: mean_var=0.381301
2026-02-13 00:06:39,480 | INFO |   Conv2d: mean_var=0.045557
2026-02-13 00:06:39,480 | INFO |   Conv2d: mean_var=0.290748
2026-02-13 00:06:39,480 | INFO |   Conv2d: mean_var=0.015336
2026-02-13 00:06:39,649 | INFO | Variance â€” entropy=0.8413, cv=2.1457, gini=0.8129
2026-02-13 00:06:39,964 | INFO | Pruning: per_step_prune=0.1000
2026-02-13 00:06:40,199 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 00:06:40,200 | INFO | Recalibrating BN running stats...
2026-02-13 00:06:41,966 | INFO | BN recalib [1/100]
2026-02-13 00:06:41,999 | INFO | BN recalib [6/100]
2026-02-13 00:06:42,444 | INFO | BN recalib [11/100]
2026-02-13 00:06:42,823 | INFO | BN recalib [16/100]
2026-02-13 00:06:43,770 | INFO | BN recalib [21/100]
2026-02-13 00:06:44,487 | INFO | BN recalib [26/100]
2026-02-13 00:06:44,863 | INFO | BN recalib [31/100]
2026-02-13 00:06:45,286 | INFO | BN recalib [36/100]
2026-02-13 00:06:45,816 | INFO | BN recalib [41/100]
2026-02-13 00:06:46,415 | INFO | BN recalib [46/100]
2026-02-13 00:06:46,812 | INFO | BN recalib [51/100]
2026-02-13 00:06:47,269 | INFO | BN recalib [56/100]
2026-02-13 00:06:47,759 | INFO | BN recalib [61/100]
2026-02-13 00:06:48,349 | INFO | BN recalib [66/100]
2026-02-13 00:06:48,923 | INFO | BN recalib [71/100]
2026-02-13 00:06:49,301 | INFO | BN recalib [76/100]
2026-02-13 00:06:49,694 | INFO | BN recalib [81/100]
2026-02-13 00:06:50,761 | INFO | BN recalib [86/100]
2026-02-13 00:06:51,424 | INFO | BN recalib [91/100]
2026-02-13 00:06:51,790 | INFO | BN recalib [96/100]
2026-02-13 00:06:52,317 | INFO | BN recalib [100/100]
2026-02-13 00:06:52,764 | INFO | BN recalibration done (100 batches)
2026-02-13 00:08:23,160 | INFO | Step 1 retention: acc=0.2562, loss=3.4295
2026-02-13 00:08:23,160 | INFO |   cumulative_keep=0.9000, MACs=0.30G, params=3.14M
2026-02-13 00:08:23,161 | INFO | 
Post-prune fine-tuning for 10 epochs (sgd, lr=0.01, wd=4e-05)...
2026-02-13 00:08:24,683 | INFO | FT 1 [1/10009] loss=4.3907
2026-02-13 00:09:32,399 | INFO | FT 1 [501/10009] loss=3.1035
2026-02-13 00:10:40,337 | INFO | FT 1 [1001/10009] loss=2.8841
2026-02-13 00:11:50,324 | INFO | FT 1 [1501/10009] loss=2.7548
2026-02-13 00:12:59,533 | INFO | FT 1 [2001/10009] loss=2.6691
2026-02-13 00:14:04,350 | INFO | FT 1 [2501/10009] loss=2.6098
2026-02-13 00:15:12,028 | INFO | FT 1 [3001/10009] loss=2.5598
2026-02-13 00:16:17,804 | INFO | FT 1 [3501/10009] loss=2.5210
2026-02-13 00:17:26,041 | INFO | FT 1 [4001/10009] loss=2.4844
2026-02-13 00:18:33,904 | INFO | FT 1 [4501/10009] loss=2.4565
2026-02-13 00:19:39,306 | INFO | FT 1 [5001/10009] loss=2.4301
2026-02-13 00:20:46,617 | INFO | FT 1 [5501/10009] loss=2.4038
2026-02-13 00:21:51,888 | INFO | FT 1 [6001/10009] loss=2.3824
2026-02-13 00:22:56,688 | INFO | FT 1 [6501/10009] loss=2.3615
2026-02-13 00:24:02,109 | INFO | FT 1 [7001/10009] loss=2.3428
2026-02-13 00:25:07,341 | INFO | FT 1 [7501/10009] loss=2.3250
2026-02-13 00:26:12,805 | INFO | FT 1 [8001/10009] loss=2.3099
2026-02-13 00:27:18,739 | INFO | FT 1 [8501/10009] loss=2.2943
2026-02-13 00:28:23,858 | INFO | FT 1 [9001/10009] loss=2.2803
2026-02-13 00:29:26,508 | INFO | FT 1 [9501/10009] loss=2.2670
2026-02-13 00:30:28,791 | INFO | FT 1 [10001/10009] loss=2.2547
2026-02-13 00:30:29,763 | INFO | FT 1 [10009/10009] loss=2.2544
2026-02-13 00:31:55,544 | INFO | FT ep 1/10: train_loss=2.2544, val_acc=0.5912
2026-02-13 00:31:55,611 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 00:31:56,516 | INFO | FT 2 [1/10009] loss=1.8128
2026-02-13 00:32:59,799 | INFO | FT 2 [501/10009] loss=1.9725
2026-02-13 00:34:04,528 | INFO | FT 2 [1001/10009] loss=1.9600
2026-02-13 00:35:10,498 | INFO | FT 2 [1501/10009] loss=1.9629
2026-02-13 00:36:17,233 | INFO | FT 2 [2001/10009] loss=1.9598
2026-02-13 00:37:22,003 | INFO | FT 2 [2501/10009] loss=1.9607
2026-02-13 00:38:28,017 | INFO | FT 2 [3001/10009] loss=1.9545
2026-02-13 00:39:35,698 | INFO | FT 2 [3501/10009] loss=1.9485
2026-02-13 00:40:42,829 | INFO | FT 2 [4001/10009] loss=1.9442
2026-02-13 00:41:50,734 | INFO | FT 2 [4501/10009] loss=1.9402
2026-02-13 00:42:58,650 | INFO | FT 2 [5001/10009] loss=1.9373
2026-02-13 00:44:06,623 | INFO | FT 2 [5501/10009] loss=1.9340
2026-02-13 00:45:13,471 | INFO | FT 2 [6001/10009] loss=1.9309
2026-02-13 00:46:20,375 | INFO | FT 2 [6501/10009] loss=1.9272
2026-02-13 00:47:27,253 | INFO | FT 2 [7001/10009] loss=1.9231
2026-02-13 00:48:32,731 | INFO | FT 2 [7501/10009] loss=1.9208
2026-02-13 00:49:39,698 | INFO | FT 2 [8001/10009] loss=1.9174
2026-02-13 00:50:46,309 | INFO | FT 2 [8501/10009] loss=1.9141
2026-02-13 00:51:50,919 | INFO | FT 2 [9001/10009] loss=1.9116
2026-02-13 00:52:58,222 | INFO | FT 2 [9501/10009] loss=1.9081
2026-02-13 00:54:04,999 | INFO | FT 2 [10001/10009] loss=1.9048
2026-02-13 00:54:05,907 | INFO | FT 2 [10009/10009] loss=1.9048
2026-02-13 00:55:32,579 | INFO | FT ep 2/10: train_loss=1.9048, val_acc=0.6157
2026-02-13 00:55:32,649 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 00:55:33,654 | INFO | FT 3 [1/10009] loss=1.8629
2026-02-13 00:56:37,115 | INFO | FT 3 [501/10009] loss=1.7977
2026-02-13 00:57:42,299 | INFO | FT 3 [1001/10009] loss=1.7987
2026-02-13 00:58:47,775 | INFO | FT 3 [1501/10009] loss=1.7989
2026-02-13 00:59:53,698 | INFO | FT 3 [2001/10009] loss=1.7976
2026-02-13 01:01:00,131 | INFO | FT 3 [2501/10009] loss=1.7938
2026-02-13 01:02:08,432 | INFO | FT 3 [3001/10009] loss=1.7896
2026-02-13 01:03:14,516 | INFO | FT 3 [3501/10009] loss=1.7877
2026-02-13 01:04:22,202 | INFO | FT 3 [4001/10009] loss=1.7874
2026-02-13 01:05:30,529 | INFO | FT 3 [4501/10009] loss=1.7855
2026-02-13 01:06:38,038 | INFO | FT 3 [5001/10009] loss=1.7866
2026-02-13 01:07:44,884 | INFO | FT 3 [5501/10009] loss=1.7864
2026-02-13 01:08:51,231 | INFO | FT 3 [6001/10009] loss=1.7841
2026-02-13 01:09:58,547 | INFO | FT 3 [6501/10009] loss=1.7830
2026-02-13 01:11:06,298 | INFO | FT 3 [7001/10009] loss=1.7816
2026-02-13 01:12:14,890 | INFO | FT 3 [7501/10009] loss=1.7812
2026-02-13 01:13:21,437 | INFO | FT 3 [8001/10009] loss=1.7788
2026-02-13 01:14:27,574 | INFO | FT 3 [8501/10009] loss=1.7762
2026-02-13 01:15:33,955 | INFO | FT 3 [9001/10009] loss=1.7734
2026-02-13 01:16:41,620 | INFO | FT 3 [9501/10009] loss=1.7718
2026-02-13 01:17:46,805 | INFO | FT 3 [10001/10009] loss=1.7701
2026-02-13 01:17:47,741 | INFO | FT 3 [10009/10009] loss=1.7701
2026-02-13 01:19:15,131 | INFO | FT ep 3/10: train_loss=1.7701, val_acc=0.6336
2026-02-13 01:19:15,197 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 01:19:16,298 | INFO | FT 4 [1/10009] loss=1.7159
2026-02-13 01:20:21,287 | INFO | FT 4 [501/10009] loss=1.6834
2026-02-13 01:21:25,574 | INFO | FT 4 [1001/10009] loss=1.6906
2026-02-13 01:22:29,838 | INFO | FT 4 [1501/10009] loss=1.6878
2026-02-13 01:23:33,586 | INFO | FT 4 [2001/10009] loss=1.6864
2026-02-13 01:24:38,915 | INFO | FT 4 [2501/10009] loss=1.6836
2026-02-13 01:25:46,276 | INFO | FT 4 [3001/10009] loss=1.6851
2026-02-13 01:26:51,354 | INFO | FT 4 [3501/10009] loss=1.6853
2026-02-13 01:27:56,834 | INFO | FT 4 [4001/10009] loss=1.6835
2026-02-13 01:29:01,023 | INFO | FT 4 [4501/10009] loss=1.6827
2026-02-13 01:30:06,354 | INFO | FT 4 [5001/10009] loss=1.6828
2026-02-13 01:31:12,151 | INFO | FT 4 [5501/10009] loss=1.6821
2026-02-13 01:32:17,129 | INFO | FT 4 [6001/10009] loss=1.6823
2026-02-13 01:33:22,834 | INFO | FT 4 [6501/10009] loss=1.6821
2026-02-13 01:34:29,158 | INFO | FT 4 [7001/10009] loss=1.6799
2026-02-13 01:35:34,755 | INFO | FT 4 [7501/10009] loss=1.6778
2026-02-13 01:36:42,198 | INFO | FT 4 [8001/10009] loss=1.6753
2026-02-13 01:37:47,171 | INFO | FT 4 [8501/10009] loss=1.6730
2026-02-13 01:38:54,508 | INFO | FT 4 [9001/10009] loss=1.6708
2026-02-13 01:39:59,930 | INFO | FT 4 [9501/10009] loss=1.6691
2026-02-13 01:41:04,501 | INFO | FT 4 [10001/10009] loss=1.6676
2026-02-13 01:41:05,429 | INFO | FT 4 [10009/10009] loss=1.6677
2026-02-13 01:42:30,534 | INFO | FT ep 4/10: train_loss=1.6677, val_acc=0.6474
2026-02-13 01:42:30,604 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 01:42:32,094 | INFO | FT 5 [1/10009] loss=1.4996
2026-02-13 01:43:39,498 | INFO | FT 5 [501/10009] loss=1.5899
2026-02-13 01:44:40,962 | INFO | FT 5 [1001/10009] loss=1.5958
2026-02-13 01:45:48,036 | INFO | FT 5 [1501/10009] loss=1.5963
2026-02-13 01:46:50,573 | INFO | FT 5 [2001/10009] loss=1.5977
2026-02-13 01:47:51,813 | INFO | FT 5 [2501/10009] loss=1.5952
2026-02-13 01:48:54,898 | INFO | FT 5 [3001/10009] loss=1.5925
2026-02-13 01:49:57,574 | INFO | FT 5 [3501/10009] loss=1.5933
2026-02-13 01:51:01,824 | INFO | FT 5 [4001/10009] loss=1.5932
2026-02-13 01:52:05,225 | INFO | FT 5 [4501/10009] loss=1.5915
2026-02-13 01:53:08,411 | INFO | FT 5 [5001/10009] loss=1.5904
2026-02-13 01:54:12,320 | INFO | FT 5 [5501/10009] loss=1.5902
2026-02-13 01:55:15,264 | INFO | FT 5 [6001/10009] loss=1.5892
2026-02-13 01:56:20,462 | INFO | FT 5 [6501/10009] loss=1.5876
2026-02-13 01:57:24,031 | INFO | FT 5 [7001/10009] loss=1.5855
2026-02-13 01:58:29,116 | INFO | FT 5 [7501/10009] loss=1.5839
2026-02-13 01:59:34,181 | INFO | FT 5 [8001/10009] loss=1.5817
2026-02-13 02:00:38,474 | INFO | FT 5 [8501/10009] loss=1.5803
2026-02-13 02:01:42,243 | INFO | FT 5 [9001/10009] loss=1.5790
2026-02-13 02:02:44,202 | INFO | FT 5 [9501/10009] loss=1.5783
2026-02-13 02:03:47,264 | INFO | FT 5 [10001/10009] loss=1.5764
2026-02-13 02:03:48,303 | INFO | FT 5 [10009/10009] loss=1.5764
2026-02-13 02:05:15,853 | INFO | FT ep 5/10: train_loss=1.5764, val_acc=0.6613
2026-02-13 02:05:15,912 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 02:05:16,951 | INFO | FT 6 [1/10009] loss=1.4703
2026-02-13 02:06:18,141 | INFO | FT 6 [501/10009] loss=1.5116
2026-02-13 02:07:20,832 | INFO | FT 6 [1001/10009] loss=1.5234
2026-02-13 02:08:22,682 | INFO | FT 6 [1501/10009] loss=1.5166
2026-02-13 02:09:24,453 | INFO | FT 6 [2001/10009] loss=1.5153
2026-02-13 02:10:26,395 | INFO | FT 6 [2501/10009] loss=1.5132
2026-02-13 02:11:28,460 | INFO | FT 6 [3001/10009] loss=1.5111
2026-02-13 02:12:30,638 | INFO | FT 6 [3501/10009] loss=1.5109
2026-02-13 02:13:33,098 | INFO | FT 6 [4001/10009] loss=1.5113
2026-02-13 02:14:35,184 | INFO | FT 6 [4501/10009] loss=1.5113
2026-02-13 02:15:37,637 | INFO | FT 6 [5001/10009] loss=1.5096
2026-02-13 02:16:39,369 | INFO | FT 6 [5501/10009] loss=1.5074
2026-02-13 02:17:41,399 | INFO | FT 6 [6001/10009] loss=1.5063
2026-02-13 02:18:43,345 | INFO | FT 6 [6501/10009] loss=1.5048
2026-02-13 02:19:47,584 | INFO | FT 6 [7001/10009] loss=1.5042
2026-02-13 02:20:55,960 | INFO | FT 6 [7501/10009] loss=1.5031
2026-02-13 02:22:04,516 | INFO | FT 6 [8001/10009] loss=1.5024
2026-02-13 02:23:14,198 | INFO | FT 6 [8501/10009] loss=1.5003
2026-02-13 02:24:24,410 | INFO | FT 6 [9001/10009] loss=1.4981
2026-02-13 02:25:31,750 | INFO | FT 6 [9501/10009] loss=1.4964
2026-02-13 02:26:38,964 | INFO | FT 6 [10001/10009] loss=1.4949
2026-02-13 02:26:40,085 | INFO | FT 6 [10009/10009] loss=1.4948
2026-02-13 02:28:06,520 | INFO | FT ep 6/10: train_loss=1.4948, val_acc=0.6717
2026-02-13 02:28:06,586 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 02:28:07,530 | INFO | FT 7 [1/10009] loss=1.9350
2026-02-13 02:29:13,974 | INFO | FT 7 [501/10009] loss=1.4566
2026-02-13 02:30:20,100 | INFO | FT 7 [1001/10009] loss=1.4433
2026-02-13 02:31:26,156 | INFO | FT 7 [1501/10009] loss=1.4383
2026-02-13 02:32:32,893 | INFO | FT 7 [2001/10009] loss=1.4348
2026-02-13 02:33:38,003 | INFO | FT 7 [2501/10009] loss=1.4363
2026-02-13 02:34:44,736 | INFO | FT 7 [3001/10009] loss=1.4326
2026-02-13 02:35:53,393 | INFO | FT 7 [3501/10009] loss=1.4312
2026-02-13 02:37:01,000 | INFO | FT 7 [4001/10009] loss=1.4333
2026-02-13 02:38:09,980 | INFO | FT 7 [4501/10009] loss=1.4305
2026-02-13 02:39:17,588 | INFO | FT 7 [5001/10009] loss=1.4299
2026-02-13 02:40:26,841 | INFO | FT 7 [5501/10009] loss=1.4286
2026-02-13 02:41:33,198 | INFO | FT 7 [6001/10009] loss=1.4272
2026-02-13 02:42:41,518 | INFO | FT 7 [6501/10009] loss=1.4260
2026-02-13 02:43:48,616 | INFO | FT 7 [7001/10009] loss=1.4254
2026-02-13 02:44:56,866 | INFO | FT 7 [7501/10009] loss=1.4235
2026-02-13 02:46:04,400 | INFO | FT 7 [8001/10009] loss=1.4228
2026-02-13 02:47:10,837 | INFO | FT 7 [8501/10009] loss=1.4213
2026-02-13 02:48:15,598 | INFO | FT 7 [9001/10009] loss=1.4210
2026-02-13 02:49:21,069 | INFO | FT 7 [9501/10009] loss=1.4201
2026-02-13 02:50:28,891 | INFO | FT 7 [10001/10009] loss=1.4187
2026-02-13 02:50:30,141 | INFO | FT 7 [10009/10009] loss=1.4186
2026-02-13 02:51:55,927 | INFO | FT ep 7/10: train_loss=1.4186, val_acc=0.6780
2026-02-13 02:51:55,998 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 02:51:56,932 | INFO | FT 8 [1/10009] loss=1.4033
2026-02-13 02:53:02,260 | INFO | FT 8 [501/10009] loss=1.3745
2026-02-13 02:54:07,239 | INFO | FT 8 [1001/10009] loss=1.3697
2026-02-13 02:55:13,183 | INFO | FT 8 [1501/10009] loss=1.3674
2026-02-13 02:56:16,452 | INFO | FT 8 [2001/10009] loss=1.3721
2026-02-13 02:57:21,796 | INFO | FT 8 [2501/10009] loss=1.3709
2026-02-13 02:58:26,098 | INFO | FT 8 [3001/10009] loss=1.3705
2026-02-13 02:59:29,487 | INFO | FT 8 [3501/10009] loss=1.3699
2026-02-13 03:00:36,426 | INFO | FT 8 [4001/10009] loss=1.3690
2026-02-13 03:01:43,608 | INFO | FT 8 [4501/10009] loss=1.3670
2026-02-13 03:02:49,331 | INFO | FT 8 [5001/10009] loss=1.3652
2026-02-13 03:03:55,699 | INFO | FT 8 [5501/10009] loss=1.3651
2026-02-13 03:05:01,663 | INFO | FT 8 [6001/10009] loss=1.3633
2026-02-13 03:06:07,984 | INFO | FT 8 [6501/10009] loss=1.3621
2026-02-13 03:07:15,798 | INFO | FT 8 [7001/10009] loss=1.3611
2026-02-13 03:08:22,504 | INFO | FT 8 [7501/10009] loss=1.3607
2026-02-13 03:09:30,446 | INFO | FT 8 [8001/10009] loss=1.3593
2026-02-13 03:10:36,733 | INFO | FT 8 [8501/10009] loss=1.3593
2026-02-13 03:11:44,402 | INFO | FT 8 [9001/10009] loss=1.3583
2026-02-13 03:12:51,834 | INFO | FT 8 [9501/10009] loss=1.3585
2026-02-13 03:13:57,824 | INFO | FT 8 [10001/10009] loss=1.3574
2026-02-13 03:13:58,767 | INFO | FT 8 [10009/10009] loss=1.3573
2026-02-13 03:15:23,688 | INFO | FT ep 8/10: train_loss=1.3573, val_acc=0.6858
2026-02-13 03:15:23,746 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 03:15:24,623 | INFO | FT 9 [1/10009] loss=1.3910
2026-02-13 03:16:30,721 | INFO | FT 9 [501/10009] loss=1.3433
2026-02-13 03:17:35,858 | INFO | FT 9 [1001/10009] loss=1.3305
2026-02-13 03:18:42,335 | INFO | FT 9 [1501/10009] loss=1.3271
2026-02-13 03:19:46,598 | INFO | FT 9 [2001/10009] loss=1.3300
2026-02-13 03:20:52,874 | INFO | FT 9 [2501/10009] loss=1.3294
2026-02-13 03:21:59,523 | INFO | FT 9 [3001/10009] loss=1.3269
2026-02-13 03:23:05,384 | INFO | FT 9 [3501/10009] loss=1.3263
2026-02-13 03:24:12,037 | INFO | FT 9 [4001/10009] loss=1.3246
2026-02-13 03:25:18,109 | INFO | FT 9 [4501/10009] loss=1.3253
2026-02-13 03:26:26,269 | INFO | FT 9 [5001/10009] loss=1.3239
2026-02-13 03:27:32,022 | INFO | FT 9 [5501/10009] loss=1.3227
2026-02-13 03:28:39,770 | INFO | FT 9 [6001/10009] loss=1.3215
2026-02-13 03:29:46,918 | INFO | FT 9 [6501/10009] loss=1.3204
2026-02-13 03:30:54,054 | INFO | FT 9 [7001/10009] loss=1.3201
2026-02-13 03:32:01,392 | INFO | FT 9 [7501/10009] loss=1.3194
2026-02-13 03:33:08,397 | INFO | FT 9 [8001/10009] loss=1.3187
2026-02-13 03:34:13,600 | INFO | FT 9 [8501/10009] loss=1.3181
2026-02-13 03:35:21,108 | INFO | FT 9 [9001/10009] loss=1.3175
2026-02-13 03:36:26,732 | INFO | FT 9 [9501/10009] loss=1.3170
2026-02-13 03:37:31,900 | INFO | FT 9 [10001/10009] loss=1.3167
2026-02-13 03:37:32,936 | INFO | FT 9 [10009/10009] loss=1.3167
2026-02-13 03:38:58,813 | INFO | FT ep 9/10: train_loss=1.3167, val_acc=0.6906
2026-02-13 03:38:58,884 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 03:38:59,856 | INFO | FT 10 [1/10009] loss=1.2020
2026-02-13 03:40:06,109 | INFO | FT 10 [501/10009] loss=1.2991
2026-02-13 03:41:13,763 | INFO | FT 10 [1001/10009] loss=1.2965
2026-02-13 03:42:20,213 | INFO | FT 10 [1501/10009] loss=1.2998
2026-02-13 03:43:27,907 | INFO | FT 10 [2001/10009] loss=1.3000
2026-02-13 03:44:34,569 | INFO | FT 10 [2501/10009] loss=1.2993
2026-02-13 03:45:41,641 | INFO | FT 10 [3001/10009] loss=1.3006
2026-02-13 03:46:48,846 | INFO | FT 10 [3501/10009] loss=1.3000
2026-02-13 03:47:56,872 | INFO | FT 10 [4001/10009] loss=1.2994
2026-02-13 03:49:02,913 | INFO | FT 10 [4501/10009] loss=1.2992
2026-02-13 03:50:12,263 | INFO | FT 10 [5001/10009] loss=1.2989
2026-02-13 03:51:20,451 | INFO | FT 10 [5501/10009] loss=1.2984
2026-02-13 03:52:30,528 | INFO | FT 10 [6001/10009] loss=1.2994
2026-02-13 03:53:38,361 | INFO | FT 10 [6501/10009] loss=1.2986
2026-02-13 03:54:46,832 | INFO | FT 10 [7001/10009] loss=1.2989
2026-02-13 03:55:55,343 | INFO | FT 10 [7501/10009] loss=1.2972
2026-02-13 03:57:02,645 | INFO | FT 10 [8001/10009] loss=1.2971
2026-02-13 03:58:10,793 | INFO | FT 10 [8501/10009] loss=1.2966
2026-02-13 03:59:16,611 | INFO | FT 10 [9001/10009] loss=1.2959
2026-02-13 04:00:23,498 | INFO | FT 10 [9501/10009] loss=1.2958
2026-02-13 04:01:27,363 | INFO | FT 10 [10001/10009] loss=1.2959
2026-02-13 04:01:28,341 | INFO | FT 10 [10009/10009] loss=1.2958
2026-02-13 04:02:53,965 | INFO | FT ep 10/10: train_loss=1.2958, val_acc=0.6909
2026-02-13 04:02:54,028 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 04:04:19,427 | INFO | ============================================================
2026-02-13 04:04:19,509 | INFO | Summary
2026-02-13 04:04:19,509 | INFO | ============================================================
2026-02-13 04:04:19,509 | INFO | Base MACs:    0.32G -> Pruned: 0.30G (93.3%)
2026-02-13 04:04:19,509 | INFO | Base Params:  3.50M -> Pruned: 3.14M (89.5%)
2026-02-13 04:04:19,510 | INFO | Original Acc: 0.7187
2026-02-13 04:04:19,510 | INFO | Final Acc:    0.6909
2026-02-13 04:04:19,510 | INFO | Best Acc:     0.6909
2026-02-13 04:04:19,572 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global_lrv1/kr_0.9/vbp_final.pth
