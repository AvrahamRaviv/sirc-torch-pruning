2026-02-13 05:29:44,034 | INFO | Logging to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_imagenet.log
2026-02-13 05:29:44,034 | INFO | ============================================================
2026-02-13 05:29:44,034 | INFO | VBP ImageNet Reproduction Script
2026-02-13 05:29:44,034 | INFO | ============================================================
2026-02-13 05:29:44,034 | INFO |   model_type: cnn
2026-02-13 05:29:44,034 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/MNv2_TP/mobilenet_v2_weights.pth
2026-02-13 05:29:44,035 | INFO |   cnn_arch: mobilenet_v2
2026-02-13 05:29:44,035 | INFO |   pretrained: True
2026-02-13 05:29:44,035 | INFO |   interior_only: True
2026-02-13 05:29:44,035 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 05:29:44,035 | INFO |   train_batch_size: 64
2026-02-13 05:29:44,035 | INFO |   val_batch_size: 128
2026-02-13 05:29:44,035 | INFO |   num_workers: 4
2026-02-13 05:29:44,035 | INFO |   max_batches: 200
2026-02-13 05:29:44,035 | INFO |   keep_ratio: 0.7
2026-02-13 05:29:44,035 | INFO |   global_pruning: True
2026-02-13 05:29:44,035 | INFO |   max_pruning_ratio: 1.0
2026-02-13 05:29:44,035 | INFO |   norm_per_layer: False
2026-02-13 05:29:44,035 | INFO |   no_compensation: False
2026-02-13 05:29:44,035 | INFO |   no_recalib: False
2026-02-13 05:29:44,035 | INFO |   criterion: variance
2026-02-13 05:29:44,035 | INFO |   epochs_ft: 10
2026-02-13 05:29:44,035 | INFO |   lr_ft: 0.0001
2026-02-13 05:29:44,035 | INFO |   opt_ft: adamw
2026-02-13 05:29:44,035 | INFO |   momentum_ft: 0.9
2026-02-13 05:29:44,035 | INFO |   wd_ft: None
2026-02-13 05:29:44,035 | INFO |   use_kd: True
2026-02-13 05:29:44,035 | INFO |   kd_alpha: 0.7
2026-02-13 05:29:44,036 | INFO |   kd_T: 2.0
2026-02-13 05:29:44,036 | INFO |   pat: False
2026-02-13 05:29:44,036 | INFO |   pat_steps: 5
2026-02-13 05:29:44,036 | INFO |   pat_epochs_per_step: 3
2026-02-13 05:29:44,036 | INFO |   var_loss_weight: 0.0
2026-02-13 05:29:44,036 | INFO |   sparse_mode: none
2026-02-13 05:29:44,036 | INFO |   epochs_sparse: 5
2026-02-13 05:29:44,036 | INFO |   lr_sparse: 0.0001
2026-02-13 05:29:44,036 | INFO |   l1_lambda: 0.0001
2026-02-13 05:29:44,036 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 05:29:44,036 | INFO |   disable_ddp: False
2026-02-13 05:29:44,036 | INFO |   local_rank: 0
2026-02-13 05:29:44,036 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7
2026-02-13 05:29:44,036 | INFO |   rank: 0
2026-02-13 05:29:44,036 | INFO |   world_size: 2
2026-02-13 05:29:44,036 | INFO | Loading ImageNet dataset...
2026-02-13 05:29:44,036 | INFO | Using cached sample lists for fast loading
2026-02-13 05:29:44,379 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 05:29:46,078 | INFO | Loaded mobilenet_v2 (pretrained=True)
2026-02-13 05:29:47,511 | INFO | Baseline: 0.32G MACs, 3.50M params
2026-02-13 05:29:47,511 | INFO | Evaluating original model...
2026-02-13 05:30:53,356 | INFO | Original accuracy: 0.7187, loss: 1.1474
2026-02-13 05:30:53,384 | INFO | Created teacher model for knowledge distillation
2026-02-13 05:30:53,384 | INFO | PAT: 1 steps, per_step_keep=0.7000, epochs_per_step=0, target_keep=0.7, criterion=variance
2026-02-13 05:30:53,384 | INFO | 
============================================================
2026-02-13 05:30:53,384 | INFO | PAT Step 1/1
2026-02-13 05:30:53,384 | INFO | ============================================================
2026-02-13 05:30:53,408 | INFO | Auto-detected 35 CNN target layers for stats
2026-02-13 05:30:53,409 | INFO | Collecting activation variance statistics...
2026-02-13 05:31:08,934 | INFO | Statistics collected for 35 layers
2026-02-13 05:31:08,935 | INFO |   Conv2d: mean_var=0.027715
2026-02-13 05:31:08,935 | INFO |   Conv2d: mean_var=0.381142
2026-02-13 05:31:08,935 | INFO |   Conv2d: mean_var=0.045527
2026-02-13 05:31:08,935 | INFO |   Conv2d: mean_var=0.290581
2026-02-13 05:31:08,935 | INFO |   Conv2d: mean_var=0.015321
2026-02-13 05:31:09,128 | INFO | Variance â€” entropy=0.8414, cv=2.1451, gini=0.8129
2026-02-13 05:31:09,550 | INFO | Pruning: per_step_prune=0.3000
2026-02-13 05:31:09,814 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 05:31:09,814 | INFO | Recalibrating BN running stats...
2026-02-13 05:31:10,581 | INFO | BN recalib [1/100]
2026-02-13 05:31:10,815 | INFO | BN recalib [6/100]
2026-02-13 05:31:11,569 | INFO | BN recalib [11/100]
2026-02-13 05:31:11,898 | INFO | BN recalib [16/100]
2026-02-13 05:31:12,441 | INFO | BN recalib [21/100]
2026-02-13 05:31:12,872 | INFO | BN recalib [26/100]
2026-02-13 05:31:13,235 | INFO | BN recalib [31/100]
2026-02-13 05:31:13,533 | INFO | BN recalib [36/100]
2026-02-13 05:31:14,194 | INFO | BN recalib [41/100]
2026-02-13 05:31:14,500 | INFO | BN recalib [46/100]
2026-02-13 05:31:14,842 | INFO | BN recalib [51/100]
2026-02-13 05:31:15,131 | INFO | BN recalib [56/100]
2026-02-13 05:31:15,730 | INFO | BN recalib [61/100]
2026-02-13 05:31:16,032 | INFO | BN recalib [66/100]
2026-02-13 05:31:16,340 | INFO | BN recalib [71/100]
2026-02-13 05:31:16,630 | INFO | BN recalib [76/100]
2026-02-13 05:31:17,241 | INFO | BN recalib [81/100]
2026-02-13 05:31:17,530 | INFO | BN recalib [86/100]
2026-02-13 05:31:17,850 | INFO | BN recalib [91/100]
2026-02-13 05:31:18,155 | INFO | BN recalib [96/100]
2026-02-13 05:31:18,528 | INFO | BN recalib [100/100]
2026-02-13 05:31:19,177 | INFO | BN recalibration done (100 batches)
2026-02-13 05:32:24,909 | INFO | Step 1 retention: acc=0.0134, loss=7.4025
2026-02-13 05:32:24,910 | INFO |   cumulative_keep=0.7000, MACs=0.24G, params=2.73M
2026-02-13 05:32:24,910 | INFO | 
Post-prune fine-tuning for 10 epochs (adamw, lr=0.0001, wd=0.01)...
2026-02-13 05:32:26,020 | INFO | FT 1 [1/10009] loss=9.4087
2026-02-13 05:33:16,425 | INFO | FT 1 [501/10009] loss=4.4682
2026-02-13 05:34:06,325 | INFO | FT 1 [1001/10009] loss=3.8586
2026-02-13 05:34:56,124 | INFO | FT 1 [1501/10009] loss=3.5287
2026-02-13 05:35:45,900 | INFO | FT 1 [2001/10009] loss=3.3188
2026-02-13 05:36:35,525 | INFO | FT 1 [2501/10009] loss=3.1663
2026-02-13 05:37:25,527 | INFO | FT 1 [3001/10009] loss=3.0485
2026-02-13 05:38:15,224 | INFO | FT 1 [3501/10009] loss=2.9548
2026-02-13 05:39:05,124 | INFO | FT 1 [4001/10009] loss=2.8753
2026-02-13 05:39:54,624 | INFO | FT 1 [4501/10009] loss=2.8097
2026-02-13 05:40:44,347 | INFO | FT 1 [5001/10009] loss=2.7529
2026-02-13 05:41:34,024 | INFO | FT 1 [5501/10009] loss=2.7026
2026-02-13 05:42:23,392 | INFO | FT 1 [6001/10009] loss=2.6589
2026-02-13 05:43:12,824 | INFO | FT 1 [6501/10009] loss=2.6194
2026-02-13 05:44:02,328 | INFO | FT 1 [7001/10009] loss=2.5841
2026-02-13 05:44:51,428 | INFO | FT 1 [7501/10009] loss=2.5525
2026-02-13 05:45:41,136 | INFO | FT 1 [8001/10009] loss=2.5244
2026-02-13 05:46:30,524 | INFO | FT 1 [8501/10009] loss=2.4975
2026-02-13 05:47:20,024 | INFO | FT 1 [9001/10009] loss=2.4723
2026-02-13 05:48:09,423 | INFO | FT 1 [9501/10009] loss=2.4491
2026-02-13 05:48:58,525 | INFO | FT 1 [10001/10009] loss=2.4276
2026-02-13 05:48:59,287 | INFO | FT 1 [10009/10009] loss=2.4272
2026-02-13 05:50:06,420 | INFO | FT ep 1/10: train_loss=2.4272, val_acc=0.5953
2026-02-13 05:50:06,468 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 05:50:07,424 | INFO | FT 2 [1/10009] loss=1.6599
2026-02-13 05:50:57,224 | INFO | FT 2 [501/10009] loss=1.9729
2026-02-13 05:51:47,124 | INFO | FT 2 [1001/10009] loss=1.9725
2026-02-13 05:52:37,024 | INFO | FT 2 [1501/10009] loss=1.9739
2026-02-13 05:53:27,024 | INFO | FT 2 [2001/10009] loss=1.9690
2026-02-13 05:54:16,727 | INFO | FT 2 [2501/10009] loss=1.9658
2026-02-13 05:55:06,624 | INFO | FT 2 [3001/10009] loss=1.9600
2026-02-13 05:55:56,468 | INFO | FT 2 [3501/10009] loss=1.9567
2026-02-13 05:56:46,424 | INFO | FT 2 [4001/10009] loss=1.9532
2026-02-13 05:57:36,123 | INFO | FT 2 [4501/10009] loss=1.9492
2026-02-13 05:58:25,624 | INFO | FT 2 [5001/10009] loss=1.9456
2026-02-13 05:59:14,924 | INFO | FT 2 [5501/10009] loss=1.9419
2026-02-13 06:00:04,124 | INFO | FT 2 [6001/10009] loss=1.9377
2026-02-13 06:00:53,126 | INFO | FT 2 [6501/10009] loss=1.9347
2026-02-13 06:01:42,815 | INFO | FT 2 [7001/10009] loss=1.9303
2026-02-13 06:02:32,446 | INFO | FT 2 [7501/10009] loss=1.9274
2026-02-13 06:03:21,824 | INFO | FT 2 [8001/10009] loss=1.9247
2026-02-13 06:04:11,242 | INFO | FT 2 [8501/10009] loss=1.9217
2026-02-13 06:05:00,531 | INFO | FT 2 [9001/10009] loss=1.9196
2026-02-13 06:05:50,022 | INFO | FT 2 [9501/10009] loss=1.9170
2026-02-13 06:06:39,424 | INFO | FT 2 [10001/10009] loss=1.9139
2026-02-13 06:06:40,189 | INFO | FT 2 [10009/10009] loss=1.9138
2026-02-13 06:07:51,479 | INFO | FT ep 2/10: train_loss=1.9138, val_acc=0.6183
2026-02-13 06:07:51,532 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 06:07:52,490 | INFO | FT 3 [1/10009] loss=2.0260
2026-02-13 06:08:42,425 | INFO | FT 3 [501/10009] loss=1.8082
2026-02-13 06:09:32,724 | INFO | FT 3 [1001/10009] loss=1.8144
2026-02-13 06:10:22,624 | INFO | FT 3 [1501/10009] loss=1.8169
2026-02-13 06:11:12,925 | INFO | FT 3 [2001/10009] loss=1.8161
2026-02-13 06:12:02,535 | INFO | FT 3 [2501/10009] loss=1.8110
2026-02-13 06:12:52,524 | INFO | FT 3 [3001/10009] loss=1.8091
2026-02-13 06:13:42,125 | INFO | FT 3 [3501/10009] loss=1.8084
2026-02-13 06:14:31,927 | INFO | FT 3 [4001/10009] loss=1.8075
2026-02-13 06:15:21,625 | INFO | FT 3 [4501/10009] loss=1.8067
2026-02-13 06:16:11,746 | INFO | FT 3 [5001/10009] loss=1.8084
2026-02-13 06:17:02,026 | INFO | FT 3 [5501/10009] loss=1.8081
2026-02-13 06:17:51,842 | INFO | FT 3 [6001/10009] loss=1.8061
2026-02-13 06:18:41,423 | INFO | FT 3 [6501/10009] loss=1.8054
2026-02-13 06:19:31,430 | INFO | FT 3 [7001/10009] loss=1.8042
2026-02-13 06:20:20,813 | INFO | FT 3 [7501/10009] loss=1.8042
2026-02-13 06:21:10,492 | INFO | FT 3 [8001/10009] loss=1.8027
2026-02-13 06:22:00,324 | INFO | FT 3 [8501/10009] loss=1.8013
2026-02-13 06:22:50,324 | INFO | FT 3 [9001/10009] loss=1.7994
2026-02-13 06:23:39,824 | INFO | FT 3 [9501/10009] loss=1.7979
2026-02-13 06:24:29,324 | INFO | FT 3 [10001/10009] loss=1.7965
2026-02-13 06:24:30,089 | INFO | FT 3 [10009/10009] loss=1.7965
2026-02-13 06:25:36,973 | INFO | FT ep 3/10: train_loss=1.7965, val_acc=0.6315
2026-02-13 06:25:37,021 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 06:25:37,984 | INFO | FT 4 [1/10009] loss=1.7985
2026-02-13 06:26:27,123 | INFO | FT 4 [501/10009] loss=1.7260
2026-02-13 06:27:16,720 | INFO | FT 4 [1001/10009] loss=1.7358
2026-02-13 06:28:06,424 | INFO | FT 4 [1501/10009] loss=1.7322
2026-02-13 06:28:55,824 | INFO | FT 4 [2001/10009] loss=1.7313
2026-02-13 06:29:45,027 | INFO | FT 4 [2501/10009] loss=1.7293
2026-02-13 06:30:34,324 | INFO | FT 4 [3001/10009] loss=1.7306
2026-02-13 06:31:23,626 | INFO | FT 4 [3501/10009] loss=1.7318
2026-02-13 06:32:13,024 | INFO | FT 4 [4001/10009] loss=1.7316
2026-02-13 06:33:01,924 | INFO | FT 4 [4501/10009] loss=1.7318
2026-02-13 06:33:50,824 | INFO | FT 4 [5001/10009] loss=1.7335
2026-02-13 06:34:39,300 | INFO | FT 4 [5501/10009] loss=1.7336
2026-02-13 06:35:27,761 | INFO | FT 4 [6001/10009] loss=1.7338
2026-02-13 06:36:16,261 | INFO | FT 4 [6501/10009] loss=1.7339
2026-02-13 06:37:04,533 | INFO | FT 4 [7001/10009] loss=1.7316
2026-02-13 06:37:52,969 | INFO | FT 4 [7501/10009] loss=1.7299
2026-02-13 06:38:41,242 | INFO | FT 4 [8001/10009] loss=1.7277
2026-02-13 06:39:29,723 | INFO | FT 4 [8501/10009] loss=1.7267
2026-02-13 06:40:18,450 | INFO | FT 4 [9001/10009] loss=1.7250
2026-02-13 06:41:06,725 | INFO | FT 4 [9501/10009] loss=1.7244
2026-02-13 06:41:54,837 | INFO | FT 4 [10001/10009] loss=1.7235
2026-02-13 06:41:55,624 | INFO | FT 4 [10009/10009] loss=1.7235
2026-02-13 06:43:03,010 | INFO | FT ep 4/10: train_loss=1.7235, val_acc=0.6400
2026-02-13 06:43:03,059 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 06:43:04,024 | INFO | FT 5 [1/10009] loss=1.6919
2026-02-13 06:43:53,011 | INFO | FT 5 [501/10009] loss=1.6603
2026-02-13 06:44:41,724 | INFO | FT 5 [1001/10009] loss=1.6665
2026-02-13 06:45:31,021 | INFO | FT 5 [1501/10009] loss=1.6684
2026-02-13 06:46:19,827 | INFO | FT 5 [2001/10009] loss=1.6704
2026-02-13 06:47:08,721 | INFO | FT 5 [2501/10009] loss=1.6722
2026-02-13 06:47:57,753 | INFO | FT 5 [3001/10009] loss=1.6712
2026-02-13 06:48:46,924 | INFO | FT 5 [3501/10009] loss=1.6723
2026-02-13 06:49:36,121 | INFO | FT 5 [4001/10009] loss=1.6718
2026-02-13 06:50:25,020 | INFO | FT 5 [4501/10009] loss=1.6722
2026-02-13 06:51:14,225 | INFO | FT 5 [5001/10009] loss=1.6713
2026-02-13 06:52:03,246 | INFO | FT 5 [5501/10009] loss=1.6712
2026-02-13 06:52:52,224 | INFO | FT 5 [6001/10009] loss=1.6709
2026-02-13 06:53:41,434 | INFO | FT 5 [6501/10009] loss=1.6706
2026-02-13 06:54:30,824 | INFO | FT 5 [7001/10009] loss=1.6687
2026-02-13 06:55:20,327 | INFO | FT 5 [7501/10009] loss=1.6681
2026-02-13 06:56:09,339 | INFO | FT 5 [8001/10009] loss=1.6666
2026-02-13 06:56:58,626 | INFO | FT 5 [8501/10009] loss=1.6656
2026-02-13 06:57:48,087 | INFO | FT 5 [9001/10009] loss=1.6649
2026-02-13 06:58:37,324 | INFO | FT 5 [9501/10009] loss=1.6648
2026-02-13 06:59:26,527 | INFO | FT 5 [10001/10009] loss=1.6635
2026-02-13 06:59:27,295 | INFO | FT 5 [10009/10009] loss=1.6634
2026-02-13 07:00:33,220 | INFO | FT ep 5/10: train_loss=1.6634, val_acc=0.6463
2026-02-13 07:00:33,270 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 07:00:34,187 | INFO | FT 6 [1/10009] loss=1.6142
2026-02-13 07:01:23,024 | INFO | FT 6 [501/10009] loss=1.6211
2026-02-13 07:02:11,219 | INFO | FT 6 [1001/10009] loss=1.6235
2026-02-13 07:02:59,574 | INFO | FT 6 [1501/10009] loss=1.6240
2026-02-13 07:03:48,331 | INFO | FT 6 [2001/10009] loss=1.6257
2026-02-13 07:04:37,311 | INFO | FT 6 [2501/10009] loss=1.6254
2026-02-13 07:05:25,727 | INFO | FT 6 [3001/10009] loss=1.6235
2026-02-13 07:06:14,228 | INFO | FT 6 [3501/10009] loss=1.6241
2026-02-13 07:07:02,681 | INFO | FT 6 [4001/10009] loss=1.6245
2026-02-13 07:07:50,823 | INFO | FT 6 [4501/10009] loss=1.6246
2026-02-13 07:08:39,323 | INFO | FT 6 [5001/10009] loss=1.6236
2026-02-13 07:09:27,771 | INFO | FT 6 [5501/10009] loss=1.6217
2026-02-13 07:10:16,086 | INFO | FT 6 [6001/10009] loss=1.6215
2026-02-13 07:11:04,723 | INFO | FT 6 [6501/10009] loss=1.6218
2026-02-13 07:11:53,323 | INFO | FT 6 [7001/10009] loss=1.6218
2026-02-13 07:12:41,823 | INFO | FT 6 [7501/10009] loss=1.6213
2026-02-13 07:13:30,421 | INFO | FT 6 [8001/10009] loss=1.6214
2026-02-13 07:14:19,023 | INFO | FT 6 [8501/10009] loss=1.6199
2026-02-13 07:15:07,507 | INFO | FT 6 [9001/10009] loss=1.6187
2026-02-13 07:15:56,026 | INFO | FT 6 [9501/10009] loss=1.6173
2026-02-13 07:16:44,626 | INFO | FT 6 [10001/10009] loss=1.6170
2026-02-13 07:16:45,402 | INFO | FT 6 [10009/10009] loss=1.6170
2026-02-13 07:17:53,199 | INFO | FT ep 6/10: train_loss=1.6170, val_acc=0.6518
2026-02-13 07:17:53,248 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 07:17:54,138 | INFO | FT 7 [1/10009] loss=2.0119
2026-02-13 07:18:42,330 | INFO | FT 7 [501/10009] loss=1.5718
2026-02-13 07:19:30,411 | INFO | FT 7 [1001/10009] loss=1.5729
2026-02-13 07:20:18,434 | INFO | FT 7 [1501/10009] loss=1.5730
2026-02-13 07:21:07,026 | INFO | FT 7 [2001/10009] loss=1.5730
2026-02-13 07:21:55,724 | INFO | FT 7 [2501/10009] loss=1.5760
2026-02-13 07:22:44,621 | INFO | FT 7 [3001/10009] loss=1.5751
2026-02-13 07:23:33,071 | INFO | FT 7 [3501/10009] loss=1.5735
2026-02-13 07:24:21,720 | INFO | FT 7 [4001/10009] loss=1.5759
2026-02-13 07:25:10,424 | INFO | FT 7 [4501/10009] loss=1.5759
2026-02-13 07:25:58,871 | INFO | FT 7 [5001/10009] loss=1.5761
2026-02-13 07:26:47,332 | INFO | FT 7 [5501/10009] loss=1.5758
2026-02-13 07:27:35,813 | INFO | FT 7 [6001/10009] loss=1.5768
2026-02-13 07:28:24,108 | INFO | FT 7 [6501/10009] loss=1.5770
2026-02-13 07:29:12,162 | INFO | FT 7 [7001/10009] loss=1.5779
2026-02-13 07:30:00,423 | INFO | FT 7 [7501/10009] loss=1.5773
2026-02-13 07:30:48,919 | INFO | FT 7 [8001/10009] loss=1.5777
2026-02-13 07:31:36,960 | INFO | FT 7 [8501/10009] loss=1.5766
2026-02-13 07:32:24,908 | INFO | FT 7 [9001/10009] loss=1.5764
2026-02-13 07:33:12,923 | INFO | FT 7 [9501/10009] loss=1.5758
2026-02-13 07:34:01,323 | INFO | FT 7 [10001/10009] loss=1.5750
2026-02-13 07:34:02,081 | INFO | FT 7 [10009/10009] loss=1.5750
2026-02-13 07:35:08,535 | INFO | FT ep 7/10: train_loss=1.5750, val_acc=0.6570
2026-02-13 07:35:08,583 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 07:35:09,445 | INFO | FT 8 [1/10009] loss=1.4421
2026-02-13 07:35:59,323 | INFO | FT 8 [501/10009] loss=1.5452
2026-02-13 07:36:48,025 | INFO | FT 8 [1001/10009] loss=1.5450
2026-02-13 07:37:36,924 | INFO | FT 8 [1501/10009] loss=1.5503
2026-02-13 07:38:25,623 | INFO | FT 8 [2001/10009] loss=1.5520
2026-02-13 07:39:14,420 | INFO | FT 8 [2501/10009] loss=1.5521
2026-02-13 07:40:03,324 | INFO | FT 8 [3001/10009] loss=1.5517
2026-02-13 07:40:52,523 | INFO | FT 8 [3501/10009] loss=1.5495
2026-02-13 07:41:41,732 | INFO | FT 8 [4001/10009] loss=1.5471
2026-02-13 07:42:30,923 | INFO | FT 8 [4501/10009] loss=1.5460
2026-02-13 07:43:19,725 | INFO | FT 8 [5001/10009] loss=1.5457
2026-02-13 07:44:08,927 | INFO | FT 8 [5501/10009] loss=1.5453
2026-02-13 07:44:57,786 | INFO | FT 8 [6001/10009] loss=1.5444
2026-02-13 07:45:46,924 | INFO | FT 8 [6501/10009] loss=1.5438
2026-02-13 07:46:36,023 | INFO | FT 8 [7001/10009] loss=1.5430
2026-02-13 07:47:25,255 | INFO | FT 8 [7501/10009] loss=1.5423
2026-02-13 07:48:14,523 | INFO | FT 8 [8001/10009] loss=1.5418
2026-02-13 07:49:03,623 | INFO | FT 8 [8501/10009] loss=1.5418
2026-02-13 07:49:52,531 | INFO | FT 8 [9001/10009] loss=1.5411
2026-02-13 07:50:41,620 | INFO | FT 8 [9501/10009] loss=1.5413
2026-02-13 07:51:30,339 | INFO | FT 8 [10001/10009] loss=1.5406
2026-02-13 07:51:31,106 | INFO | FT 8 [10009/10009] loss=1.5406
2026-02-13 07:52:36,700 | INFO | FT ep 8/10: train_loss=1.5406, val_acc=0.6602
2026-02-13 07:52:36,748 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 07:52:37,562 | INFO | FT 9 [1/10009] loss=1.7335
2026-02-13 07:53:27,022 | INFO | FT 9 [501/10009] loss=1.5334
2026-02-13 07:54:15,623 | INFO | FT 9 [1001/10009] loss=1.5251
2026-02-13 07:55:03,923 | INFO | FT 9 [1501/10009] loss=1.5235
2026-02-13 07:55:52,523 | INFO | FT 9 [2001/10009] loss=1.5218
2026-02-13 07:56:40,723 | INFO | FT 9 [2501/10009] loss=1.5218
2026-02-13 07:57:29,117 | INFO | FT 9 [3001/10009] loss=1.5224
2026-02-13 07:58:17,544 | INFO | FT 9 [3501/10009] loss=1.5247
2026-02-13 07:59:06,037 | INFO | FT 9 [4001/10009] loss=1.5237
2026-02-13 07:59:54,973 | INFO | FT 9 [4501/10009] loss=1.5232
2026-02-13 08:00:44,123 | INFO | FT 9 [5001/10009] loss=1.5218
2026-02-13 08:01:32,426 | INFO | FT 9 [5501/10009] loss=1.5216
2026-02-13 08:02:21,122 | INFO | FT 9 [6001/10009] loss=1.5214
2026-02-13 08:03:09,624 | INFO | FT 9 [6501/10009] loss=1.5208
2026-02-13 08:03:57,978 | INFO | FT 9 [7001/10009] loss=1.5211
2026-02-13 08:04:46,423 | INFO | FT 9 [7501/10009] loss=1.5211
2026-02-13 08:05:35,198 | INFO | FT 9 [8001/10009] loss=1.5213
2026-02-13 08:06:23,909 | INFO | FT 9 [8501/10009] loss=1.5204
2026-02-13 08:07:12,926 | INFO | FT 9 [9001/10009] loss=1.5198
2026-02-13 08:08:01,332 | INFO | FT 9 [9501/10009] loss=1.5200
2026-02-13 08:08:49,908 | INFO | FT 9 [10001/10009] loss=1.5203
2026-02-13 08:08:50,666 | INFO | FT 9 [10009/10009] loss=1.5202
2026-02-13 08:09:57,284 | INFO | FT ep 9/10: train_loss=1.5202, val_acc=0.6620
2026-02-13 08:09:57,331 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 08:09:58,180 | INFO | FT 10 [1/10009] loss=1.9146
2026-02-13 08:10:47,623 | INFO | FT 10 [501/10009] loss=1.5122
2026-02-13 08:11:36,523 | INFO | FT 10 [1001/10009] loss=1.5100
2026-02-13 08:12:25,159 | INFO | FT 10 [1501/10009] loss=1.5092
2026-02-13 08:13:13,906 | INFO | FT 10 [2001/10009] loss=1.5070
2026-02-13 08:14:02,821 | INFO | FT 10 [2501/10009] loss=1.5102
2026-02-13 08:14:50,835 | INFO | FT 10 [3001/10009] loss=1.5115
2026-02-13 08:15:39,597 | INFO | FT 10 [3501/10009] loss=1.5107
2026-02-13 08:16:28,426 | INFO | FT 10 [4001/10009] loss=1.5099
2026-02-13 08:17:17,223 | INFO | FT 10 [4501/10009] loss=1.5098
2026-02-13 08:18:06,224 | INFO | FT 10 [5001/10009] loss=1.5096
2026-02-13 08:18:55,283 | INFO | FT 10 [5501/10009] loss=1.5093
2026-02-13 08:19:43,734 | INFO | FT 10 [6001/10009] loss=1.5102
2026-02-13 08:20:32,507 | INFO | FT 10 [6501/10009] loss=1.5098
2026-02-13 08:21:21,135 | INFO | FT 10 [7001/10009] loss=1.5100
2026-02-13 08:22:09,923 | INFO | FT 10 [7501/10009] loss=1.5092
2026-02-13 08:22:58,338 | INFO | FT 10 [8001/10009] loss=1.5091
2026-02-13 08:23:47,111 | INFO | FT 10 [8501/10009] loss=1.5088
2026-02-13 08:24:36,076 | INFO | FT 10 [9001/10009] loss=1.5083
2026-02-13 08:25:24,566 | INFO | FT 10 [9501/10009] loss=1.5083
2026-02-13 08:26:13,233 | INFO | FT 10 [10001/10009] loss=1.5088
2026-02-13 08:26:13,989 | INFO | FT 10 [10009/10009] loss=1.5088
2026-02-13 08:27:19,467 | INFO | FT ep 10/10: train_loss=1.5088, val_acc=0.6638
2026-02-13 08:27:19,513 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_best.pth
2026-02-13 08:28:25,645 | INFO | ============================================================
2026-02-13 08:28:25,646 | INFO | Summary
2026-02-13 08:28:25,646 | INFO | ============================================================
2026-02-13 08:28:25,646 | INFO | Base MACs:    0.32G -> Pruned: 0.24G (76.2%)
2026-02-13 08:28:25,646 | INFO | Base Params:  3.50M -> Pruned: 2.73M (78.0%)
2026-02-13 08:28:25,646 | INFO | Original Acc: 0.7187
2026-02-13 08:28:25,646 | INFO | Final Acc:    0.6638
2026-02-13 08:28:25,646 | INFO | Best Acc:     0.6638
2026-02-13 08:28:25,692 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.7/vbp_final.pth
