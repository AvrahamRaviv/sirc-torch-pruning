2026-02-13 05:29:43,486 | INFO | Logging to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_imagenet.log
2026-02-13 05:29:43,486 | INFO | ============================================================
2026-02-13 05:29:43,486 | INFO | VBP ImageNet Reproduction Script
2026-02-13 05:29:43,486 | INFO | ============================================================
2026-02-13 05:29:43,486 | INFO |   model_type: cnn
2026-02-13 05:29:43,487 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/MNv2_TP/mobilenet_v2_weights.pth
2026-02-13 05:29:43,487 | INFO |   cnn_arch: mobilenet_v2
2026-02-13 05:29:43,487 | INFO |   pretrained: True
2026-02-13 05:29:43,487 | INFO |   interior_only: True
2026-02-13 05:29:43,487 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 05:29:43,487 | INFO |   train_batch_size: 64
2026-02-13 05:29:43,487 | INFO |   val_batch_size: 128
2026-02-13 05:29:43,487 | INFO |   num_workers: 4
2026-02-13 05:29:43,487 | INFO |   max_batches: 200
2026-02-13 05:29:43,487 | INFO |   keep_ratio: 0.8
2026-02-13 05:29:43,487 | INFO |   global_pruning: True
2026-02-13 05:29:43,487 | INFO |   max_pruning_ratio: 1.0
2026-02-13 05:29:43,487 | INFO |   norm_per_layer: False
2026-02-13 05:29:43,487 | INFO |   no_compensation: False
2026-02-13 05:29:43,487 | INFO |   no_recalib: False
2026-02-13 05:29:43,487 | INFO |   criterion: variance
2026-02-13 05:29:43,487 | INFO |   epochs_ft: 10
2026-02-13 05:29:43,487 | INFO |   lr_ft: 0.0001
2026-02-13 05:29:43,487 | INFO |   opt_ft: adamw
2026-02-13 05:29:43,487 | INFO |   momentum_ft: 0.9
2026-02-13 05:29:43,487 | INFO |   wd_ft: None
2026-02-13 05:29:43,487 | INFO |   use_kd: True
2026-02-13 05:29:43,487 | INFO |   kd_alpha: 0.7
2026-02-13 05:29:43,487 | INFO |   kd_T: 2.0
2026-02-13 05:29:43,487 | INFO |   pat: False
2026-02-13 05:29:43,487 | INFO |   pat_steps: 5
2026-02-13 05:29:43,488 | INFO |   pat_epochs_per_step: 3
2026-02-13 05:29:43,488 | INFO |   var_loss_weight: 0.0
2026-02-13 05:29:43,488 | INFO |   sparse_mode: none
2026-02-13 05:29:43,488 | INFO |   epochs_sparse: 5
2026-02-13 05:29:43,488 | INFO |   lr_sparse: 0.0001
2026-02-13 05:29:43,488 | INFO |   l1_lambda: 0.0001
2026-02-13 05:29:43,488 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 05:29:43,488 | INFO |   disable_ddp: False
2026-02-13 05:29:43,488 | INFO |   local_rank: 0
2026-02-13 05:29:43,488 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8
2026-02-13 05:29:43,488 | INFO |   rank: 0
2026-02-13 05:29:43,488 | INFO |   world_size: 2
2026-02-13 05:29:43,488 | INFO | Loading ImageNet dataset...
2026-02-13 05:29:43,488 | INFO | Using cached sample lists for fast loading
2026-02-13 05:29:43,832 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 05:29:45,770 | INFO | Loaded mobilenet_v2 (pretrained=True)
2026-02-13 05:29:47,199 | INFO | Baseline: 0.32G MACs, 3.50M params
2026-02-13 05:29:47,199 | INFO | Evaluating original model...
2026-02-13 05:30:53,345 | INFO | Original accuracy: 0.7187, loss: 1.1474
2026-02-13 05:30:53,373 | INFO | Created teacher model for knowledge distillation
2026-02-13 05:30:53,373 | INFO | PAT: 1 steps, per_step_keep=0.8000, epochs_per_step=0, target_keep=0.8, criterion=variance
2026-02-13 05:30:53,373 | INFO | 
============================================================
2026-02-13 05:30:53,374 | INFO | PAT Step 1/1
2026-02-13 05:30:53,374 | INFO | ============================================================
2026-02-13 05:30:53,395 | INFO | Auto-detected 35 CNN target layers for stats
2026-02-13 05:30:53,395 | INFO | Collecting activation variance statistics...
2026-02-13 05:31:08,931 | INFO | Statistics collected for 35 layers
2026-02-13 05:31:08,932 | INFO |   Conv2d: mean_var=0.027782
2026-02-13 05:31:08,932 | INFO |   Conv2d: mean_var=0.382939
2026-02-13 05:31:08,932 | INFO |   Conv2d: mean_var=0.045719
2026-02-13 05:31:08,932 | INFO |   Conv2d: mean_var=0.291565
2026-02-13 05:31:08,932 | INFO |   Conv2d: mean_var=0.015361
2026-02-13 05:31:09,122 | INFO | Variance â€” entropy=0.8413, cv=2.1457, gini=0.8130
2026-02-13 05:31:09,409 | INFO | Pruning: per_step_prune=0.2000
2026-02-13 05:31:09,647 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 05:31:09,647 | INFO | Recalibrating BN running stats...
2026-02-13 05:31:10,418 | INFO | BN recalib [1/100]
2026-02-13 05:31:10,657 | INFO | BN recalib [6/100]
2026-02-13 05:31:11,240 | INFO | BN recalib [11/100]
2026-02-13 05:31:11,827 | INFO | BN recalib [16/100]
2026-02-13 05:31:12,394 | INFO | BN recalib [21/100]
2026-02-13 05:31:12,739 | INFO | BN recalib [26/100]
2026-02-13 05:31:13,099 | INFO | BN recalib [31/100]
2026-02-13 05:31:13,448 | INFO | BN recalib [36/100]
2026-02-13 05:31:14,021 | INFO | BN recalib [41/100]
2026-02-13 05:31:14,382 | INFO | BN recalib [46/100]
2026-02-13 05:31:14,713 | INFO | BN recalib [51/100]
2026-02-13 05:31:15,189 | INFO | BN recalib [56/100]
2026-02-13 05:31:15,644 | INFO | BN recalib [61/100]
2026-02-13 05:31:16,026 | INFO | BN recalib [66/100]
2026-02-13 05:31:16,369 | INFO | BN recalib [71/100]
2026-02-13 05:31:16,728 | INFO | BN recalib [76/100]
2026-02-13 05:31:17,240 | INFO | BN recalib [81/100]
2026-02-13 05:31:17,588 | INFO | BN recalib [86/100]
2026-02-13 05:31:18,029 | INFO | BN recalib [91/100]
2026-02-13 05:31:18,330 | INFO | BN recalib [96/100]
2026-02-13 05:31:18,702 | INFO | BN recalib [100/100]
2026-02-13 05:31:19,266 | INFO | BN recalibration done (100 batches)
2026-02-13 05:32:24,936 | INFO | Step 1 retention: acc=0.0504, loss=6.0727
2026-02-13 05:32:24,936 | INFO |   cumulative_keep=0.8000, MACs=0.27G, params=2.92M
2026-02-13 05:32:24,936 | INFO | 
Post-prune fine-tuning for 10 epochs (adamw, lr=0.0001, wd=0.01)...
2026-02-13 05:32:26,035 | INFO | FT 1 [1/10009] loss=8.0055
2026-02-13 05:33:17,337 | INFO | FT 1 [501/10009] loss=3.6799
2026-02-13 05:34:07,348 | INFO | FT 1 [1001/10009] loss=3.1893
2026-02-13 05:34:57,327 | INFO | FT 1 [1501/10009] loss=2.9287
2026-02-13 05:35:47,332 | INFO | FT 1 [2001/10009] loss=2.7646
2026-02-13 05:36:37,324 | INFO | FT 1 [2501/10009] loss=2.6478
2026-02-13 05:37:27,326 | INFO | FT 1 [3001/10009] loss=2.5585
2026-02-13 05:38:17,565 | INFO | FT 1 [3501/10009] loss=2.4871
2026-02-13 05:39:07,742 | INFO | FT 1 [4001/10009] loss=2.4280
2026-02-13 05:39:57,877 | INFO | FT 1 [4501/10009] loss=2.3806
2026-02-13 05:40:47,934 | INFO | FT 1 [5001/10009] loss=2.3378
2026-02-13 05:41:38,274 | INFO | FT 1 [5501/10009] loss=2.3001
2026-02-13 05:42:28,366 | INFO | FT 1 [6001/10009] loss=2.2684
2026-02-13 05:43:18,346 | INFO | FT 1 [6501/10009] loss=2.2389
2026-02-13 05:44:08,529 | INFO | FT 1 [7001/10009] loss=2.2135
2026-02-13 05:44:58,528 | INFO | FT 1 [7501/10009] loss=2.1901
2026-02-13 05:45:48,430 | INFO | FT 1 [8001/10009] loss=2.1690
2026-02-13 05:46:38,434 | INFO | FT 1 [8501/10009] loss=2.1497
2026-02-13 05:47:28,624 | INFO | FT 1 [9001/10009] loss=2.1314
2026-02-13 05:48:18,626 | INFO | FT 1 [9501/10009] loss=2.1144
2026-02-13 05:49:08,529 | INFO | FT 1 [10001/10009] loss=2.0993
2026-02-13 05:49:09,339 | INFO | FT 1 [10009/10009] loss=2.0991
2026-02-13 05:50:14,565 | INFO | FT ep 1/10: train_loss=2.0991, val_acc=0.6229
2026-02-13 05:50:14,612 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 05:50:15,532 | INFO | FT 2 [1/10009] loss=1.7442
2026-02-13 05:51:05,824 | INFO | FT 2 [501/10009] loss=1.7589
2026-02-13 05:51:55,931 | INFO | FT 2 [1001/10009] loss=1.7624
2026-02-13 05:52:45,935 | INFO | FT 2 [1501/10009] loss=1.7627
2026-02-13 05:53:36,076 | INFO | FT 2 [2001/10009] loss=1.7599
2026-02-13 05:54:26,323 | INFO | FT 2 [2501/10009] loss=1.7556
2026-02-13 05:55:16,360 | INFO | FT 2 [3001/10009] loss=1.7525
2026-02-13 05:56:06,424 | INFO | FT 2 [3501/10009] loss=1.7509
2026-02-13 05:56:56,529 | INFO | FT 2 [4001/10009] loss=1.7498
2026-02-13 05:57:46,526 | INFO | FT 2 [4501/10009] loss=1.7474
2026-02-13 05:58:36,724 | INFO | FT 2 [5001/10009] loss=1.7451
2026-02-13 05:59:26,826 | INFO | FT 2 [5501/10009] loss=1.7430
2026-02-13 06:00:16,835 | INFO | FT 2 [6001/10009] loss=1.7402
2026-02-13 06:01:06,836 | INFO | FT 2 [6501/10009] loss=1.7389
2026-02-13 06:01:56,924 | INFO | FT 2 [7001/10009] loss=1.7366
2026-02-13 06:02:46,937 | INFO | FT 2 [7501/10009] loss=1.7343
2026-02-13 06:03:36,948 | INFO | FT 2 [8001/10009] loss=1.7318
2026-02-13 06:04:26,927 | INFO | FT 2 [8501/10009] loss=1.7299
2026-02-13 06:05:16,932 | INFO | FT 2 [9001/10009] loss=1.7282
2026-02-13 06:06:07,124 | INFO | FT 2 [9501/10009] loss=1.7256
2026-02-13 06:06:57,123 | INFO | FT 2 [10001/10009] loss=1.7235
2026-02-13 06:06:57,943 | INFO | FT 2 [10009/10009] loss=1.7234
2026-02-13 06:08:01,805 | INFO | FT ep 2/10: train_loss=1.7234, val_acc=0.6393
2026-02-13 06:08:01,860 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 06:08:02,847 | INFO | FT 3 [1/10009] loss=2.0474
2026-02-13 06:08:52,930 | INFO | FT 3 [501/10009] loss=1.6400
2026-02-13 06:09:42,922 | INFO | FT 3 [1001/10009] loss=1.6427
2026-02-13 06:10:32,953 | INFO | FT 3 [1501/10009] loss=1.6422
2026-02-13 06:11:22,924 | INFO | FT 3 [2001/10009] loss=1.6450
2026-02-13 06:12:13,039 | INFO | FT 3 [2501/10009] loss=1.6432
2026-02-13 06:13:03,023 | INFO | FT 3 [3001/10009] loss=1.6421
2026-02-13 06:13:53,045 | INFO | FT 3 [3501/10009] loss=1.6411
2026-02-13 06:14:43,024 | INFO | FT 3 [4001/10009] loss=1.6427
2026-02-13 06:15:33,035 | INFO | FT 3 [4501/10009] loss=1.6412
2026-02-13 06:16:23,031 | INFO | FT 3 [5001/10009] loss=1.6428
2026-02-13 06:17:13,023 | INFO | FT 3 [5501/10009] loss=1.6417
2026-02-13 06:18:03,050 | INFO | FT 3 [6001/10009] loss=1.6408
2026-02-13 06:18:53,025 | INFO | FT 3 [6501/10009] loss=1.6399
2026-02-13 06:19:43,024 | INFO | FT 3 [7001/10009] loss=1.6395
2026-02-13 06:20:33,028 | INFO | FT 3 [7501/10009] loss=1.6380
2026-02-13 06:21:23,024 | INFO | FT 3 [8001/10009] loss=1.6369
2026-02-13 06:22:13,134 | INFO | FT 3 [8501/10009] loss=1.6355
2026-02-13 06:23:03,126 | INFO | FT 3 [9001/10009] loss=1.6340
2026-02-13 06:23:53,105 | INFO | FT 3 [9501/10009] loss=1.6323
2026-02-13 06:24:42,996 | INFO | FT 3 [10001/10009] loss=1.6316
2026-02-13 06:24:43,821 | INFO | FT 3 [10009/10009] loss=1.6318
2026-02-13 06:25:49,541 | INFO | FT ep 3/10: train_loss=1.6318, val_acc=0.6503
2026-02-13 06:25:49,591 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 06:25:50,472 | INFO | FT 4 [1/10009] loss=1.5741
2026-02-13 06:26:40,524 | INFO | FT 4 [501/10009] loss=1.5807
2026-02-13 06:27:30,525 | INFO | FT 4 [1001/10009] loss=1.5817
2026-02-13 06:28:20,523 | INFO | FT 4 [1501/10009] loss=1.5814
2026-02-13 06:29:10,531 | INFO | FT 4 [2001/10009] loss=1.5776
2026-02-13 06:30:00,524 | INFO | FT 4 [2501/10009] loss=1.5779
2026-02-13 06:30:50,324 | INFO | FT 4 [3001/10009] loss=1.5804
2026-02-13 06:31:40,326 | INFO | FT 4 [3501/10009] loss=1.5795
2026-02-13 06:32:30,329 | INFO | FT 4 [4001/10009] loss=1.5788
2026-02-13 06:33:20,142 | INFO | FT 4 [4501/10009] loss=1.5783
2026-02-13 06:34:10,123 | INFO | FT 4 [5001/10009] loss=1.5791
2026-02-13 06:35:00,117 | INFO | FT 4 [5501/10009] loss=1.5787
2026-02-13 06:35:50,131 | INFO | FT 4 [6001/10009] loss=1.5784
2026-02-13 06:36:40,013 | INFO | FT 4 [6501/10009] loss=1.5789
2026-02-13 06:37:30,025 | INFO | FT 4 [7001/10009] loss=1.5784
2026-02-13 06:38:20,032 | INFO | FT 4 [7501/10009] loss=1.5769
2026-02-13 06:39:09,925 | INFO | FT 4 [8001/10009] loss=1.5754
2026-02-13 06:39:59,933 | INFO | FT 4 [8501/10009] loss=1.5737
2026-02-13 06:40:49,928 | INFO | FT 4 [9001/10009] loss=1.5727
2026-02-13 06:41:39,668 | INFO | FT 4 [9501/10009] loss=1.5724
2026-02-13 06:42:29,225 | INFO | FT 4 [10001/10009] loss=1.5722
2026-02-13 06:42:30,042 | INFO | FT 4 [10009/10009] loss=1.5722
2026-02-13 06:43:33,285 | INFO | FT ep 4/10: train_loss=1.5722, val_acc=0.6584
2026-02-13 06:43:33,341 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 06:43:34,326 | INFO | FT 5 [1/10009] loss=1.8507
2026-02-13 06:44:23,808 | INFO | FT 5 [501/10009] loss=1.5236
2026-02-13 06:45:13,321 | INFO | FT 5 [1001/10009] loss=1.5317
2026-02-13 06:46:03,221 | INFO | FT 5 [1501/10009] loss=1.5323
2026-02-13 06:46:53,079 | INFO | FT 5 [2001/10009] loss=1.5293
2026-02-13 06:47:42,932 | INFO | FT 5 [2501/10009] loss=1.5282
2026-02-13 06:48:32,924 | INFO | FT 5 [3001/10009] loss=1.5275
2026-02-13 06:49:22,822 | INFO | FT 5 [3501/10009] loss=1.5255
2026-02-13 06:50:12,634 | INFO | FT 5 [4001/10009] loss=1.5267
2026-02-13 06:51:02,529 | INFO | FT 5 [4501/10009] loss=1.5273
2026-02-13 06:51:52,501 | INFO | FT 5 [5001/10009] loss=1.5262
2026-02-13 06:52:42,524 | INFO | FT 5 [5501/10009] loss=1.5259
2026-02-13 06:53:32,424 | INFO | FT 5 [6001/10009] loss=1.5262
2026-02-13 06:54:22,275 | INFO | FT 5 [6501/10009] loss=1.5255
2026-02-13 06:55:12,126 | INFO | FT 5 [7001/10009] loss=1.5243
2026-02-13 06:56:02,125 | INFO | FT 5 [7501/10009] loss=1.5239
2026-02-13 06:56:51,960 | INFO | FT 5 [8001/10009] loss=1.5230
2026-02-13 06:57:41,927 | INFO | FT 5 [8501/10009] loss=1.5223
2026-02-13 06:58:31,731 | INFO | FT 5 [9001/10009] loss=1.5212
2026-02-13 06:59:21,628 | INFO | FT 5 [9501/10009] loss=1.5215
2026-02-13 07:00:11,525 | INFO | FT 5 [10001/10009] loss=1.5199
2026-02-13 07:00:12,348 | INFO | FT 5 [10009/10009] loss=1.5198
2026-02-13 07:01:18,937 | INFO | FT ep 5/10: train_loss=1.5198, val_acc=0.6663
2026-02-13 07:01:18,988 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 07:01:19,950 | INFO | FT 6 [1/10009] loss=1.3690
2026-02-13 07:02:09,653 | INFO | FT 6 [501/10009] loss=1.4946
2026-02-13 07:02:59,530 | INFO | FT 6 [1001/10009] loss=1.4876
2026-02-13 07:03:49,424 | INFO | FT 6 [1501/10009] loss=1.4841
2026-02-13 07:04:39,526 | INFO | FT 6 [2001/10009] loss=1.4857
2026-02-13 07:05:29,477 | INFO | FT 6 [2501/10009] loss=1.4845
2026-02-13 07:06:19,324 | INFO | FT 6 [3001/10009] loss=1.4818
2026-02-13 07:07:09,316 | INFO | FT 6 [3501/10009] loss=1.4831
2026-02-13 07:07:59,330 | INFO | FT 6 [4001/10009] loss=1.4851
2026-02-13 07:08:49,330 | INFO | FT 6 [4501/10009] loss=1.4845
2026-02-13 07:09:39,332 | INFO | FT 6 [5001/10009] loss=1.4844
2026-02-13 07:10:29,324 | INFO | FT 6 [5501/10009] loss=1.4841
2026-02-13 07:11:19,328 | INFO | FT 6 [6001/10009] loss=1.4828
2026-02-13 07:12:09,329 | INFO | FT 6 [6501/10009] loss=1.4823
2026-02-13 07:12:59,326 | INFO | FT 6 [7001/10009] loss=1.4819
2026-02-13 07:13:49,339 | INFO | FT 6 [7501/10009] loss=1.4815
2026-02-13 07:14:39,329 | INFO | FT 6 [8001/10009] loss=1.4805
2026-02-13 07:15:29,337 | INFO | FT 6 [8501/10009] loss=1.4795
2026-02-13 07:16:19,326 | INFO | FT 6 [9001/10009] loss=1.4778
2026-02-13 07:17:09,223 | INFO | FT 6 [9501/10009] loss=1.4764
2026-02-13 07:17:59,026 | INFO | FT 6 [10001/10009] loss=1.4759
2026-02-13 07:17:59,845 | INFO | FT 6 [10009/10009] loss=1.4758
2026-02-13 07:19:09,162 | INFO | FT ep 6/10: train_loss=1.4758, val_acc=0.6697
2026-02-13 07:19:09,216 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 07:19:10,126 | INFO | FT 7 [1/10009] loss=1.6720
2026-02-13 07:19:59,755 | INFO | FT 7 [501/10009] loss=1.4408
2026-02-13 07:20:49,359 | INFO | FT 7 [1001/10009] loss=1.4483
2026-02-13 07:21:39,223 | INFO | FT 7 [1501/10009] loss=1.4421
2026-02-13 07:22:29,240 | INFO | FT 7 [2001/10009] loss=1.4405
2026-02-13 07:23:19,124 | INFO | FT 7 [2501/10009] loss=1.4400
2026-02-13 07:24:09,117 | INFO | FT 7 [3001/10009] loss=1.4384
2026-02-13 07:24:58,923 | INFO | FT 7 [3501/10009] loss=1.4383
2026-02-13 07:25:48,823 | INFO | FT 7 [4001/10009] loss=1.4406
2026-02-13 07:26:38,724 | INFO | FT 7 [4501/10009] loss=1.4394
2026-02-13 07:27:28,625 | INFO | FT 7 [5001/10009] loss=1.4383
2026-02-13 07:28:18,324 | INFO | FT 7 [5501/10009] loss=1.4373
2026-02-13 07:29:08,223 | INFO | FT 7 [6001/10009] loss=1.4374
2026-02-13 07:29:58,025 | INFO | FT 7 [6501/10009] loss=1.4375
2026-02-13 07:30:47,920 | INFO | FT 7 [7001/10009] loss=1.4377
2026-02-13 07:31:37,724 | INFO | FT 7 [7501/10009] loss=1.4366
2026-02-13 07:32:27,623 | INFO | FT 7 [8001/10009] loss=1.4374
2026-02-13 07:33:17,526 | INFO | FT 7 [8501/10009] loss=1.4363
2026-02-13 07:34:07,246 | INFO | FT 7 [9001/10009] loss=1.4361
2026-02-13 07:34:56,752 | INFO | FT 7 [9501/10009] loss=1.4353
2026-02-13 07:35:46,637 | INFO | FT 7 [10001/10009] loss=1.4351
2026-02-13 07:35:47,450 | INFO | FT 7 [10009/10009] loss=1.4350
2026-02-13 07:36:55,945 | INFO | FT ep 7/10: train_loss=1.4350, val_acc=0.6743
2026-02-13 07:36:55,997 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 07:36:56,926 | INFO | FT 8 [1/10009] loss=1.4466
2026-02-13 07:37:47,024 | INFO | FT 8 [501/10009] loss=1.4117
2026-02-13 07:38:36,815 | INFO | FT 8 [1001/10009] loss=1.4054
2026-02-13 07:39:26,637 | INFO | FT 8 [1501/10009] loss=1.4069
2026-02-13 07:40:16,734 | INFO | FT 8 [2001/10009] loss=1.4100
2026-02-13 07:41:06,737 | INFO | FT 8 [2501/10009] loss=1.4120
2026-02-13 07:41:56,728 | INFO | FT 8 [3001/10009] loss=1.4112
2026-02-13 07:42:46,724 | INFO | FT 8 [3501/10009] loss=1.4098
2026-02-13 07:43:36,730 | INFO | FT 8 [4001/10009] loss=1.4095
2026-02-13 07:44:26,733 | INFO | FT 8 [4501/10009] loss=1.4085
2026-02-13 07:45:16,729 | INFO | FT 8 [5001/10009] loss=1.4075
2026-02-13 07:46:06,735 | INFO | FT 8 [5501/10009] loss=1.4069
2026-02-13 07:46:56,621 | INFO | FT 8 [6001/10009] loss=1.4066
2026-02-13 07:47:46,725 | INFO | FT 8 [6501/10009] loss=1.4055
2026-02-13 07:48:36,724 | INFO | FT 8 [7001/10009] loss=1.4047
2026-02-13 07:49:26,720 | INFO | FT 8 [7501/10009] loss=1.4042
2026-02-13 07:50:16,924 | INFO | FT 8 [8001/10009] loss=1.4040
2026-02-13 07:51:06,925 | INFO | FT 8 [8501/10009] loss=1.4037
2026-02-13 07:51:56,521 | INFO | FT 8 [9001/10009] loss=1.4028
2026-02-13 07:52:46,123 | INFO | FT 8 [9501/10009] loss=1.4036
2026-02-13 07:53:35,924 | INFO | FT 8 [10001/10009] loss=1.4030
2026-02-13 07:53:36,724 | INFO | FT 8 [10009/10009] loss=1.4030
2026-02-13 07:54:46,849 | INFO | FT ep 8/10: train_loss=1.4030, val_acc=0.6773
2026-02-13 07:54:46,907 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 07:54:47,856 | INFO | FT 9 [1/10009] loss=1.4876
2026-02-13 07:55:37,829 | INFO | FT 9 [501/10009] loss=1.3947
2026-02-13 07:56:27,724 | INFO | FT 9 [1001/10009] loss=1.3862
2026-02-13 07:57:17,730 | INFO | FT 9 [1501/10009] loss=1.3860
2026-02-13 07:58:07,724 | INFO | FT 9 [2001/10009] loss=1.3853
2026-02-13 07:58:57,729 | INFO | FT 9 [2501/10009] loss=1.3872
2026-02-13 07:59:47,722 | INFO | FT 9 [3001/10009] loss=1.3844
2026-02-13 08:00:37,725 | INFO | FT 9 [3501/10009] loss=1.3850
2026-02-13 08:01:27,728 | INFO | FT 9 [4001/10009] loss=1.3845
2026-02-13 08:02:17,823 | INFO | FT 9 [4501/10009] loss=1.3848
2026-02-13 08:03:07,827 | INFO | FT 9 [5001/10009] loss=1.3844
2026-02-13 08:03:57,824 | INFO | FT 9 [5501/10009] loss=1.3830
2026-02-13 08:04:47,822 | INFO | FT 9 [6001/10009] loss=1.3827
2026-02-13 08:05:37,872 | INFO | FT 9 [6501/10009] loss=1.3823
2026-02-13 08:06:27,931 | INFO | FT 9 [7001/10009] loss=1.3829
2026-02-13 08:07:17,923 | INFO | FT 9 [7501/10009] loss=1.3827
2026-02-13 08:08:08,029 | INFO | FT 9 [8001/10009] loss=1.3830
2026-02-13 08:08:58,023 | INFO | FT 9 [8501/10009] loss=1.3827
2026-02-13 08:09:48,031 | INFO | FT 9 [9001/10009] loss=1.3824
2026-02-13 08:10:37,931 | INFO | FT 9 [9501/10009] loss=1.3822
2026-02-13 08:11:27,939 | INFO | FT 9 [10001/10009] loss=1.3824
2026-02-13 08:11:28,742 | INFO | FT 9 [10009/10009] loss=1.3823
2026-02-13 08:12:36,915 | INFO | FT ep 9/10: train_loss=1.3823, val_acc=0.6807
2026-02-13 08:12:36,968 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_best.pth
2026-02-13 08:12:37,892 | INFO | FT 10 [1/10009] loss=1.2638
2026-02-13 08:13:27,924 | INFO | FT 10 [501/10009] loss=1.3795
2026-02-13 08:14:17,824 | INFO | FT 10 [1001/10009] loss=1.3740
2026-02-13 08:15:07,625 | INFO | FT 10 [1501/10009] loss=1.3751
2026-02-13 08:15:57,545 | INFO | FT 10 [2001/10009] loss=1.3737
2026-02-13 08:16:47,541 | INFO | FT 10 [2501/10009] loss=1.3762
2026-02-13 08:17:37,535 | INFO | FT 10 [3001/10009] loss=1.3756
2026-02-13 08:18:27,632 | INFO | FT 10 [3501/10009] loss=1.3756
2026-02-13 08:19:17,628 | INFO | FT 10 [4001/10009] loss=1.3754
2026-02-13 08:20:07,626 | INFO | FT 10 [4501/10009] loss=1.3757
2026-02-13 08:20:57,624 | INFO | FT 10 [5001/10009] loss=1.3763
2026-02-13 08:21:47,625 | INFO | FT 10 [5501/10009] loss=1.3758
2026-02-13 08:22:37,639 | INFO | FT 10 [6001/10009] loss=1.3761
2026-02-13 08:23:27,629 | INFO | FT 10 [6501/10009] loss=1.3751
2026-02-13 08:24:17,640 | INFO | FT 10 [7001/10009] loss=1.3755
2026-02-13 08:25:07,533 | INFO | FT 10 [7501/10009] loss=1.3743
2026-02-13 08:25:57,169 | INFO | FT 10 [8001/10009] loss=1.3746
2026-02-13 08:26:46,712 | INFO | FT 10 [8501/10009] loss=1.3747
2026-02-13 08:27:36,157 | INFO | FT 10 [9001/10009] loss=1.3739
2026-02-13 08:28:25,620 | INFO | FT 10 [9501/10009] loss=1.3742
2026-02-13 08:29:15,003 | INFO | FT 10 [10001/10009] loss=1.3745
2026-02-13 08:29:15,805 | INFO | FT 10 [10009/10009] loss=1.3745
2026-02-13 08:30:18,465 | INFO | FT ep 10/10: train_loss=1.3745, val_acc=0.6805
2026-02-13 08:31:28,275 | INFO | ============================================================
2026-02-13 08:31:28,275 | INFO | Summary
2026-02-13 08:31:28,275 | INFO | ============================================================
2026-02-13 08:31:28,275 | INFO | Base MACs:    0.32G -> Pruned: 0.27G (84.8%)
2026-02-13 08:31:28,275 | INFO | Base Params:  3.50M -> Pruned: 2.92M (83.4%)
2026-02-13 08:31:28,275 | INFO | Original Acc: 0.7187
2026-02-13 08:31:28,275 | INFO | Final Acc:    0.6805
2026-02-13 08:31:28,275 | INFO | Best Acc:     0.6807
2026-02-13 08:31:28,322 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.8/vbp_final.pth
