2026-02-13 05:29:43,409 | INFO | Logging to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_imagenet.log
2026-02-13 05:29:43,409 | INFO | ============================================================
2026-02-13 05:29:43,409 | INFO | VBP ImageNet Reproduction Script
2026-02-13 05:29:43,409 | INFO | ============================================================
2026-02-13 05:29:43,409 | INFO |   model_type: cnn
2026-02-13 05:29:43,409 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/MNv2_TP/mobilenet_v2_weights.pth
2026-02-13 05:29:43,409 | INFO |   cnn_arch: mobilenet_v2
2026-02-13 05:29:43,409 | INFO |   pretrained: True
2026-02-13 05:29:43,409 | INFO |   interior_only: True
2026-02-13 05:29:43,409 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 05:29:43,409 | INFO |   train_batch_size: 64
2026-02-13 05:29:43,409 | INFO |   val_batch_size: 128
2026-02-13 05:29:43,409 | INFO |   num_workers: 4
2026-02-13 05:29:43,409 | INFO |   max_batches: 200
2026-02-13 05:29:43,409 | INFO |   keep_ratio: 0.9
2026-02-13 05:29:43,409 | INFO |   global_pruning: True
2026-02-13 05:29:43,409 | INFO |   max_pruning_ratio: 1.0
2026-02-13 05:29:43,409 | INFO |   norm_per_layer: False
2026-02-13 05:29:43,409 | INFO |   no_compensation: False
2026-02-13 05:29:43,409 | INFO |   no_recalib: False
2026-02-13 05:29:43,409 | INFO |   criterion: variance
2026-02-13 05:29:43,410 | INFO |   epochs_ft: 10
2026-02-13 05:29:43,410 | INFO |   lr_ft: 0.0001
2026-02-13 05:29:43,410 | INFO |   opt_ft: adamw
2026-02-13 05:29:43,410 | INFO |   momentum_ft: 0.9
2026-02-13 05:29:43,410 | INFO |   wd_ft: None
2026-02-13 05:29:43,410 | INFO |   use_kd: True
2026-02-13 05:29:43,410 | INFO |   kd_alpha: 0.7
2026-02-13 05:29:43,410 | INFO |   kd_T: 2.0
2026-02-13 05:29:43,410 | INFO |   pat: False
2026-02-13 05:29:43,410 | INFO |   pat_steps: 5
2026-02-13 05:29:43,410 | INFO |   pat_epochs_per_step: 3
2026-02-13 05:29:43,410 | INFO |   var_loss_weight: 0.0
2026-02-13 05:29:43,410 | INFO |   sparse_mode: none
2026-02-13 05:29:43,410 | INFO |   epochs_sparse: 5
2026-02-13 05:29:43,410 | INFO |   lr_sparse: 0.0001
2026-02-13 05:29:43,410 | INFO |   l1_lambda: 0.0001
2026-02-13 05:29:43,410 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 05:29:43,410 | INFO |   disable_ddp: False
2026-02-13 05:29:43,410 | INFO |   local_rank: 0
2026-02-13 05:29:43,410 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9
2026-02-13 05:29:43,410 | INFO |   rank: 0
2026-02-13 05:29:43,410 | INFO |   world_size: 2
2026-02-13 05:29:43,411 | INFO | Loading ImageNet dataset...
2026-02-13 05:29:43,411 | INFO | Using cached sample lists for fast loading
2026-02-13 05:29:43,766 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 05:29:45,617 | INFO | Loaded mobilenet_v2 (pretrained=True)
2026-02-13 05:29:46,970 | INFO | Baseline: 0.32G MACs, 3.50M params
2026-02-13 05:29:46,971 | INFO | Evaluating original model...
2026-02-13 05:30:53,283 | INFO | Original accuracy: 0.7187, loss: 1.1474
2026-02-13 05:30:53,310 | INFO | Created teacher model for knowledge distillation
2026-02-13 05:30:53,310 | INFO | PAT: 1 steps, per_step_keep=0.9000, epochs_per_step=0, target_keep=0.9, criterion=variance
2026-02-13 05:30:53,311 | INFO | 
============================================================
2026-02-13 05:30:53,311 | INFO | PAT Step 1/1
2026-02-13 05:30:53,311 | INFO | ============================================================
2026-02-13 05:30:53,336 | INFO | Auto-detected 35 CNN target layers for stats
2026-02-13 05:30:53,337 | INFO | Collecting activation variance statistics...
2026-02-13 05:31:08,915 | INFO | Statistics collected for 35 layers
2026-02-13 05:31:08,916 | INFO |   Conv2d: mean_var=0.027814
2026-02-13 05:31:08,916 | INFO |   Conv2d: mean_var=0.381585
2026-02-13 05:31:08,916 | INFO |   Conv2d: mean_var=0.045580
2026-02-13 05:31:08,916 | INFO |   Conv2d: mean_var=0.290806
2026-02-13 05:31:08,916 | INFO |   Conv2d: mean_var=0.015335
2026-02-13 05:31:09,115 | INFO | Variance â€” entropy=0.8414, cv=2.1450, gini=0.8129
2026-02-13 05:31:09,396 | INFO | Pruning: per_step_prune=0.1000
2026-02-13 05:31:09,615 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 05:31:09,615 | INFO | Recalibrating BN running stats...
2026-02-13 05:31:10,402 | INFO | BN recalib [1/100]
2026-02-13 05:31:10,598 | INFO | BN recalib [6/100]
2026-02-13 05:31:10,939 | INFO | BN recalib [11/100]
2026-02-13 05:31:11,688 | INFO | BN recalib [16/100]
2026-02-13 05:31:12,334 | INFO | BN recalib [21/100]
2026-02-13 05:31:12,603 | INFO | BN recalib [26/100]
2026-02-13 05:31:12,983 | INFO | BN recalib [31/100]
2026-02-13 05:31:13,283 | INFO | BN recalib [36/100]
2026-02-13 05:31:13,928 | INFO | BN recalib [41/100]
2026-02-13 05:31:14,244 | INFO | BN recalib [46/100]
2026-02-13 05:31:14,580 | INFO | BN recalib [51/100]
2026-02-13 05:31:14,888 | INFO | BN recalib [56/100]
2026-02-13 05:31:15,487 | INFO | BN recalib [61/100]
2026-02-13 05:31:15,773 | INFO | BN recalib [66/100]
2026-02-13 05:31:16,074 | INFO | BN recalib [71/100]
2026-02-13 05:31:16,389 | INFO | BN recalib [76/100]
2026-02-13 05:31:17,025 | INFO | BN recalib [81/100]
2026-02-13 05:31:17,341 | INFO | BN recalib [86/100]
2026-02-13 05:31:17,635 | INFO | BN recalib [91/100]
2026-02-13 05:31:18,008 | INFO | BN recalib [96/100]
2026-02-13 05:31:18,285 | INFO | BN recalib [100/100]
2026-02-13 05:31:18,926 | INFO | BN recalibration done (100 batches)
2026-02-13 05:32:24,880 | INFO | Step 1 retention: acc=0.2557, loss=3.4291
2026-02-13 05:32:24,881 | INFO |   cumulative_keep=0.9000, MACs=0.30G, params=3.14M
2026-02-13 05:32:24,881 | INFO | 
Post-prune fine-tuning for 10 epochs (adamw, lr=0.0001, wd=0.01)...
2026-02-13 05:32:25,994 | INFO | FT 1 [1/10009] loss=4.2319
2026-02-13 05:33:16,760 | INFO | FT 1 [501/10009] loss=2.4692
2026-02-13 05:34:07,327 | INFO | FT 1 [1001/10009] loss=2.2356
2026-02-13 05:34:57,845 | INFO | FT 1 [1501/10009] loss=2.1093
2026-02-13 05:35:48,452 | INFO | FT 1 [2001/10009] loss=2.0296
2026-02-13 05:36:39,261 | INFO | FT 1 [2501/10009] loss=1.9738
2026-02-13 05:37:29,872 | INFO | FT 1 [3001/10009] loss=1.9290
2026-02-13 05:38:20,680 | INFO | FT 1 [3501/10009] loss=1.8967
2026-02-13 05:39:11,447 | INFO | FT 1 [4001/10009] loss=1.8671
2026-02-13 05:40:02,049 | INFO | FT 1 [4501/10009] loss=1.8440
2026-02-13 05:40:52,734 | INFO | FT 1 [5001/10009] loss=1.8237
2026-02-13 05:41:43,434 | INFO | FT 1 [5501/10009] loss=1.8057
2026-02-13 05:42:34,051 | INFO | FT 1 [6001/10009] loss=1.7914
2026-02-13 05:43:24,502 | INFO | FT 1 [6501/10009] loss=1.7774
2026-02-13 05:44:15,200 | INFO | FT 1 [7001/10009] loss=1.7653
2026-02-13 05:45:05,780 | INFO | FT 1 [7501/10009] loss=1.7543
2026-02-13 05:45:56,325 | INFO | FT 1 [8001/10009] loss=1.7447
2026-02-13 05:46:46,825 | INFO | FT 1 [8501/10009] loss=1.7355
2026-02-13 05:47:37,439 | INFO | FT 1 [9001/10009] loss=1.7275
2026-02-13 05:48:27,968 | INFO | FT 1 [9501/10009] loss=1.7194
2026-02-13 05:49:18,271 | INFO | FT 1 [10001/10009] loss=1.7122
2026-02-13 05:49:19,081 | INFO | FT 1 [10009/10009] loss=1.7122
2026-02-13 05:50:22,620 | INFO | FT ep 1/10: train_loss=1.7122, val_acc=0.6526
2026-02-13 05:50:22,670 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 05:50:23,569 | INFO | FT 2 [1/10009] loss=1.3443
2026-02-13 05:51:13,883 | INFO | FT 2 [501/10009] loss=1.5413
2026-02-13 05:52:04,460 | INFO | FT 2 [1001/10009] loss=1.5387
2026-02-13 05:52:54,981 | INFO | FT 2 [1501/10009] loss=1.5395
2026-02-13 05:53:45,563 | INFO | FT 2 [2001/10009] loss=1.5388
2026-02-13 05:54:36,126 | INFO | FT 2 [2501/10009] loss=1.5368
2026-02-13 05:55:26,840 | INFO | FT 2 [3001/10009] loss=1.5324
2026-02-13 05:56:17,481 | INFO | FT 2 [3501/10009] loss=1.5318
2026-02-13 05:57:08,072 | INFO | FT 2 [4001/10009] loss=1.5328
2026-02-13 05:57:58,624 | INFO | FT 2 [4501/10009] loss=1.5309
2026-02-13 05:58:49,140 | INFO | FT 2 [5001/10009] loss=1.5295
2026-02-13 05:59:39,712 | INFO | FT 2 [5501/10009] loss=1.5282
2026-02-13 06:00:30,063 | INFO | FT 2 [6001/10009] loss=1.5266
2026-02-13 06:01:20,617 | INFO | FT 2 [6501/10009] loss=1.5267
2026-02-13 06:02:11,260 | INFO | FT 2 [7001/10009] loss=1.5254
2026-02-13 06:03:01,811 | INFO | FT 2 [7501/10009] loss=1.5238
2026-02-13 06:03:52,396 | INFO | FT 2 [8001/10009] loss=1.5232
2026-02-13 06:04:42,999 | INFO | FT 2 [8501/10009] loss=1.5224
2026-02-13 06:05:33,551 | INFO | FT 2 [9001/10009] loss=1.5225
2026-02-13 06:06:24,080 | INFO | FT 2 [9501/10009] loss=1.5218
2026-02-13 06:07:14,373 | INFO | FT 2 [10001/10009] loss=1.5202
2026-02-13 06:07:15,194 | INFO | FT 2 [10009/10009] loss=1.5201
2026-02-13 06:08:19,919 | INFO | FT ep 2/10: train_loss=1.5201, val_acc=0.6629
2026-02-13 06:08:19,969 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 06:08:20,895 | INFO | FT 3 [1/10009] loss=1.5267
2026-02-13 06:09:11,547 | INFO | FT 3 [501/10009] loss=1.4583
2026-02-13 06:10:02,195 | INFO | FT 3 [1001/10009] loss=1.4652
2026-02-13 06:10:52,819 | INFO | FT 3 [1501/10009] loss=1.4676
2026-02-13 06:11:43,465 | INFO | FT 3 [2001/10009] loss=1.4693
2026-02-13 06:12:33,987 | INFO | FT 3 [2501/10009] loss=1.4671
2026-02-13 06:13:24,661 | INFO | FT 3 [3001/10009] loss=1.4656
2026-02-13 06:14:15,475 | INFO | FT 3 [3501/10009] loss=1.4640
2026-02-13 06:15:06,188 | INFO | FT 3 [4001/10009] loss=1.4641
2026-02-13 06:15:56,951 | INFO | FT 3 [4501/10009] loss=1.4639
2026-02-13 06:16:47,870 | INFO | FT 3 [5001/10009] loss=1.4665
2026-02-13 06:17:38,489 | INFO | FT 3 [5501/10009] loss=1.4670
2026-02-13 06:18:29,053 | INFO | FT 3 [6001/10009] loss=1.4671
2026-02-13 06:19:19,635 | INFO | FT 3 [6501/10009] loss=1.4662
2026-02-13 06:20:10,052 | INFO | FT 3 [7001/10009] loss=1.4665
2026-02-13 06:21:00,497 | INFO | FT 3 [7501/10009] loss=1.4667
2026-02-13 06:21:51,025 | INFO | FT 3 [8001/10009] loss=1.4655
2026-02-13 06:22:41,462 | INFO | FT 3 [8501/10009] loss=1.4653
2026-02-13 06:23:32,177 | INFO | FT 3 [9001/10009] loss=1.4639
2026-02-13 06:24:23,099 | INFO | FT 3 [9501/10009] loss=1.4631
2026-02-13 06:25:13,762 | INFO | FT 3 [10001/10009] loss=1.4630
2026-02-13 06:25:14,585 | INFO | FT 3 [10009/10009] loss=1.4631
2026-02-13 06:26:19,186 | INFO | FT ep 3/10: train_loss=1.4631, val_acc=0.6680
2026-02-13 06:26:19,251 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 06:26:20,126 | INFO | FT 4 [1/10009] loss=1.3970
2026-02-13 06:27:10,726 | INFO | FT 4 [501/10009] loss=1.4171
2026-02-13 06:28:01,507 | INFO | FT 4 [1001/10009] loss=1.4221
2026-02-13 06:28:52,350 | INFO | FT 4 [1501/10009] loss=1.4230
2026-02-13 06:29:43,275 | INFO | FT 4 [2001/10009] loss=1.4195
2026-02-13 06:30:34,063 | INFO | FT 4 [2501/10009] loss=1.4181
2026-02-13 06:31:24,889 | INFO | FT 4 [3001/10009] loss=1.4210
2026-02-13 06:32:15,772 | INFO | FT 4 [3501/10009] loss=1.4221
2026-02-13 06:33:06,634 | INFO | FT 4 [4001/10009] loss=1.4219
2026-02-13 06:33:57,423 | INFO | FT 4 [4501/10009] loss=1.4209
2026-02-13 06:34:48,209 | INFO | FT 4 [5001/10009] loss=1.4224
2026-02-13 06:35:39,127 | INFO | FT 4 [5501/10009] loss=1.4220
2026-02-13 06:36:29,991 | INFO | FT 4 [6001/10009] loss=1.4228
2026-02-13 06:37:20,855 | INFO | FT 4 [6501/10009] loss=1.4223
2026-02-13 06:38:11,763 | INFO | FT 4 [7001/10009] loss=1.4214
2026-02-13 06:39:02,682 | INFO | FT 4 [7501/10009] loss=1.4203
2026-02-13 06:39:53,503 | INFO | FT 4 [8001/10009] loss=1.4191
2026-02-13 06:40:44,290 | INFO | FT 4 [8501/10009] loss=1.4180
2026-02-13 06:41:35,102 | INFO | FT 4 [9001/10009] loss=1.4170
2026-02-13 06:42:25,844 | INFO | FT 4 [9501/10009] loss=1.4168
2026-02-13 06:43:16,486 | INFO | FT 4 [10001/10009] loss=1.4157
2026-02-13 06:43:17,308 | INFO | FT 4 [10009/10009] loss=1.4157
2026-02-13 06:44:26,033 | INFO | FT ep 4/10: train_loss=1.4157, val_acc=0.6743
2026-02-13 06:44:26,084 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 06:44:26,995 | INFO | FT 5 [1/10009] loss=1.3857
2026-02-13 06:45:17,571 | INFO | FT 5 [501/10009] loss=1.3578
2026-02-13 06:46:08,440 | INFO | FT 5 [1001/10009] loss=1.3651
2026-02-13 06:46:59,316 | INFO | FT 5 [1501/10009] loss=1.3727
2026-02-13 06:47:50,221 | INFO | FT 5 [2001/10009] loss=1.3731
2026-02-13 06:48:41,156 | INFO | FT 5 [2501/10009] loss=1.3727
2026-02-13 06:49:32,185 | INFO | FT 5 [3001/10009] loss=1.3730
2026-02-13 06:50:22,949 | INFO | FT 5 [3501/10009] loss=1.3740
2026-02-13 06:51:13,817 | INFO | FT 5 [4001/10009] loss=1.3747
2026-02-13 06:52:04,575 | INFO | FT 5 [4501/10009] loss=1.3747
2026-02-13 06:52:55,464 | INFO | FT 5 [5001/10009] loss=1.3744
2026-02-13 06:53:46,303 | INFO | FT 5 [5501/10009] loss=1.3740
2026-02-13 06:54:37,133 | INFO | FT 5 [6001/10009] loss=1.3739
2026-02-13 06:55:27,883 | INFO | FT 5 [6501/10009] loss=1.3737
2026-02-13 06:56:18,656 | INFO | FT 5 [7001/10009] loss=1.3725
2026-02-13 06:57:09,483 | INFO | FT 5 [7501/10009] loss=1.3718
2026-02-13 06:58:00,207 | INFO | FT 5 [8001/10009] loss=1.3713
2026-02-13 06:58:51,062 | INFO | FT 5 [8501/10009] loss=1.3707
2026-02-13 06:59:41,687 | INFO | FT 5 [9001/10009] loss=1.3701
2026-02-13 07:00:31,726 | INFO | FT 5 [9501/10009] loss=1.3703
2026-02-13 07:01:22,442 | INFO | FT 5 [10001/10009] loss=1.3695
2026-02-13 07:01:23,265 | INFO | FT 5 [10009/10009] loss=1.3694
2026-02-13 07:02:31,488 | INFO | FT ep 5/10: train_loss=1.3694, val_acc=0.6810
2026-02-13 07:02:31,540 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 07:02:32,446 | INFO | FT 6 [1/10009] loss=1.6972
2026-02-13 07:03:23,175 | INFO | FT 6 [501/10009] loss=1.3478
2026-02-13 07:04:14,128 | INFO | FT 6 [1001/10009] loss=1.3458
2026-02-13 07:05:05,056 | INFO | FT 6 [1501/10009] loss=1.3393
2026-02-13 07:05:55,977 | INFO | FT 6 [2001/10009] loss=1.3402
2026-02-13 07:06:46,737 | INFO | FT 6 [2501/10009] loss=1.3387
2026-02-13 07:07:37,564 | INFO | FT 6 [3001/10009] loss=1.3391
2026-02-13 07:08:28,472 | INFO | FT 6 [3501/10009] loss=1.3405
2026-02-13 07:09:19,408 | INFO | FT 6 [4001/10009] loss=1.3411
2026-02-13 07:10:10,127 | INFO | FT 6 [4501/10009] loss=1.3406
2026-02-13 07:11:00,893 | INFO | FT 6 [5001/10009] loss=1.3395
2026-02-13 07:11:51,820 | INFO | FT 6 [5501/10009] loss=1.3387
2026-02-13 07:12:42,860 | INFO | FT 6 [6001/10009] loss=1.3378
2026-02-13 07:13:33,835 | INFO | FT 6 [6501/10009] loss=1.3375
2026-02-13 07:14:24,850 | INFO | FT 6 [7001/10009] loss=1.3374
2026-02-13 07:15:15,882 | INFO | FT 6 [7501/10009] loss=1.3365
2026-02-13 07:16:06,918 | INFO | FT 6 [8001/10009] loss=1.3364
2026-02-13 07:16:57,942 | INFO | FT 6 [8501/10009] loss=1.3354
2026-02-13 07:17:48,711 | INFO | FT 6 [9001/10009] loss=1.3340
2026-02-13 07:18:39,526 | INFO | FT 6 [9501/10009] loss=1.3324
2026-02-13 07:19:30,255 | INFO | FT 6 [10001/10009] loss=1.3322
2026-02-13 07:19:31,085 | INFO | FT 6 [10009/10009] loss=1.3321
2026-02-13 07:20:40,576 | INFO | FT ep 6/10: train_loss=1.3321, val_acc=0.6852
2026-02-13 07:20:40,629 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 07:20:41,535 | INFO | FT 7 [1/10009] loss=1.3090
2026-02-13 07:21:32,496 | INFO | FT 7 [501/10009] loss=1.2974
2026-02-13 07:22:23,716 | INFO | FT 7 [1001/10009] loss=1.2993
2026-02-13 07:23:14,964 | INFO | FT 7 [1501/10009] loss=1.2979
2026-02-13 07:24:06,339 | INFO | FT 7 [2001/10009] loss=1.2982
2026-02-13 07:24:57,441 | INFO | FT 7 [2501/10009] loss=1.2991
2026-02-13 07:25:48,753 | INFO | FT 7 [3001/10009] loss=1.2990
2026-02-13 07:26:40,157 | INFO | FT 7 [3501/10009] loss=1.2989
2026-02-13 07:27:31,580 | INFO | FT 7 [4001/10009] loss=1.3011
2026-02-13 07:28:22,898 | INFO | FT 7 [4501/10009] loss=1.2998
2026-02-13 07:29:14,132 | INFO | FT 7 [5001/10009] loss=1.2993
2026-02-13 07:30:05,137 | INFO | FT 7 [5501/10009] loss=1.2981
2026-02-13 07:30:56,227 | INFO | FT 7 [6001/10009] loss=1.2979
2026-02-13 07:31:47,331 | INFO | FT 7 [6501/10009] loss=1.2970
2026-02-13 07:32:38,487 | INFO | FT 7 [7001/10009] loss=1.2968
2026-02-13 07:33:29,472 | INFO | FT 7 [7501/10009] loss=1.2965
2026-02-13 07:34:20,618 | INFO | FT 7 [8001/10009] loss=1.2968
2026-02-13 07:35:11,524 | INFO | FT 7 [8501/10009] loss=1.2959
2026-02-13 07:36:02,343 | INFO | FT 7 [9001/10009] loss=1.2955
2026-02-13 07:36:53,101 | INFO | FT 7 [9501/10009] loss=1.2952
2026-02-13 07:37:43,644 | INFO | FT 7 [10001/10009] loss=1.2949
2026-02-13 07:37:44,459 | INFO | FT 7 [10009/10009] loss=1.2948
2026-02-13 07:38:48,078 | INFO | FT ep 7/10: train_loss=1.2948, val_acc=0.6877
2026-02-13 07:38:48,129 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 07:38:49,024 | INFO | FT 8 [1/10009] loss=1.3794
2026-02-13 07:39:39,669 | INFO | FT 8 [501/10009] loss=1.2717
2026-02-13 07:40:30,469 | INFO | FT 8 [1001/10009] loss=1.2678
2026-02-13 07:41:21,453 | INFO | FT 8 [1501/10009] loss=1.2681
2026-02-13 07:42:12,377 | INFO | FT 8 [2001/10009] loss=1.2736
2026-02-13 07:43:03,379 | INFO | FT 8 [2501/10009] loss=1.2725
2026-02-13 07:43:54,586 | INFO | FT 8 [3001/10009] loss=1.2721
2026-02-13 07:44:45,560 | INFO | FT 8 [3501/10009] loss=1.2708
2026-02-13 07:45:36,563 | INFO | FT 8 [4001/10009] loss=1.2703
2026-02-13 07:46:27,767 | INFO | FT 8 [4501/10009] loss=1.2688
2026-02-13 07:47:18,817 | INFO | FT 8 [5001/10009] loss=1.2675
2026-02-13 07:48:10,025 | INFO | FT 8 [5501/10009] loss=1.2665
2026-02-13 07:49:01,377 | INFO | FT 8 [6001/10009] loss=1.2657
2026-02-13 07:49:52,679 | INFO | FT 8 [6501/10009] loss=1.2654
2026-02-13 07:50:43,847 | INFO | FT 8 [7001/10009] loss=1.2655
2026-02-13 07:51:35,011 | INFO | FT 8 [7501/10009] loss=1.2646
2026-02-13 07:52:25,961 | INFO | FT 8 [8001/10009] loss=1.2645
2026-02-13 07:53:16,736 | INFO | FT 8 [8501/10009] loss=1.2646
2026-02-13 07:54:07,397 | INFO | FT 8 [9001/10009] loss=1.2642
2026-02-13 07:54:58,166 | INFO | FT 8 [9501/10009] loss=1.2641
2026-02-13 07:55:49,092 | INFO | FT 8 [10001/10009] loss=1.2634
2026-02-13 07:55:49,925 | INFO | FT 8 [10009/10009] loss=1.2635
2026-02-13 07:57:00,227 | INFO | FT ep 8/10: train_loss=1.2635, val_acc=0.6925
2026-02-13 07:57:00,283 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 07:57:01,150 | INFO | FT 9 [1/10009] loss=1.6221
2026-02-13 07:57:52,425 | INFO | FT 9 [501/10009] loss=1.2630
2026-02-13 07:58:43,694 | INFO | FT 9 [1001/10009] loss=1.2508
2026-02-13 07:59:34,981 | INFO | FT 9 [1501/10009] loss=1.2446
2026-02-13 08:00:26,253 | INFO | FT 9 [2001/10009] loss=1.2470
2026-02-13 08:01:17,493 | INFO | FT 9 [2501/10009] loss=1.2463
2026-02-13 08:02:08,662 | INFO | FT 9 [3001/10009] loss=1.2445
2026-02-13 08:02:59,719 | INFO | FT 9 [3501/10009] loss=1.2453
2026-02-13 08:03:50,835 | INFO | FT 9 [4001/10009] loss=1.2456
2026-02-13 08:04:41,950 | INFO | FT 9 [4501/10009] loss=1.2461
2026-02-13 08:05:33,095 | INFO | FT 9 [5001/10009] loss=1.2452
2026-02-13 08:06:24,150 | INFO | FT 9 [5501/10009] loss=1.2453
2026-02-13 08:07:15,367 | INFO | FT 9 [6001/10009] loss=1.2456
2026-02-13 08:08:06,566 | INFO | FT 9 [6501/10009] loss=1.2440
2026-02-13 08:08:57,596 | INFO | FT 9 [7001/10009] loss=1.2441
2026-02-13 08:09:48,485 | INFO | FT 9 [7501/10009] loss=1.2441
2026-02-13 08:10:39,651 | INFO | FT 9 [8001/10009] loss=1.2443
2026-02-13 08:11:30,901 | INFO | FT 9 [8501/10009] loss=1.2439
2026-02-13 08:12:21,660 | INFO | FT 9 [9001/10009] loss=1.2436
2026-02-13 08:13:12,490 | INFO | FT 9 [9501/10009] loss=1.2435
2026-02-13 08:14:03,377 | INFO | FT 9 [10001/10009] loss=1.2439
2026-02-13 08:14:04,192 | INFO | FT 9 [10009/10009] loss=1.2438
2026-02-13 08:15:13,853 | INFO | FT ep 9/10: train_loss=1.2438, val_acc=0.6946
2026-02-13 08:15:13,909 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_best.pth
2026-02-13 08:15:14,835 | INFO | FT 10 [1/10009] loss=1.3953
2026-02-13 08:16:05,375 | INFO | FT 10 [501/10009] loss=1.2505
2026-02-13 08:16:56,586 | INFO | FT 10 [1001/10009] loss=1.2376
2026-02-13 08:17:48,067 | INFO | FT 10 [1501/10009] loss=1.2364
2026-02-13 08:18:39,500 | INFO | FT 10 [2001/10009] loss=1.2320
2026-02-13 08:19:30,799 | INFO | FT 10 [2501/10009] loss=1.2370
2026-02-13 08:20:21,992 | INFO | FT 10 [3001/10009] loss=1.2382
2026-02-13 08:21:13,088 | INFO | FT 10 [3501/10009] loss=1.2385
2026-02-13 08:22:04,292 | INFO | FT 10 [4001/10009] loss=1.2382
2026-02-13 08:22:55,393 | INFO | FT 10 [4501/10009] loss=1.2384
2026-02-13 08:23:46,510 | INFO | FT 10 [5001/10009] loss=1.2393
2026-02-13 08:24:37,597 | INFO | FT 10 [5501/10009] loss=1.2398
2026-02-13 08:25:28,678 | INFO | FT 10 [6001/10009] loss=1.2398
2026-02-13 08:26:19,741 | INFO | FT 10 [6501/10009] loss=1.2396
2026-02-13 08:27:10,453 | INFO | FT 10 [7001/10009] loss=1.2395
2026-02-13 08:28:01,187 | INFO | FT 10 [7501/10009] loss=1.2385
2026-02-13 08:28:51,955 | INFO | FT 10 [8001/10009] loss=1.2382
2026-02-13 08:29:42,711 | INFO | FT 10 [8501/10009] loss=1.2382
2026-02-13 08:30:33,529 | INFO | FT 10 [9001/10009] loss=1.2377
2026-02-13 08:31:24,082 | INFO | FT 10 [9501/10009] loss=1.2374
2026-02-13 08:32:14,810 | INFO | FT 10 [10001/10009] loss=1.2377
2026-02-13 08:32:15,625 | INFO | FT 10 [10009/10009] loss=1.2376
2026-02-13 08:33:19,641 | INFO | FT ep 10/10: train_loss=1.2376, val_acc=0.6941
2026-02-13 08:34:25,288 | INFO | ============================================================
2026-02-13 08:34:25,288 | INFO | Summary
2026-02-13 08:34:25,289 | INFO | ============================================================
2026-02-13 08:34:25,289 | INFO | Base MACs:    0.32G -> Pruned: 0.30G (93.3%)
2026-02-13 08:34:25,289 | INFO | Base Params:  3.50M -> Pruned: 3.14M (89.5%)
2026-02-13 08:34:25,289 | INFO | Original Acc: 0.7187
2026-02-13 08:34:25,289 | INFO | Final Acc:    0.6941
2026-02-13 08:34:25,289 | INFO | Best Acc:     0.6946
2026-02-13 08:34:25,337 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/MNv2_TP/global/kr_0.9/vbp_final.pth
