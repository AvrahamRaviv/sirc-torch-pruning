2026-02-13 05:02:22,611 | INFO | Logging to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_imagenet.log
2026-02-13 05:02:22,611 | INFO | ============================================================
2026-02-13 05:02:22,611 | INFO | VBP ImageNet Reproduction Script
2026-02-13 05:02:22,612 | INFO | ============================================================
2026-02-13 05:02:22,612 | INFO |   model_type: cnn
2026-02-13 05:02:22,612 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/ResNet50_TP/resnet50_imagenet1k.pth
2026-02-13 05:02:22,612 | INFO |   cnn_arch: resnet50
2026-02-13 05:02:22,612 | INFO |   pretrained: True
2026-02-13 05:02:22,612 | INFO |   interior_only: True
2026-02-13 05:02:22,612 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 05:02:22,612 | INFO |   train_batch_size: 64
2026-02-13 05:02:22,612 | INFO |   val_batch_size: 128
2026-02-13 05:02:22,612 | INFO |   num_workers: 4
2026-02-13 05:02:22,612 | INFO |   max_batches: 200
2026-02-13 05:02:22,612 | INFO |   keep_ratio: 0.8
2026-02-13 05:02:22,612 | INFO |   global_pruning: True
2026-02-13 05:02:22,612 | INFO |   max_pruning_ratio: 1.0
2026-02-13 05:02:22,612 | INFO |   norm_per_layer: False
2026-02-13 05:02:22,612 | INFO |   no_compensation: False
2026-02-13 05:02:22,612 | INFO |   no_recalib: False
2026-02-13 05:02:22,612 | INFO |   criterion: variance
2026-02-13 05:02:22,612 | INFO |   epochs_ft: 10
2026-02-13 05:02:22,612 | INFO |   lr_ft: 0.01
2026-02-13 05:02:22,612 | INFO |   opt_ft: sgd
2026-02-13 05:02:22,612 | INFO |   momentum_ft: 0.9
2026-02-13 05:02:22,612 | INFO |   wd_ft: 0.0001
2026-02-13 05:02:22,612 | INFO |   use_kd: True
2026-02-13 05:02:22,612 | INFO |   kd_alpha: 0.7
2026-02-13 05:02:22,612 | INFO |   kd_T: 2.0
2026-02-13 05:02:22,612 | INFO |   pat: False
2026-02-13 05:02:22,613 | INFO |   pat_steps: 5
2026-02-13 05:02:22,613 | INFO |   pat_epochs_per_step: 3
2026-02-13 05:02:22,613 | INFO |   var_loss_weight: 0.0
2026-02-13 05:02:22,613 | INFO |   sparse_mode: none
2026-02-13 05:02:22,613 | INFO |   epochs_sparse: 5
2026-02-13 05:02:22,613 | INFO |   lr_sparse: 0.0001
2026-02-13 05:02:22,613 | INFO |   l1_lambda: 0.0001
2026-02-13 05:02:22,613 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 05:02:22,613 | INFO |   disable_ddp: False
2026-02-13 05:02:22,613 | INFO |   local_rank: 0
2026-02-13 05:02:22,613 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8
2026-02-13 05:02:22,613 | INFO |   rank: 0
2026-02-13 05:02:22,613 | INFO |   world_size: 2
2026-02-13 05:02:22,613 | INFO | Loading ImageNet dataset...
2026-02-13 05:02:22,614 | INFO | Using cached sample lists for fast loading
2026-02-13 05:02:23,006 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 05:02:25,581 | INFO | Loaded resnet50 (pretrained=True)
2026-02-13 05:02:27,144 | INFO | Baseline: 4.12G MACs, 25.56M params
2026-02-13 05:02:27,145 | INFO | Evaluating original model...
2026-02-13 05:03:46,852 | INFO | Original accuracy: 0.8035, loss: 1.4124
2026-02-13 05:03:46,880 | INFO | Created teacher model for knowledge distillation
2026-02-13 05:03:46,880 | INFO | PAT: 1 steps, per_step_keep=0.8000, epochs_per_step=0, target_keep=0.8, criterion=variance
2026-02-13 05:03:46,880 | INFO | 
============================================================
2026-02-13 05:03:46,880 | INFO | PAT Step 1/1
2026-02-13 05:03:46,880 | INFO | ============================================================
2026-02-13 05:03:46,905 | INFO | Auto-detected 53 CNN target layers for stats
2026-02-13 05:03:46,905 | INFO | Collecting activation variance statistics...
2026-02-13 05:04:08,982 | INFO | Statistics collected for 53 layers
2026-02-13 05:04:08,983 | INFO |   Conv2d: mean_var=5.507903
2026-02-13 05:04:08,983 | INFO |   Conv2d: mean_var=1.089924
2026-02-13 05:04:08,983 | INFO |   Conv2d: mean_var=2.065964
2026-02-13 05:04:08,983 | INFO |   Conv2d: mean_var=13.723125
2026-02-13 05:04:08,983 | INFO |   Conv2d: mean_var=16.653915
2026-02-13 05:04:09,112 | INFO | Variance — entropy=0.9314, cv=1.4980, gini=0.6154
2026-02-13 05:04:09,158 | INFO | Pruning: per_step_prune=0.2000
2026-02-13 05:04:09,697 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 05:04:09,697 | INFO | Recalibrating BN running stats...
2026-02-13 05:04:10,610 | INFO | BN recalib [1/100]
2026-02-13 05:04:10,858 | INFO | BN recalib [6/100]
2026-02-13 05:04:11,201 | INFO | BN recalib [11/100]
2026-02-13 05:04:12,142 | INFO | BN recalib [16/100]
2026-02-13 05:04:12,674 | INFO | BN recalib [21/100]
2026-02-13 05:04:13,117 | INFO | BN recalib [26/100]
2026-02-13 05:04:13,485 | INFO | BN recalib [31/100]
2026-02-13 05:04:13,958 | INFO | BN recalib [36/100]
2026-02-13 05:04:14,606 | INFO | BN recalib [41/100]
2026-02-13 05:04:14,998 | INFO | BN recalib [46/100]
2026-02-13 05:04:15,358 | INFO | BN recalib [51/100]
2026-02-13 05:04:15,947 | INFO | BN recalib [56/100]
2026-02-13 05:04:16,327 | INFO | BN recalib [61/100]
2026-02-13 05:04:16,744 | INFO | BN recalib [66/100]
2026-02-13 05:04:17,175 | INFO | BN recalib [71/100]
2026-02-13 05:04:17,638 | INFO | BN recalib [76/100]
2026-02-13 05:04:18,103 | INFO | BN recalib [81/100]
2026-02-13 05:04:18,504 | INFO | BN recalib [86/100]
2026-02-13 05:04:19,044 | INFO | BN recalib [91/100]
2026-02-13 05:04:19,388 | INFO | BN recalib [96/100]
2026-02-13 05:04:19,874 | INFO | BN recalib [100/100]
2026-02-13 05:04:20,260 | INFO | BN recalibration done (100 batches)
2026-02-13 05:05:38,519 | INFO | Step 1 retention: acc=0.7459, loss=1.7498
2026-02-13 05:05:38,519 | INFO |   cumulative_keep=0.8000, MACs=3.48G, params=18.97M
2026-02-13 05:05:38,519 | INFO | 
Post-prune fine-tuning for 10 epochs (sgd, lr=0.01, wd=0.0001)...
2026-02-13 05:05:39,681 | INFO | FT 1 [1/10009] loss=1.4190
2026-02-13 05:07:13,916 | INFO | FT 1 [501/10009] loss=1.0468
2026-02-13 05:08:48,797 | INFO | FT 1 [1001/10009] loss=1.0280
2026-02-13 05:10:23,190 | INFO | FT 1 [1501/10009] loss=1.0162
2026-02-13 05:11:57,657 | INFO | FT 1 [2001/10009] loss=1.0072
2026-02-13 05:13:32,088 | INFO | FT 1 [2501/10009] loss=1.0028
2026-02-13 05:15:06,504 | INFO | FT 1 [3001/10009] loss=0.9991
2026-02-13 05:16:41,021 | INFO | FT 1 [3501/10009] loss=0.9966
2026-02-13 05:18:15,633 | INFO | FT 1 [4001/10009] loss=0.9934
2026-02-13 05:19:50,216 | INFO | FT 1 [4501/10009] loss=0.9930
2026-02-13 05:21:24,776 | INFO | FT 1 [5001/10009] loss=0.9923
2026-02-13 05:22:59,376 | INFO | FT 1 [5501/10009] loss=0.9901
2026-02-13 05:24:33,922 | INFO | FT 1 [6001/10009] loss=0.9888
2026-02-13 05:26:08,490 | INFO | FT 1 [6501/10009] loss=0.9874
2026-02-13 05:27:43,092 | INFO | FT 1 [7001/10009] loss=0.9861
2026-02-13 05:29:17,695 | INFO | FT 1 [7501/10009] loss=0.9849
2026-02-13 05:30:52,268 | INFO | FT 1 [8001/10009] loss=0.9843
2026-02-13 05:32:26,858 | INFO | FT 1 [8501/10009] loss=0.9837
2026-02-13 05:34:01,451 | INFO | FT 1 [9001/10009] loss=0.9835
2026-02-13 05:35:36,055 | INFO | FT 1 [9501/10009] loss=0.9829
2026-02-13 05:37:10,649 | INFO | FT 1 [10001/10009] loss=0.9827
2026-02-13 05:37:12,181 | INFO | FT 1 [10009/10009] loss=0.9827
2026-02-13 05:38:32,874 | INFO | FT ep 1/10: train_loss=0.9827, val_acc=0.7649
2026-02-13 05:38:33,053 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 05:38:34,147 | INFO | FT 2 [1/10009] loss=0.8568
2026-02-13 05:40:08,537 | INFO | FT 2 [501/10009] loss=0.9408
2026-02-13 05:41:43,100 | INFO | FT 2 [1001/10009] loss=0.9433
2026-02-13 05:43:17,757 | INFO | FT 2 [1501/10009] loss=0.9450
2026-02-13 05:44:52,356 | INFO | FT 2 [2001/10009] loss=0.9452
2026-02-13 05:46:27,002 | INFO | FT 2 [2501/10009] loss=0.9460
2026-02-13 05:48:01,602 | INFO | FT 2 [3001/10009] loss=0.9463
2026-02-13 05:49:36,209 | INFO | FT 2 [3501/10009] loss=0.9471
2026-02-13 05:51:10,815 | INFO | FT 2 [4001/10009] loss=0.9495
2026-02-13 05:52:45,391 | INFO | FT 2 [4501/10009] loss=0.9498
2026-02-13 05:54:19,973 | INFO | FT 2 [5001/10009] loss=0.9515
2026-02-13 05:55:54,574 | INFO | FT 2 [5501/10009] loss=0.9518
2026-02-13 05:57:29,164 | INFO | FT 2 [6001/10009] loss=0.9530
2026-02-13 05:59:03,758 | INFO | FT 2 [6501/10009] loss=0.9537
2026-02-13 06:00:38,385 | INFO | FT 2 [7001/10009] loss=0.9547
2026-02-13 06:02:12,980 | INFO | FT 2 [7501/10009] loss=0.9551
2026-02-13 06:03:47,570 | INFO | FT 2 [8001/10009] loss=0.9558
2026-02-13 06:05:22,129 | INFO | FT 2 [8501/10009] loss=0.9568
2026-02-13 06:06:56,665 | INFO | FT 2 [9001/10009] loss=0.9581
2026-02-13 06:08:31,255 | INFO | FT 2 [9501/10009] loss=0.9587
2026-02-13 06:10:05,808 | INFO | FT 2 [10001/10009] loss=0.9592
2026-02-13 06:10:07,338 | INFO | FT 2 [10009/10009] loss=0.9592
2026-02-13 06:11:30,483 | INFO | FT ep 2/10: train_loss=0.9592, val_acc=0.7637
2026-02-13 06:11:31,682 | INFO | FT 3 [1/10009] loss=1.1424
2026-02-13 06:13:06,005 | INFO | FT 3 [501/10009] loss=0.9369
2026-02-13 06:14:40,603 | INFO | FT 3 [1001/10009] loss=0.9359
2026-02-13 06:16:15,181 | INFO | FT 3 [1501/10009] loss=0.9399
2026-02-13 06:17:49,788 | INFO | FT 3 [2001/10009] loss=0.9397
2026-02-13 06:19:24,391 | INFO | FT 3 [2501/10009] loss=0.9396
2026-02-13 06:20:58,969 | INFO | FT 3 [3001/10009] loss=0.9411
2026-02-13 06:22:33,570 | INFO | FT 3 [3501/10009] loss=0.9422
2026-02-13 06:24:08,174 | INFO | FT 3 [4001/10009] loss=0.9444
2026-02-13 06:25:42,751 | INFO | FT 3 [4501/10009] loss=0.9449
2026-02-13 06:27:17,329 | INFO | FT 3 [5001/10009] loss=0.9474
2026-02-13 06:28:51,916 | INFO | FT 3 [5501/10009] loss=0.9478
2026-02-13 06:30:26,514 | INFO | FT 3 [6001/10009] loss=0.9477
2026-02-13 06:32:01,109 | INFO | FT 3 [6501/10009] loss=0.9489
2026-02-13 06:33:35,678 | INFO | FT 3 [7001/10009] loss=0.9502
2026-02-13 06:35:10,261 | INFO | FT 3 [7501/10009] loss=0.9511
2026-02-13 06:36:44,882 | INFO | FT 3 [8001/10009] loss=0.9518
2026-02-13 06:38:19,494 | INFO | FT 3 [8501/10009] loss=0.9529
2026-02-13 06:39:54,083 | INFO | FT 3 [9001/10009] loss=0.9524
2026-02-13 06:41:28,643 | INFO | FT 3 [9501/10009] loss=0.9531
2026-02-13 06:43:03,243 | INFO | FT 3 [10001/10009] loss=0.9539
2026-02-13 06:43:04,777 | INFO | FT 3 [10009/10009] loss=0.9540
2026-02-13 06:44:25,470 | INFO | FT ep 3/10: train_loss=0.9540, val_acc=0.7628
2026-02-13 06:44:26,564 | INFO | FT 4 [1/10009] loss=0.8814
2026-02-13 06:46:01,004 | INFO | FT 4 [501/10009] loss=0.9168
2026-02-13 06:47:35,660 | INFO | FT 4 [1001/10009] loss=0.9253
2026-02-13 06:49:10,382 | INFO | FT 4 [1501/10009] loss=0.9270
2026-02-13 06:50:45,131 | INFO | FT 4 [2001/10009] loss=0.9286
2026-02-13 06:52:19,836 | INFO | FT 4 [2501/10009] loss=0.9301
2026-02-13 06:53:54,588 | INFO | FT 4 [3001/10009] loss=0.9330
2026-02-13 06:55:29,235 | INFO | FT 4 [3501/10009] loss=0.9343
2026-02-13 06:57:03,841 | INFO | FT 4 [4001/10009] loss=0.9353
2026-02-13 06:58:38,487 | INFO | FT 4 [4501/10009] loss=0.9355
2026-02-13 07:00:13,144 | INFO | FT 4 [5001/10009] loss=0.9372
2026-02-13 07:01:47,820 | INFO | FT 4 [5501/10009] loss=0.9382
2026-02-13 07:03:22,486 | INFO | FT 4 [6001/10009] loss=0.9389
2026-02-13 07:04:57,112 | INFO | FT 4 [6501/10009] loss=0.9402
2026-02-13 07:06:31,756 | INFO | FT 4 [7001/10009] loss=0.9403
2026-02-13 07:08:06,417 | INFO | FT 4 [7501/10009] loss=0.9404
2026-02-13 07:09:41,016 | INFO | FT 4 [8001/10009] loss=0.9398
2026-02-13 07:11:15,604 | INFO | FT 4 [8501/10009] loss=0.9400
2026-02-13 07:12:50,204 | INFO | FT 4 [9001/10009] loss=0.9403
2026-02-13 07:14:24,783 | INFO | FT 4 [9501/10009] loss=0.9408
2026-02-13 07:15:59,333 | INFO | FT 4 [10001/10009] loss=0.9413
2026-02-13 07:16:00,867 | INFO | FT 4 [10009/10009] loss=0.9413
2026-02-13 07:17:20,848 | INFO | FT ep 4/10: train_loss=0.9413, val_acc=0.7658
2026-02-13 07:17:21,121 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 07:17:22,253 | INFO | FT 5 [1/10009] loss=0.8820
2026-02-13 07:18:56,534 | INFO | FT 5 [501/10009] loss=0.9088
2026-02-13 07:20:31,008 | INFO | FT 5 [1001/10009] loss=0.9124
2026-02-13 07:22:05,605 | INFO | FT 5 [1501/10009] loss=0.9143
2026-02-13 07:23:40,370 | INFO | FT 5 [2001/10009] loss=0.9157
2026-02-13 07:25:15,114 | INFO | FT 5 [2501/10009] loss=0.9175
2026-02-13 07:26:49,875 | INFO | FT 5 [3001/10009] loss=0.9160
2026-02-13 07:28:24,648 | INFO | FT 5 [3501/10009] loss=0.9167
2026-02-13 07:29:59,418 | INFO | FT 5 [4001/10009] loss=0.9174
2026-02-13 07:31:34,133 | INFO | FT 5 [4501/10009] loss=0.9171
2026-02-13 07:33:08,895 | INFO | FT 5 [5001/10009] loss=0.9177
2026-02-13 07:34:43,641 | INFO | FT 5 [5501/10009] loss=0.9179
2026-02-13 07:36:18,394 | INFO | FT 5 [6001/10009] loss=0.9180
2026-02-13 07:37:53,142 | INFO | FT 5 [6501/10009] loss=0.9183
2026-02-13 07:39:27,869 | INFO | FT 5 [7001/10009] loss=0.9180
2026-02-13 07:41:02,599 | INFO | FT 5 [7501/10009] loss=0.9183
2026-02-13 07:42:37,298 | INFO | FT 5 [8001/10009] loss=0.9181
2026-02-13 07:44:11,955 | INFO | FT 5 [8501/10009] loss=0.9182
2026-02-13 07:45:46,703 | INFO | FT 5 [9001/10009] loss=0.9180
2026-02-13 07:47:21,416 | INFO | FT 5 [9501/10009] loss=0.9185
2026-02-13 07:48:56,121 | INFO | FT 5 [10001/10009] loss=0.9182
2026-02-13 07:48:57,655 | INFO | FT 5 [10009/10009] loss=0.9182
2026-02-13 07:50:20,510 | INFO | FT ep 5/10: train_loss=0.9182, val_acc=0.7689
2026-02-13 07:50:20,693 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 07:50:21,974 | INFO | FT 6 [1/10009] loss=0.9345
2026-02-13 07:51:56,366 | INFO | FT 6 [501/10009] loss=0.8940
2026-02-13 07:53:30,939 | INFO | FT 6 [1001/10009] loss=0.8937
2026-02-13 07:55:05,543 | INFO | FT 6 [1501/10009] loss=0.8872
2026-02-13 07:56:40,239 | INFO | FT 6 [2001/10009] loss=0.8884
2026-02-13 07:58:14,912 | INFO | FT 6 [2501/10009] loss=0.8893
2026-02-13 07:59:49,509 | INFO | FT 6 [3001/10009] loss=0.8878
2026-02-13 08:01:24,125 | INFO | FT 6 [3501/10009] loss=0.8896
2026-02-13 08:02:58,752 | INFO | FT 6 [4001/10009] loss=0.8903
2026-02-13 08:04:33,303 | INFO | FT 6 [4501/10009] loss=0.8904
2026-02-13 08:06:07,892 | INFO | FT 6 [5001/10009] loss=0.8904
2026-02-13 08:07:42,515 | INFO | FT 6 [5501/10009] loss=0.8896
2026-02-13 08:09:17,093 | INFO | FT 6 [6001/10009] loss=0.8894
2026-02-13 08:10:51,690 | INFO | FT 6 [6501/10009] loss=0.8891
2026-02-13 08:12:26,294 | INFO | FT 6 [7001/10009] loss=0.8894
2026-02-13 08:14:00,849 | INFO | FT 6 [7501/10009] loss=0.8892
2026-02-13 08:15:35,403 | INFO | FT 6 [8001/10009] loss=0.8893
2026-02-13 08:17:09,962 | INFO | FT 6 [8501/10009] loss=0.8891
2026-02-13 08:18:44,505 | INFO | FT 6 [9001/10009] loss=0.8882
2026-02-13 08:20:19,049 | INFO | FT 6 [9501/10009] loss=0.8877
2026-02-13 08:21:53,628 | INFO | FT 6 [10001/10009] loss=0.8877
2026-02-13 08:21:55,160 | INFO | FT 6 [10009/10009] loss=0.8877
2026-02-13 08:23:16,665 | INFO | FT ep 6/10: train_loss=0.8877, val_acc=0.7744
2026-02-13 08:23:16,851 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 08:23:17,934 | INFO | FT 7 [1/10009] loss=0.9457
2026-02-13 08:24:52,297 | INFO | FT 7 [501/10009] loss=0.8527
2026-02-13 08:26:26,925 | INFO | FT 7 [1001/10009] loss=0.8548
2026-02-13 08:28:01,527 | INFO | FT 7 [1501/10009] loss=0.8553
2026-02-13 08:29:36,098 | INFO | FT 7 [2001/10009] loss=0.8551
2026-02-13 08:31:10,669 | INFO | FT 7 [2501/10009] loss=0.8569
2026-02-13 08:32:45,253 | INFO | FT 7 [3001/10009] loss=0.8568
2026-02-13 08:34:19,884 | INFO | FT 7 [3501/10009] loss=0.8569
2026-02-13 08:35:54,463 | INFO | FT 7 [4001/10009] loss=0.8574
2026-02-13 08:37:29,019 | INFO | FT 7 [4501/10009] loss=0.8572
2026-02-13 08:39:03,596 | INFO | FT 7 [5001/10009] loss=0.8567
2026-02-13 08:40:38,160 | INFO | FT 7 [5501/10009] loss=0.8566
2026-02-13 08:42:12,726 | INFO | FT 7 [6001/10009] loss=0.8565
2026-02-13 08:43:47,262 | INFO | FT 7 [6501/10009] loss=0.8558
2026-02-13 08:45:21,851 | INFO | FT 7 [7001/10009] loss=0.8556
2026-02-13 08:46:56,409 | INFO | FT 7 [7501/10009] loss=0.8549
2026-02-13 08:48:30,941 | INFO | FT 7 [8001/10009] loss=0.8548
2026-02-13 08:50:05,500 | INFO | FT 7 [8501/10009] loss=0.8541
2026-02-13 08:51:40,015 | INFO | FT 7 [9001/10009] loss=0.8538
2026-02-13 08:53:14,568 | INFO | FT 7 [9501/10009] loss=0.8533
2026-02-13 08:54:49,098 | INFO | FT 7 [10001/10009] loss=0.8532
2026-02-13 08:54:50,628 | INFO | FT 7 [10009/10009] loss=0.8531
2026-02-13 08:56:11,020 | INFO | FT ep 7/10: train_loss=0.8531, val_acc=0.7791
2026-02-13 08:56:11,201 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 08:56:12,262 | INFO | FT 8 [1/10009] loss=0.7510
2026-02-13 08:57:46,653 | INFO | FT 8 [501/10009] loss=0.8252
2026-02-13 08:59:21,306 | INFO | FT 8 [1001/10009] loss=0.8239
2026-02-13 09:00:55,986 | INFO | FT 8 [1501/10009] loss=0.8250
2026-02-13 09:02:30,674 | INFO | FT 8 [2001/10009] loss=0.8230
2026-02-13 09:04:05,338 | INFO | FT 8 [2501/10009] loss=0.8220
2026-02-13 09:05:40,024 | INFO | FT 8 [3001/10009] loss=0.8223
2026-02-13 09:07:14,656 | INFO | FT 8 [3501/10009] loss=0.8226
2026-02-13 09:08:49,287 | INFO | FT 8 [4001/10009] loss=0.8216
2026-02-13 09:24:46,470 | INFO | Logging to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_imagenet.log
2026-02-13 09:24:46,470 | INFO | ============================================================
2026-02-13 09:24:46,470 | INFO | VBP ImageNet Reproduction Script
2026-02-13 09:24:46,470 | INFO | ============================================================
2026-02-13 09:24:46,470 | INFO |   model_type: cnn
2026-02-13 09:24:46,470 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/ResNet50_TP/resnet50_imagenet1k.pth
2026-02-13 09:24:46,470 | INFO |   cnn_arch: resnet50
2026-02-13 09:24:46,470 | INFO |   pretrained: True
2026-02-13 09:24:46,470 | INFO |   interior_only: True
2026-02-13 09:24:46,470 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 09:24:46,470 | INFO |   train_batch_size: 64
2026-02-13 09:24:46,470 | INFO |   val_batch_size: 128
2026-02-13 09:24:46,470 | INFO |   num_workers: 4
2026-02-13 09:24:46,470 | INFO |   max_batches: 200
2026-02-13 09:24:46,470 | INFO |   keep_ratio: 0.8
2026-02-13 09:24:46,470 | INFO |   global_pruning: True
2026-02-13 09:24:46,470 | INFO |   max_pruning_ratio: 1.0
2026-02-13 09:24:46,470 | INFO |   norm_per_layer: False
2026-02-13 09:24:46,470 | INFO |   no_compensation: False
2026-02-13 09:24:46,471 | INFO |   no_recalib: False
2026-02-13 09:24:46,471 | INFO |   criterion: variance
2026-02-13 09:24:46,471 | INFO |   epochs_ft: 10
2026-02-13 09:24:46,471 | INFO |   lr_ft: 0.01
2026-02-13 09:24:46,471 | INFO |   opt_ft: sgd
2026-02-13 09:24:46,471 | INFO |   momentum_ft: 0.9
2026-02-13 09:24:46,471 | INFO |   wd_ft: 0.0001
2026-02-13 09:24:46,471 | INFO |   use_kd: True
2026-02-13 09:24:46,471 | INFO |   kd_alpha: 0.7
2026-02-13 09:24:46,471 | INFO |   kd_T: 2.0
2026-02-13 09:24:46,471 | INFO |   pat: False
2026-02-13 09:24:46,471 | INFO |   pat_steps: 5
2026-02-13 09:24:46,471 | INFO |   pat_epochs_per_step: 3
2026-02-13 09:24:46,471 | INFO |   var_loss_weight: 0.0
2026-02-13 09:24:46,471 | INFO |   sparse_mode: none
2026-02-13 09:24:46,471 | INFO |   epochs_sparse: 5
2026-02-13 09:24:46,471 | INFO |   lr_sparse: 0.0001
2026-02-13 09:24:46,471 | INFO |   l1_lambda: 0.0001
2026-02-13 09:24:46,471 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 09:24:46,471 | INFO |   disable_ddp: False
2026-02-13 09:24:46,471 | INFO |   local_rank: 0
2026-02-13 09:24:46,471 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8
2026-02-13 09:24:46,471 | INFO |   rank: 0
2026-02-13 09:24:46,472 | INFO |   world_size: 2
2026-02-13 09:24:46,472 | INFO | Loading ImageNet dataset...
2026-02-13 09:24:46,472 | INFO | Using cached sample lists for fast loading
2026-02-13 09:24:46,840 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 09:24:48,842 | INFO | Loaded resnet50 (pretrained=True)
2026-02-13 09:24:49,922 | INFO | Baseline: 4.12G MACs, 25.56M params
2026-02-13 09:24:49,923 | INFO | Evaluating original model...
2026-02-13 09:26:08,250 | INFO | Original accuracy: 0.8035, loss: 1.4124
2026-02-13 09:26:08,274 | INFO | Created teacher model for knowledge distillation
2026-02-13 09:26:08,274 | INFO | PAT: 1 steps, per_step_keep=0.8000, epochs_per_step=0, target_keep=0.8, criterion=variance
2026-02-13 09:26:08,274 | INFO | 
============================================================
2026-02-13 09:26:08,274 | INFO | PAT Step 1/1
2026-02-13 09:26:08,274 | INFO | ============================================================
2026-02-13 09:26:08,297 | INFO | Auto-detected 53 CNN target layers for stats
2026-02-13 09:26:08,297 | INFO | Collecting activation variance statistics...
2026-02-13 09:26:28,032 | INFO | Statistics collected for 53 layers
2026-02-13 09:26:28,032 | INFO |   Conv2d: mean_var=5.526824
2026-02-13 09:26:28,033 | INFO |   Conv2d: mean_var=1.093062
2026-02-13 09:26:28,033 | INFO |   Conv2d: mean_var=2.074985
2026-02-13 09:26:28,033 | INFO |   Conv2d: mean_var=13.791623
2026-02-13 09:26:28,033 | INFO |   Conv2d: mean_var=16.733500
2026-02-13 09:26:28,177 | INFO | Variance — entropy=0.9313, cv=1.5012, gini=0.6156
2026-02-13 09:26:28,224 | INFO | Pruning: per_step_prune=0.2000
2026-02-13 09:26:28,755 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 09:26:28,756 | INFO | Recalibrating BN running stats...
2026-02-13 09:26:29,530 | INFO | BN recalib [1/100]
2026-02-13 09:26:29,812 | INFO | BN recalib [6/100]
2026-02-13 09:26:30,593 | INFO | BN recalib [11/100]
2026-02-13 09:26:31,132 | INFO | BN recalib [16/100]
2026-02-13 09:26:31,612 | INFO | BN recalib [21/100]
2026-02-13 09:26:31,960 | INFO | BN recalib [26/100]
2026-02-13 09:26:32,443 | INFO | BN recalib [31/100]
2026-02-13 09:26:32,865 | INFO | BN recalib [36/100]
2026-02-13 09:26:33,482 | INFO | BN recalib [41/100]
2026-02-13 09:26:33,853 | INFO | BN recalib [46/100]
2026-02-13 09:26:34,233 | INFO | BN recalib [51/100]
2026-02-13 09:26:34,750 | INFO | BN recalib [56/100]
2026-02-13 09:26:35,267 | INFO | BN recalib [61/100]
2026-02-13 09:26:35,604 | INFO | BN recalib [66/100]
2026-02-13 09:26:36,166 | INFO | BN recalib [71/100]
2026-02-13 09:26:36,528 | INFO | BN recalib [76/100]
2026-02-13 09:26:36,939 | INFO | BN recalib [81/100]
2026-02-13 09:26:37,265 | INFO | BN recalib [86/100]
2026-02-13 09:26:37,980 | INFO | BN recalib [91/100]
2026-02-13 09:26:38,332 | INFO | BN recalib [96/100]
2026-02-13 09:26:38,769 | INFO | BN recalib [100/100]
2026-02-13 09:26:39,164 | INFO | BN recalibration done (100 batches)
2026-02-13 09:27:57,121 | INFO | Step 1 retention: acc=0.7455, loss=1.7319
2026-02-13 09:27:57,122 | INFO |   cumulative_keep=0.8000, MACs=3.48G, params=18.99M
2026-02-13 09:27:57,122 | INFO | 
Post-prune fine-tuning for 10 epochs (sgd, lr=0.01, wd=0.0001)...
2026-02-13 09:27:58,195 | INFO | FT 1 [1/10009] loss=1.3983
2026-02-13 09:29:32,538 | INFO | FT 1 [501/10009] loss=1.0374
2026-02-13 09:31:07,550 | INFO | FT 1 [1001/10009] loss=1.0235
2026-02-13 09:32:42,252 | INFO | FT 1 [1501/10009] loss=1.0112
2026-02-13 09:34:16,944 | INFO | FT 1 [2001/10009] loss=1.0044
2026-02-13 09:35:51,741 | INFO | FT 1 [2501/10009] loss=1.0003
2026-02-13 09:37:26,345 | INFO | FT 1 [3001/10009] loss=0.9968
2026-02-13 09:39:00,963 | INFO | FT 1 [3501/10009] loss=0.9940
2026-02-13 09:40:35,638 | INFO | FT 1 [4001/10009] loss=0.9912
2026-02-13 09:42:10,478 | INFO | FT 1 [4501/10009] loss=0.9898
2026-02-13 09:43:45,143 | INFO | FT 1 [5001/10009] loss=0.9890
2026-02-13 09:45:19,822 | INFO | FT 1 [5501/10009] loss=0.9872
2026-02-13 09:46:54,481 | INFO | FT 1 [6001/10009] loss=0.9867
2026-02-13 09:48:29,149 | INFO | FT 1 [6501/10009] loss=0.9851
2026-02-13 09:50:03,924 | INFO | FT 1 [7001/10009] loss=0.9846
2026-02-13 09:51:38,624 | INFO | FT 1 [7501/10009] loss=0.9829
2026-02-13 09:53:13,314 | INFO | FT 1 [8001/10009] loss=0.9819
2026-02-13 09:54:48,083 | INFO | FT 1 [8501/10009] loss=0.9818
2026-02-13 09:56:22,803 | INFO | FT 1 [9001/10009] loss=0.9809
2026-02-13 09:57:57,481 | INFO | FT 1 [9501/10009] loss=0.9804
2026-02-13 09:59:32,214 | INFO | FT 1 [10001/10009] loss=0.9800
2026-02-13 09:59:33,748 | INFO | FT 1 [10009/10009] loss=0.9799
2026-02-13 10:00:58,121 | INFO | FT ep 1/10: train_loss=0.9799, val_acc=0.7638
2026-02-13 10:00:58,283 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 10:00:59,315 | INFO | FT 2 [1/10009] loss=0.7389
2026-02-13 10:02:33,811 | INFO | FT 2 [501/10009] loss=0.9458
2026-02-13 10:04:08,531 | INFO | FT 2 [1001/10009] loss=0.9467
2026-02-13 10:05:43,312 | INFO | FT 2 [1501/10009] loss=0.9471
2026-02-13 10:07:18,042 | INFO | FT 2 [2001/10009] loss=0.9483
2026-02-13 10:08:52,795 | INFO | FT 2 [2501/10009] loss=0.9481
2026-02-13 10:10:27,509 | INFO | FT 2 [3001/10009] loss=0.9478
2026-02-13 10:12:02,209 | INFO | FT 2 [3501/10009] loss=0.9498
2026-02-13 10:13:37,030 | INFO | FT 2 [4001/10009] loss=0.9512
2026-02-13 10:15:11,752 | INFO | FT 2 [4501/10009] loss=0.9517
2026-02-13 10:16:46,482 | INFO | FT 2 [5001/10009] loss=0.9523
2026-02-13 10:18:21,156 | INFO | FT 2 [5501/10009] loss=0.9527
2026-02-13 10:19:55,780 | INFO | FT 2 [6001/10009] loss=0.9530
2026-02-13 10:21:30,840 | INFO | FT 2 [6501/10009] loss=0.9540
2026-02-13 10:23:05,578 | INFO | FT 2 [7001/10009] loss=0.9551
2026-02-13 10:24:40,293 | INFO | FT 2 [7501/10009] loss=0.9560
2026-02-13 10:26:15,032 | INFO | FT 2 [8001/10009] loss=0.9567
2026-02-13 10:27:49,816 | INFO | FT 2 [8501/10009] loss=0.9571
2026-02-13 10:29:24,526 | INFO | FT 2 [9001/10009] loss=0.9582
2026-02-13 10:30:59,332 | INFO | FT 2 [9501/10009] loss=0.9586
2026-02-13 10:32:34,061 | INFO | FT 2 [10001/10009] loss=0.9586
2026-02-13 10:32:35,592 | INFO | FT 2 [10009/10009] loss=0.9585
2026-02-13 10:33:58,568 | INFO | FT ep 2/10: train_loss=0.9585, val_acc=0.7612
2026-02-13 10:33:59,656 | INFO | FT 3 [1/10009] loss=1.1377
2026-02-13 10:35:34,279 | INFO | FT 3 [501/10009] loss=0.9407
2026-02-13 10:37:09,511 | INFO | FT 3 [1001/10009] loss=0.9351
2026-02-13 10:38:44,184 | INFO | FT 3 [1501/10009] loss=0.9392
2026-02-13 10:40:19,264 | INFO | FT 3 [2001/10009] loss=0.9395
2026-02-13 10:41:54,005 | INFO | FT 3 [2501/10009] loss=0.9402
2026-02-13 10:43:28,684 | INFO | FT 3 [3001/10009] loss=0.9410
2026-02-13 10:45:03,478 | INFO | FT 3 [3501/10009] loss=0.9424
2026-02-13 10:46:38,197 | INFO | FT 3 [4001/10009] loss=0.9443
2026-02-13 10:48:12,875 | INFO | FT 3 [4501/10009] loss=0.9437
2026-02-13 10:49:47,691 | INFO | FT 3 [5001/10009] loss=0.9457
2026-02-13 10:51:22,478 | INFO | FT 3 [5501/10009] loss=0.9455
2026-02-13 10:52:57,151 | INFO | FT 3 [6001/10009] loss=0.9453
2026-02-13 10:54:31,828 | INFO | FT 3 [6501/10009] loss=0.9474
2026-02-13 10:56:06,448 | INFO | FT 3 [7001/10009] loss=0.9487
2026-02-13 10:57:41,166 | INFO | FT 3 [7501/10009] loss=0.9499
2026-02-13 10:59:15,914 | INFO | FT 3 [8001/10009] loss=0.9502
2026-02-13 11:00:50,628 | INFO | FT 3 [8501/10009] loss=0.9511
2026-02-13 11:02:25,329 | INFO | FT 3 [9001/10009] loss=0.9512
2026-02-13 11:04:00,086 | INFO | FT 3 [9501/10009] loss=0.9519
2026-02-13 11:05:34,797 | INFO | FT 3 [10001/10009] loss=0.9523
2026-02-13 11:05:36,329 | INFO | FT 3 [10009/10009] loss=0.9523
2026-02-13 11:06:59,167 | INFO | FT ep 3/10: train_loss=0.9523, val_acc=0.7605
2026-02-13 11:07:00,135 | INFO | FT 4 [1/10009] loss=0.7817
2026-02-13 11:08:34,859 | INFO | FT 4 [501/10009] loss=0.9191
2026-02-13 11:10:09,490 | INFO | FT 4 [1001/10009] loss=0.9265
2026-02-13 11:11:44,202 | INFO | FT 4 [1501/10009] loss=0.9299
2026-02-13 11:13:19,064 | INFO | FT 4 [2001/10009] loss=0.9297
2026-02-13 11:14:53,829 | INFO | FT 4 [2501/10009] loss=0.9301
2026-02-13 11:16:28,580 | INFO | FT 4 [3001/10009] loss=0.9329
2026-02-13 11:18:03,362 | INFO | FT 4 [3501/10009] loss=0.9350
2026-02-13 11:19:38,095 | INFO | FT 4 [4001/10009] loss=0.9355
2026-02-13 11:21:12,841 | INFO | FT 4 [4501/10009] loss=0.9363
2026-02-13 11:22:47,586 | INFO | FT 4 [5001/10009] loss=0.9367
2026-02-13 11:24:22,725 | INFO | FT 4 [5501/10009] loss=0.9375
2026-02-13 11:25:57,800 | INFO | FT 4 [6001/10009] loss=0.9383
2026-02-13 11:27:35,091 | INFO | FT 4 [6501/10009] loss=0.9394
2026-02-13 11:29:09,738 | INFO | FT 4 [7001/10009] loss=0.9394
2026-02-13 11:30:44,471 | INFO | FT 4 [7501/10009] loss=0.9394
2026-02-13 11:32:19,163 | INFO | FT 4 [8001/10009] loss=0.9397
2026-02-13 11:33:53,830 | INFO | FT 4 [8501/10009] loss=0.9395
2026-02-13 11:35:28,539 | INFO | FT 4 [9001/10009] loss=0.9396
2026-02-13 11:37:03,404 | INFO | FT 4 [9501/10009] loss=0.9403
2026-02-13 11:38:38,167 | INFO | FT 4 [10001/10009] loss=0.9404
2026-02-13 11:38:39,698 | INFO | FT 4 [10009/10009] loss=0.9404
2026-02-13 11:40:03,405 | INFO | FT ep 4/10: train_loss=0.9404, val_acc=0.7660
2026-02-13 11:40:03,563 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 11:40:04,604 | INFO | FT 5 [1/10009] loss=0.9962
2026-02-13 11:41:39,088 | INFO | FT 5 [501/10009] loss=0.9062
2026-02-13 11:43:13,692 | INFO | FT 5 [1001/10009] loss=0.9119
2026-02-13 11:44:48,442 | INFO | FT 5 [1501/10009] loss=0.9135
2026-02-13 11:46:23,219 | INFO | FT 5 [2001/10009] loss=0.9151
2026-02-13 11:47:57,976 | INFO | FT 5 [2501/10009] loss=0.9160
2026-02-13 11:49:32,738 | INFO | FT 5 [3001/10009] loss=0.9159
2026-02-13 11:51:07,517 | INFO | FT 5 [3501/10009] loss=0.9149
2026-02-13 11:52:42,302 | INFO | FT 5 [4001/10009] loss=0.9154
2026-02-13 11:54:17,066 | INFO | FT 5 [4501/10009] loss=0.9158
2026-02-13 11:55:51,898 | INFO | FT 5 [5001/10009] loss=0.9162
2026-02-13 11:57:26,631 | INFO | FT 5 [5501/10009] loss=0.9172
2026-02-13 11:59:01,383 | INFO | FT 5 [6001/10009] loss=0.9187
2026-02-13 12:00:36,119 | INFO | FT 5 [6501/10009] loss=0.9186
2026-02-13 12:02:10,802 | INFO | FT 5 [7001/10009] loss=0.9184
2026-02-13 12:03:45,542 | INFO | FT 5 [7501/10009] loss=0.9184
2026-02-13 12:05:20,315 | INFO | FT 5 [8001/10009] loss=0.9182
2026-02-13 12:06:55,071 | INFO | FT 5 [8501/10009] loss=0.9183
2026-02-13 12:08:29,885 | INFO | FT 5 [9001/10009] loss=0.9184
2026-02-13 12:10:04,724 | INFO | FT 5 [9501/10009] loss=0.9187
2026-02-13 12:11:39,497 | INFO | FT 5 [10001/10009] loss=0.9185
2026-02-13 12:11:41,029 | INFO | FT 5 [10009/10009] loss=0.9185
2026-02-13 12:13:02,509 | INFO | FT ep 5/10: train_loss=0.9185, val_acc=0.7708
2026-02-13 12:13:02,676 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.8/vbp_best.pth
2026-02-13 12:13:03,731 | INFO | FT 6 [1/10009] loss=1.1994
2026-02-13 12:14:38,481 | INFO | FT 6 [501/10009] loss=0.8897
2026-02-13 12:16:13,156 | INFO | FT 6 [1001/10009] loss=0.8934
2026-02-13 12:17:47,890 | INFO | FT 6 [1501/10009] loss=0.8889
2026-02-13 12:19:22,741 | INFO | FT 6 [2001/10009] loss=0.8931
2026-02-13 12:20:57,483 | INFO | FT 6 [2501/10009] loss=0.8925
2026-02-13 12:22:32,276 | INFO | FT 6 [3001/10009] loss=0.8905
2026-02-13 12:24:07,090 | INFO | FT 6 [3501/10009] loss=0.8911
2026-02-13 12:25:41,859 | INFO | FT 6 [4001/10009] loss=0.8916
2026-02-13 12:27:16,645 | INFO | FT 6 [4501/10009] loss=0.8916
2026-02-13 12:28:51,398 | INFO | FT 6 [5001/10009] loss=0.8908
2026-02-13 12:30:26,130 | INFO | FT 6 [5501/10009] loss=0.8900
2026-02-13 12:32:00,878 | INFO | FT 6 [6001/10009] loss=0.8902
2026-02-13 12:33:35,801 | INFO | FT 6 [6501/10009] loss=0.8900
2026-02-13 12:35:10,562 | INFO | FT 6 [7001/10009] loss=0.8908
2026-02-13 12:36:45,321 | INFO | FT 6 [7501/10009] loss=0.8905
2026-02-13 12:38:20,097 | INFO | FT 6 [8001/10009] loss=0.8903
2026-02-13 12:39:54,823 | INFO | FT 6 [8501/10009] loss=0.8899
