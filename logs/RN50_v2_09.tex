2026-02-13 04:05:35,097 | INFO | Logging to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_imagenet.log
2026-02-13 04:05:35,097 | INFO | ============================================================
2026-02-13 04:05:35,097 | INFO | VBP ImageNet Reproduction Script
2026-02-13 04:05:35,097 | INFO | ============================================================
2026-02-13 04:05:35,097 | INFO |   model_type: cnn
2026-02-13 04:05:35,097 | INFO |   model_name: /algo/NetOptimization/outputs/VBP/ResNet50_TP/resnet50_imagenet1k.pth
2026-02-13 04:05:35,097 | INFO |   cnn_arch: resnet50
2026-02-13 04:05:35,097 | INFO |   pretrained: True
2026-02-13 04:05:35,097 | INFO |   interior_only: True
2026-02-13 04:05:35,097 | INFO |   data_path: /algo/NetOptimization/outputs/VBP/
2026-02-13 04:05:35,097 | INFO |   train_batch_size: 64
2026-02-13 04:05:35,097 | INFO |   val_batch_size: 128
2026-02-13 04:05:35,097 | INFO |   num_workers: 4
2026-02-13 04:05:35,097 | INFO |   max_batches: 200
2026-02-13 04:05:35,097 | INFO |   keep_ratio: 0.9
2026-02-13 04:05:35,098 | INFO |   global_pruning: True
2026-02-13 04:05:35,098 | INFO |   max_pruning_ratio: 1.0
2026-02-13 04:05:35,098 | INFO |   norm_per_layer: False
2026-02-13 04:05:35,098 | INFO |   no_compensation: False
2026-02-13 04:05:35,098 | INFO |   no_recalib: False
2026-02-13 04:05:35,098 | INFO |   criterion: variance
2026-02-13 04:05:35,098 | INFO |   epochs_ft: 10
2026-02-13 04:05:35,098 | INFO |   lr_ft: 0.01
2026-02-13 04:05:35,098 | INFO |   opt_ft: sgd
2026-02-13 04:05:35,098 | INFO |   momentum_ft: 0.9
2026-02-13 04:05:35,098 | INFO |   wd_ft: 0.0001
2026-02-13 04:05:35,098 | INFO |   use_kd: True
2026-02-13 04:05:35,098 | INFO |   kd_alpha: 0.7
2026-02-13 04:05:35,098 | INFO |   kd_T: 2.0
2026-02-13 04:05:35,098 | INFO |   pat: False
2026-02-13 04:05:35,098 | INFO |   pat_steps: 5
2026-02-13 04:05:35,098 | INFO |   pat_epochs_per_step: 3
2026-02-13 04:05:35,098 | INFO |   var_loss_weight: 0.0
2026-02-13 04:05:35,098 | INFO |   sparse_mode: none
2026-02-13 04:05:35,098 | INFO |   epochs_sparse: 5
2026-02-13 04:05:35,098 | INFO |   lr_sparse: 0.0001
2026-02-13 04:05:35,098 | INFO |   l1_lambda: 0.0001
2026-02-13 04:05:35,098 | INFO |   gmp_target_sparsity: 0.5
2026-02-13 04:05:35,098 | INFO |   disable_ddp: False
2026-02-13 04:05:35,098 | INFO |   local_rank: 0
2026-02-13 04:05:35,098 | INFO |   save_dir: /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9
2026-02-13 04:05:35,099 | INFO |   rank: 0
2026-02-13 04:05:35,099 | INFO |   world_size: 2
2026-02-13 04:05:35,099 | INFO | Loading ImageNet dataset...
2026-02-13 04:05:35,099 | INFO | Using cached sample lists for fast loading
2026-02-13 04:05:35,496 | INFO | Train samples: 1281167, Val samples: 50000
2026-02-13 04:05:39,160 | INFO | Loaded resnet50 (pretrained=True)
2026-02-13 04:05:41,255 | INFO | Baseline: 4.12G MACs, 25.56M params
2026-02-13 04:05:41,255 | INFO | Evaluating original model...
2026-02-13 04:07:07,820 | INFO | Original accuracy: 0.8035, loss: 1.4124
2026-02-13 04:07:07,843 | INFO | Created teacher model for knowledge distillation
2026-02-13 04:07:07,844 | INFO | PAT: 1 steps, per_step_keep=0.9000, epochs_per_step=0, target_keep=0.9, criterion=variance
2026-02-13 04:07:07,844 | INFO | 
============================================================
2026-02-13 04:07:07,844 | INFO | PAT Step 1/1
2026-02-13 04:07:07,844 | INFO | ============================================================
2026-02-13 04:07:07,866 | INFO | Auto-detected 53 CNN target layers for stats
2026-02-13 04:07:07,866 | INFO | Collecting activation variance statistics...
2026-02-13 04:07:31,286 | INFO | Statistics collected for 53 layers
2026-02-13 04:07:31,286 | INFO |   Conv2d: mean_var=5.532274
2026-02-13 04:07:31,286 | INFO |   Conv2d: mean_var=1.094799
2026-02-13 04:07:31,286 | INFO |   Conv2d: mean_var=2.078005
2026-02-13 04:07:31,286 | INFO |   Conv2d: mean_var=13.810093
2026-02-13 04:07:31,286 | INFO |   Conv2d: mean_var=16.755754
2026-02-13 04:07:32,245 | INFO | Variance â€” entropy=0.9313, cv=1.5020, gini=0.6155
2026-02-13 04:07:32,289 | INFO | Pruning: per_step_prune=0.1000
2026-02-13 04:07:32,822 | INFO | Pruning complete (criterion=VBP, compensation=on)
2026-02-13 04:07:32,822 | INFO | Recalibrating BN running stats...
2026-02-13 04:07:33,749 | INFO | BN recalib [1/100]
2026-02-13 04:07:34,022 | INFO | BN recalib [6/100]
2026-02-13 04:07:34,429 | INFO | BN recalib [11/100]
2026-02-13 04:07:34,801 | INFO | BN recalib [16/100]
2026-02-13 04:07:35,851 | INFO | BN recalib [21/100]
2026-02-13 04:07:36,301 | INFO | BN recalib [26/100]
2026-02-13 04:07:36,787 | INFO | BN recalib [31/100]
2026-02-13 04:07:37,208 | INFO | BN recalib [36/100]
2026-02-13 04:07:37,963 | INFO | BN recalib [41/100]
2026-02-13 04:07:38,322 | INFO | BN recalib [46/100]
2026-02-13 04:07:38,733 | INFO | BN recalib [51/100]
2026-02-13 04:07:39,149 | INFO | BN recalib [56/100]
2026-02-13 04:07:39,894 | INFO | BN recalib [61/100]
2026-02-13 04:07:40,294 | INFO | BN recalib [66/100]
2026-02-13 04:07:40,658 | INFO | BN recalib [71/100]
2026-02-13 04:07:41,058 | INFO | BN recalib [76/100]
2026-02-13 04:07:41,851 | INFO | BN recalib [81/100]
2026-02-13 04:07:42,245 | INFO | BN recalib [86/100]
2026-02-13 04:07:42,611 | INFO | BN recalib [91/100]
2026-02-13 04:07:43,019 | INFO | BN recalib [96/100]
2026-02-13 04:07:43,412 | INFO | BN recalib [100/100]
2026-02-13 04:07:44,245 | INFO | BN recalibration done (100 batches)
2026-02-13 04:09:08,972 | INFO | Step 1 retention: acc=0.7613, loss=1.4390
2026-02-13 04:09:08,982 | INFO |   cumulative_keep=0.9000, MACs=3.79G, params=21.72M
2026-02-13 04:09:08,982 | INFO | 
Post-prune fine-tuning for 10 epochs (sgd, lr=0.01, wd=0.0001)...
2026-02-13 04:09:10,172 | INFO | FT 1 [1/10009] loss=1.1481
2026-02-13 04:10:46,576 | INFO | FT 1 [501/10009] loss=0.9554
2026-02-13 04:12:23,694 | INFO | FT 1 [1001/10009] loss=0.9322
2026-02-13 04:14:00,114 | INFO | FT 1 [1501/10009] loss=0.9262
2026-02-13 04:15:36,811 | INFO | FT 1 [2001/10009] loss=0.9204
2026-02-13 04:17:13,383 | INFO | FT 1 [2501/10009] loss=0.9190
2026-02-13 04:18:50,041 | INFO | FT 1 [3001/10009] loss=0.9178
2026-02-13 04:20:26,709 | INFO | FT 1 [3501/10009] loss=0.9177
2026-02-13 04:22:03,551 | INFO | FT 1 [4001/10009] loss=0.9166
2026-02-13 04:23:40,361 | INFO | FT 1 [4501/10009] loss=0.9172
2026-02-13 04:25:17,105 | INFO | FT 1 [5001/10009] loss=0.9165
2026-02-13 04:26:53,636 | INFO | FT 1 [5501/10009] loss=0.9153
2026-02-13 04:28:30,131 | INFO | FT 1 [6001/10009] loss=0.9157
2026-02-13 04:30:06,865 | INFO | FT 1 [6501/10009] loss=0.9147
2026-02-13 04:31:43,465 | INFO | FT 1 [7001/10009] loss=0.9149
2026-02-13 04:33:20,179 | INFO | FT 1 [7501/10009] loss=0.9148
2026-02-13 04:34:56,673 | INFO | FT 1 [8001/10009] loss=0.9152
2026-02-13 04:36:33,270 | INFO | FT 1 [8501/10009] loss=0.9150
2026-02-13 04:38:09,890 | INFO | FT 1 [9001/10009] loss=0.9153
2026-02-13 04:39:46,461 | INFO | FT 1 [9501/10009] loss=0.9152
2026-02-13 04:41:23,066 | INFO | FT 1 [10001/10009] loss=0.9156
2026-02-13 04:41:24,638 | INFO | FT 1 [10009/10009] loss=0.9156
2026-02-13 04:42:47,341 | INFO | FT ep 1/10: train_loss=0.9156, val_acc=0.7704
2026-02-13 04:42:47,583 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 04:42:49,152 | INFO | FT 2 [1/10009] loss=0.7400
2026-02-13 04:44:25,350 | INFO | FT 2 [501/10009] loss=0.8805
2026-02-13 04:46:02,169 | INFO | FT 2 [1001/10009] loss=0.8852
2026-02-13 04:47:38,641 | INFO | FT 2 [1501/10009] loss=0.8915
2026-02-13 04:49:15,107 | INFO | FT 2 [2001/10009] loss=0.8912
2026-02-13 04:50:51,673 | INFO | FT 2 [2501/10009] loss=0.8912
2026-02-13 04:52:28,184 | INFO | FT 2 [3001/10009] loss=0.8911
2026-02-13 04:54:04,911 | INFO | FT 2 [3501/10009] loss=0.8927
2026-02-13 04:55:41,447 | INFO | FT 2 [4001/10009] loss=0.8946
2026-02-13 04:57:18,555 | INFO | FT 2 [4501/10009] loss=0.8963
2026-02-13 04:58:55,199 | INFO | FT 2 [5001/10009] loss=0.8970
2026-02-13 05:00:32,046 | INFO | FT 2 [5501/10009] loss=0.8977
2026-02-13 05:02:09,101 | INFO | FT 2 [6001/10009] loss=0.8987
2026-02-13 05:03:46,094 | INFO | FT 2 [6501/10009] loss=0.9006
2026-02-13 05:05:23,084 | INFO | FT 2 [7001/10009] loss=0.9019
2026-02-13 05:07:00,132 | INFO | FT 2 [7501/10009] loss=0.9032
2026-02-13 05:08:37,034 | INFO | FT 2 [8001/10009] loss=0.9046
2026-02-13 05:10:13,759 | INFO | FT 2 [8501/10009] loss=0.9056
2026-02-13 05:11:50,418 | INFO | FT 2 [9001/10009] loss=0.9071
2026-02-13 05:13:27,433 | INFO | FT 2 [9501/10009] loss=0.9080
2026-02-13 05:15:04,072 | INFO | FT 2 [10001/10009] loss=0.9086
2026-02-13 05:15:05,630 | INFO | FT 2 [10009/10009] loss=0.9086
2026-02-13 05:16:33,444 | INFO | FT ep 2/10: train_loss=0.9086, val_acc=0.7660
2026-02-13 05:16:35,223 | INFO | FT 3 [1/10009] loss=1.2142
2026-02-13 05:18:11,461 | INFO | FT 3 [501/10009] loss=0.8861
2026-02-13 05:19:48,250 | INFO | FT 3 [1001/10009] loss=0.8887
2026-02-13 05:21:25,237 | INFO | FT 3 [1501/10009] loss=0.8924
2026-02-13 05:23:01,758 | INFO | FT 3 [2001/10009] loss=0.8953
2026-02-13 05:24:38,654 | INFO | FT 3 [2501/10009] loss=0.8962
2026-02-13 05:26:15,745 | INFO | FT 3 [3001/10009] loss=0.8963
2026-02-13 05:27:53,184 | INFO | FT 3 [3501/10009] loss=0.8974
2026-02-13 05:29:29,721 | INFO | FT 3 [4001/10009] loss=0.8983
2026-02-13 05:31:06,498 | INFO | FT 3 [4501/10009] loss=0.8996
2026-02-13 05:32:43,479 | INFO | FT 3 [5001/10009] loss=0.9021
2026-02-13 05:34:20,119 | INFO | FT 3 [5501/10009] loss=0.9024
2026-02-13 05:35:56,726 | INFO | FT 3 [6001/10009] loss=0.9031
2026-02-13 05:37:33,352 | INFO | FT 3 [6501/10009] loss=0.9042
2026-02-13 05:39:10,290 | INFO | FT 3 [7001/10009] loss=0.9052
2026-02-13 05:40:47,129 | INFO | FT 3 [7501/10009] loss=0.9068
2026-02-13 05:42:23,653 | INFO | FT 3 [8001/10009] loss=0.9072
2026-02-13 05:44:00,438 | INFO | FT 3 [8501/10009] loss=0.9078
2026-02-13 05:45:37,278 | INFO | FT 3 [9001/10009] loss=0.9079
2026-02-13 05:47:14,232 | INFO | FT 3 [9501/10009] loss=0.9084
2026-02-13 05:48:50,814 | INFO | FT 3 [10001/10009] loss=0.9089
2026-02-13 05:48:52,388 | INFO | FT 3 [10009/10009] loss=0.9089
2026-02-13 05:50:17,664 | INFO | FT ep 3/10: train_loss=0.9089, val_acc=0.7650
2026-02-13 05:50:18,774 | INFO | FT 4 [1/10009] loss=0.8080
2026-02-13 05:51:55,400 | INFO | FT 4 [501/10009] loss=0.8804
2026-02-13 05:53:31,929 | INFO | FT 4 [1001/10009] loss=0.8840
2026-02-13 05:55:08,709 | INFO | FT 4 [1501/10009] loss=0.8860
2026-02-13 05:56:45,470 | INFO | FT 4 [2001/10009] loss=0.8864
2026-02-13 05:58:22,367 | INFO | FT 4 [2501/10009] loss=0.8883
2026-02-13 05:59:59,398 | INFO | FT 4 [3001/10009] loss=0.8902
2026-02-13 06:01:36,083 | INFO | FT 4 [3501/10009] loss=0.8911
2026-02-13 06:03:12,862 | INFO | FT 4 [4001/10009] loss=0.8931
2026-02-13 06:04:49,462 | INFO | FT 4 [4501/10009] loss=0.8940
2026-02-13 06:06:26,037 | INFO | FT 4 [5001/10009] loss=0.8959
2026-02-13 06:08:02,589 | INFO | FT 4 [5501/10009] loss=0.8964
2026-02-13 06:09:39,213 | INFO | FT 4 [6001/10009] loss=0.8972
2026-02-13 06:11:15,756 | INFO | FT 4 [6501/10009] loss=0.8987
2026-02-13 06:12:52,300 | INFO | FT 4 [7001/10009] loss=0.8989
2026-02-13 06:14:28,872 | INFO | FT 4 [7501/10009] loss=0.8987
2026-02-13 06:16:05,422 | INFO | FT 4 [8001/10009] loss=0.8978
2026-02-13 06:17:41,952 | INFO | FT 4 [8501/10009] loss=0.8981
2026-02-13 06:19:18,786 | INFO | FT 4 [9001/10009] loss=0.8983
2026-02-13 06:20:55,615 | INFO | FT 4 [9501/10009] loss=0.8991
2026-02-13 06:22:32,504 | INFO | FT 4 [10001/10009] loss=0.8995
2026-02-13 06:22:34,075 | INFO | FT 4 [10009/10009] loss=0.8995
2026-02-13 06:24:02,071 | INFO | FT ep 4/10: train_loss=0.8995, val_acc=0.7679
2026-02-13 06:24:03,260 | INFO | FT 5 [1/10009] loss=0.7933
2026-02-13 06:25:39,423 | INFO | FT 5 [501/10009] loss=0.8626
2026-02-13 06:27:16,081 | INFO | FT 5 [1001/10009] loss=0.8675
2026-02-13 06:28:52,895 | INFO | FT 5 [1501/10009] loss=0.8678
2026-02-13 06:30:29,733 | INFO | FT 5 [2001/10009] loss=0.8700
2026-02-13 06:32:07,191 | INFO | FT 5 [2501/10009] loss=0.8709
2026-02-13 06:33:44,381 | INFO | FT 5 [3001/10009] loss=0.8705
2026-02-13 06:35:21,080 | INFO | FT 5 [3501/10009] loss=0.8718
2026-02-13 06:36:57,576 | INFO | FT 5 [4001/10009] loss=0.8730
2026-02-13 06:38:34,136 | INFO | FT 5 [4501/10009] loss=0.8745
2026-02-13 06:40:10,629 | INFO | FT 5 [5001/10009] loss=0.8751
2026-02-13 06:41:47,179 | INFO | FT 5 [5501/10009] loss=0.8755
2026-02-13 06:43:23,738 | INFO | FT 5 [6001/10009] loss=0.8760
2026-02-13 06:45:00,341 | INFO | FT 5 [6501/10009] loss=0.8760
2026-02-13 06:46:36,944 | INFO | FT 5 [7001/10009] loss=0.8757
2026-02-13 06:48:13,994 | INFO | FT 5 [7501/10009] loss=0.8764
2026-02-13 06:49:50,723 | INFO | FT 5 [8001/10009] loss=0.8762
2026-02-13 06:51:27,254 | INFO | FT 5 [8501/10009] loss=0.8765
2026-02-13 06:53:04,356 | INFO | FT 5 [9001/10009] loss=0.8765
2026-02-13 06:54:41,202 | INFO | FT 5 [9501/10009] loss=0.8767
2026-02-13 06:56:18,118 | INFO | FT 5 [10001/10009] loss=0.8763
2026-02-13 06:56:19,682 | INFO | FT 5 [10009/10009] loss=0.8763
2026-02-13 06:57:45,645 | INFO | FT ep 5/10: train_loss=0.8763, val_acc=0.7738
2026-02-13 06:57:45,948 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 06:57:47,235 | INFO | FT 6 [1/10009] loss=1.0308
2026-02-13 06:59:23,760 | INFO | FT 6 [501/10009] loss=0.8478
2026-02-13 07:01:00,238 | INFO | FT 6 [1001/10009] loss=0.8432
2026-02-13 07:02:37,225 | INFO | FT 6 [1501/10009] loss=0.8422
2026-02-13 07:04:13,982 | INFO | FT 6 [2001/10009] loss=0.8457
2026-02-13 07:05:50,944 | INFO | FT 6 [2501/10009] loss=0.8476
2026-02-13 07:07:27,495 | INFO | FT 6 [3001/10009] loss=0.8468
2026-02-13 07:09:04,197 | INFO | FT 6 [3501/10009] loss=0.8476
2026-02-13 07:10:40,996 | INFO | FT 6 [4001/10009] loss=0.8476
2026-02-13 07:12:17,563 | INFO | FT 6 [4501/10009] loss=0.8489
2026-02-13 07:13:54,189 | INFO | FT 6 [5001/10009] loss=0.8489
2026-02-13 07:15:30,836 | INFO | FT 6 [5501/10009] loss=0.8489
2026-02-13 07:17:07,406 | INFO | FT 6 [6001/10009] loss=0.8490
2026-02-13 07:18:44,034 | INFO | FT 6 [6501/10009] loss=0.8483
2026-02-13 07:20:20,609 | INFO | FT 6 [7001/10009] loss=0.8489
2026-02-13 07:21:57,153 | INFO | FT 6 [7501/10009] loss=0.8488
2026-02-13 07:23:33,843 | INFO | FT 6 [8001/10009] loss=0.8491
2026-02-13 07:25:10,364 | INFO | FT 6 [8501/10009] loss=0.8487
2026-02-13 07:26:46,902 | INFO | FT 6 [9001/10009] loss=0.8478
2026-02-13 07:28:23,719 | INFO | FT 6 [9501/10009] loss=0.8469
2026-02-13 07:30:00,215 | INFO | FT 6 [10001/10009] loss=0.8463
2026-02-13 07:30:01,777 | INFO | FT 6 [10009/10009] loss=0.8463
2026-02-13 07:31:26,727 | INFO | FT ep 6/10: train_loss=0.8463, val_acc=0.7775
2026-02-13 07:31:27,192 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 07:31:28,406 | INFO | FT 7 [1/10009] loss=0.9098
2026-02-13 07:33:04,725 | INFO | FT 7 [501/10009] loss=0.8091
2026-02-13 07:34:41,141 | INFO | FT 7 [1001/10009] loss=0.8118
2026-02-13 07:36:17,944 | INFO | FT 7 [1501/10009] loss=0.8083
2026-02-13 07:37:54,459 | INFO | FT 7 [2001/10009] loss=0.8107
2026-02-13 07:39:31,413 | INFO | FT 7 [2501/10009] loss=0.8114
2026-02-13 07:41:07,959 | INFO | FT 7 [3001/10009] loss=0.8117
2026-02-13 07:42:44,752 | INFO | FT 7 [3501/10009] loss=0.8114
2026-02-13 07:44:21,335 | INFO | FT 7 [4001/10009] loss=0.8129
2026-02-13 07:45:58,407 | INFO | FT 7 [4501/10009] loss=0.8118
2026-02-13 07:47:34,994 | INFO | FT 7 [5001/10009] loss=0.8120
2026-02-13 07:49:11,561 | INFO | FT 7 [5501/10009] loss=0.8113
2026-02-13 07:50:48,121 | INFO | FT 7 [6001/10009] loss=0.8110
2026-02-13 07:52:24,706 | INFO | FT 7 [6501/10009] loss=0.8106
2026-02-13 07:54:01,414 | INFO | FT 7 [7001/10009] loss=0.8107
2026-02-13 07:55:37,942 | INFO | FT 7 [7501/10009] loss=0.8109
2026-02-13 07:57:14,757 | INFO | FT 7 [8001/10009] loss=0.8109
2026-02-13 07:58:51,263 | INFO | FT 7 [8501/10009] loss=0.8100
2026-02-13 08:00:27,810 | INFO | FT 7 [9001/10009] loss=0.8095
2026-02-13 08:02:04,365 | INFO | FT 7 [9501/10009] loss=0.8097
2026-02-13 08:03:40,925 | INFO | FT 7 [10001/10009] loss=0.8093
2026-02-13 08:03:42,492 | INFO | FT 7 [10009/10009] loss=0.8093
2026-02-13 08:05:09,284 | INFO | FT ep 7/10: train_loss=0.8093, val_acc=0.7842
2026-02-13 08:05:09,628 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 08:05:10,704 | INFO | FT 8 [1/10009] loss=0.7649
2026-02-13 08:06:47,203 | INFO | FT 8 [501/10009] loss=0.7758
2026-02-13 08:08:23,774 | INFO | FT 8 [1001/10009] loss=0.7762
2026-02-13 08:10:00,221 | INFO | FT 8 [1501/10009] loss=0.7770
2026-02-13 08:11:36,715 | INFO | FT 8 [2001/10009] loss=0.7776
2026-02-13 08:13:13,683 | INFO | FT 8 [2501/10009] loss=0.7770
2026-02-13 08:14:50,157 | INFO | FT 8 [3001/10009] loss=0.7770
2026-02-13 08:16:26,860 | INFO | FT 8 [3501/10009] loss=0.7779
2026-02-13 08:18:03,436 | INFO | FT 8 [4001/10009] loss=0.7772
2026-02-13 08:19:40,005 | INFO | FT 8 [4501/10009] loss=0.7771
2026-02-13 08:21:16,614 | INFO | FT 8 [5001/10009] loss=0.7766
2026-02-13 08:22:53,190 | INFO | FT 8 [5501/10009] loss=0.7764
2026-02-13 08:24:29,757 | INFO | FT 8 [6001/10009] loss=0.7756
2026-02-13 08:26:06,527 | INFO | FT 8 [6501/10009] loss=0.7748
2026-02-13 08:27:43,043 | INFO | FT 8 [7001/10009] loss=0.7752
2026-02-13 08:29:19,631 | INFO | FT 8 [7501/10009] loss=0.7748
2026-02-13 08:30:56,236 | INFO | FT 8 [8001/10009] loss=0.7748
2026-02-13 08:32:32,779 | INFO | FT 8 [8501/10009] loss=0.7748
2026-02-13 08:34:09,669 | INFO | FT 8 [9001/10009] loss=0.7739
2026-02-13 08:35:46,114 | INFO | FT 8 [9501/10009] loss=0.7736
2026-02-13 08:37:22,642 | INFO | FT 8 [10001/10009] loss=0.7735
2026-02-13 08:37:24,204 | INFO | FT 8 [10009/10009] loss=0.7735
2026-02-13 08:38:47,919 | INFO | FT ep 8/10: train_loss=0.7735, val_acc=0.7895
2026-02-13 08:38:48,117 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 08:38:49,202 | INFO | FT 9 [1/10009] loss=0.9156
2026-02-13 08:40:25,604 | INFO | FT 9 [501/10009] loss=0.7575
2026-02-13 08:42:01,915 | INFO | FT 9 [1001/10009] loss=0.7532
2026-02-13 08:43:38,519 | INFO | FT 9 [1501/10009] loss=0.7512
2026-02-13 08:45:15,176 | INFO | FT 9 [2001/10009] loss=0.7501
2026-02-13 08:46:51,764 | INFO | FT 9 [2501/10009] loss=0.7492
2026-02-13 08:48:29,067 | INFO | FT 9 [3001/10009] loss=0.7492
2026-02-13 08:50:05,776 | INFO | FT 9 [3501/10009] loss=0.7489
2026-02-13 08:51:42,615 | INFO | FT 9 [4001/10009] loss=0.7490
2026-02-13 08:53:19,310 | INFO | FT 9 [4501/10009] loss=0.7496
2026-02-13 08:54:55,796 | INFO | FT 9 [5001/10009] loss=0.7495
2026-02-13 08:56:32,498 | INFO | FT 9 [5501/10009] loss=0.7496
2026-02-13 08:58:09,344 | INFO | FT 9 [6001/10009] loss=0.7491
2026-02-13 08:59:45,823 | INFO | FT 9 [6501/10009] loss=0.7473
2026-02-13 09:01:22,325 | INFO | FT 9 [7001/10009] loss=0.7472
2026-02-13 09:02:58,836 | INFO | FT 9 [7501/10009] loss=0.7464
2026-02-13 09:04:35,331 | INFO | FT 9 [8001/10009] loss=0.7462
2026-02-13 09:06:11,990 | INFO | FT 9 [8501/10009] loss=0.7454
2026-02-13 09:07:48,686 | INFO | FT 9 [9001/10009] loss=0.7447
2026-02-13 09:09:25,222 | INFO | FT 9 [9501/10009] loss=0.7448
2026-02-13 09:11:02,061 | INFO | FT 9 [10001/10009] loss=0.7448
2026-02-13 09:11:03,619 | INFO | FT 9 [10009/10009] loss=0.7448
2026-02-13 09:12:30,330 | INFO | FT ep 9/10: train_loss=0.7448, val_acc=0.7920
2026-02-13 09:12:30,529 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 09:12:31,646 | INFO | FT 10 [1/10009] loss=0.6740
2026-02-13 09:14:08,013 | INFO | FT 10 [501/10009] loss=0.7329
2026-02-13 09:15:44,420 | INFO | FT 10 [1001/10009] loss=0.7295
2026-02-13 09:17:21,762 | INFO | FT 10 [1501/10009] loss=0.7300
2026-02-13 09:18:58,245 | INFO | FT 10 [2001/10009] loss=0.7283
2026-02-13 09:20:34,836 | INFO | FT 10 [2501/10009] loss=0.7300
2026-02-13 09:22:11,537 | INFO | FT 10 [3001/10009] loss=0.7316
2026-02-13 09:23:48,351 | INFO | FT 10 [3501/10009] loss=0.7312
2026-02-13 09:25:25,121 | INFO | FT 10 [4001/10009] loss=0.7319
2026-02-13 09:27:01,606 | INFO | FT 10 [4501/10009] loss=0.7313
2026-02-13 09:28:38,134 | INFO | FT 10 [5001/10009] loss=0.7312
2026-02-13 09:30:14,687 | INFO | FT 10 [5501/10009] loss=0.7315
2026-02-13 09:31:51,417 | INFO | FT 10 [6001/10009] loss=0.7318
2026-02-13 09:33:27,895 | INFO | FT 10 [6501/10009] loss=0.7316
2026-02-13 09:35:04,600 | INFO | FT 10 [7001/10009] loss=0.7322
2026-02-13 09:36:41,321 | INFO | FT 10 [7501/10009] loss=0.7317
2026-02-13 09:38:17,808 | INFO | FT 10 [8001/10009] loss=0.7318
2026-02-13 09:39:54,355 | INFO | FT 10 [8501/10009] loss=0.7321
2026-02-13 09:41:30,906 | INFO | FT 10 [9001/10009] loss=0.7322
2026-02-13 09:43:07,422 | INFO | FT 10 [9501/10009] loss=0.7318
2026-02-13 09:44:43,879 | INFO | FT 10 [10001/10009] loss=0.7320
2026-02-13 09:44:45,437 | INFO | FT 10 [10009/10009] loss=0.7320
2026-02-13 09:46:11,825 | INFO | FT ep 10/10: train_loss=0.7320, val_acc=0.7929
2026-02-13 09:46:12,036 | INFO | New best! Saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_best.pth
2026-02-13 09:47:38,684 | INFO | ============================================================
2026-02-13 09:47:38,684 | INFO | Summary
2026-02-13 09:47:38,684 | INFO | ============================================================
2026-02-13 09:47:38,684 | INFO | Base MACs:    4.12G -> Pruned: 3.79G (91.9%)
2026-02-13 09:47:38,684 | INFO | Base Params:  25.56M -> Pruned: 21.72M (85.0%)
2026-02-13 09:47:38,684 | INFO | Original Acc: 0.8035
2026-02-13 09:47:38,684 | INFO | Final Acc:    0.7929
2026-02-13 09:47:38,684 | INFO | Best Acc:     0.7929
2026-02-13 09:47:38,889 | INFO | Final model saved to /algo/NetOptimization/outputs/VBP/ResNet50_TP/global_lrv1/kr_0.9/vbp_final.pth
